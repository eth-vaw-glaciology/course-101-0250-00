<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <script src="/libs/lunr/lunr.min.js"></script> <script src="/libs/lunr/lunr_index.js"></script> <script src="/libs/lunr/lunrclient.min.js"></script> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/poole_hyde.css"> <link rel=stylesheet  href="/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=icon  href="/assets/favicon.png"> <title>Lecture 11</title> <style> .content {max-width: 50rem} </style> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/assets/vaw_logo.png" style="width: 180px; height: auto; display: inline"> <div style="margin-bottom: 0.5em"><a href="/"> Fall 2025</a> <span style="opacity: 0.7;">| <a href=https://www.vorlesungen.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2025W&ansicht=KATALOGDATEN&lerneinheitId=193496&lang=en> ETHZ 101-0250-00</a></span></div> <br> <h1><a href="/">Solving partial differential equations in parallel on GPUs I</a></h1> <div style="line-height:18px; font-size: 15px; opacity: 0.85">by &nbsp; <a href="https://vaw.ethz.ch/en/people/person-detail.MjcwOTYw.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ludovic R√§ss</a>, &nbsp; <a href="https://vaw.ethz.ch/en/personen/person-detail.html?persid=124402">Mauro Werder</a>, &nbsp; <a href="https://www.cscs.ch/about/staff/">Samuel Omlin</a> & <br> <a href="https://vaw.ethz.ch/en/people/person-detail.MzAwMjIy.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ivan Utkin</a> </div> </div> <br> <style> </style> <nav class=sidebar-nav  style="opacity: 0.9; margin-bottom: 1cm;"> <a class="sidebar-nav-item " href="/"><b>Welcome</b></a> <a class="sidebar-nav-item " href="/logistics/">Logistics</a> <a class="sidebar-nav-item " href="/homework/">Homeworks</a> <a class="sidebar-nav-item " href="/software_install/">Software install</a> <a class="sidebar-nav-item " href="/extras/">Extras</a> <br> <div class=course-section >Part 1 ‚Äì Introduction</div> <a class="sidebar-nav-item active" href="/lecture1/">Lecture 1 ‚Äì Introduction to Julia</a> <a class="sidebar-nav-item " href="/lecture2/">Lecture 2 ‚Äì PDEs & physical processes</a> <a class="sidebar-nav-item " href="/lecture3/"> Lecture 3 ‚Äì Solving elliptic PDEs</a> <a class="sidebar-nav-item " href="/lecture4/">Lecture 4 ‚Äì Coupled multi-physics</a> <div class=course-section >Part 2 ‚Äì Solving PDEs on GPUs</div> <a class="sidebar-nav-item " href="/lecture5/">Lecture 5 ‚Äì Porous convection</a> <a class="sidebar-nav-item " href="/lecture6/">Lecture 6 ‚Äì Parallel computing</a> <a class="sidebar-nav-item " href="/lecture7/">Lecture 7 ‚Äì GPU computing</a> <a class="sidebar-nav-item " href="/lecture8/">Lecture 8 ‚Äì xPU computing</a> <div class=course-section >Part 3 ‚Äì Multi-GPU computing (projects)</div> <a class="sidebar-nav-item " href="/lecture9/">Lecture 9 ‚Äì Julia MPI & multi-xPU</a> <a class="sidebar-nav-item " href="/lecture10/">Lecture 10 ‚Äì ImplicitGlobalGrid.jl</a> <a class="sidebar-nav-item active" href="/lecture11/">Lecture 11 ‚Äì Multi-xPU & final project</a> <div class="sidebar-nav-item under-construction"> Lecture 12 ‚Äì Advanced optimisations</div> </nav> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/search/index.html"> </form> <br> <br> </div> </div> <div class="content container"> <div class=franklin-content > <h1 id=lecture_11 ><a href="#lecture_11" class=header-anchor >Lecture 11</a></h1> <blockquote> <p><strong>Agenda</strong><br />üìö Multi-xPU thermal porous convection 3D<br />üíª Automatic documentation and CI<br />üöß Project:</p> <ul> <li><p>Multi-xPU thermal porous convection 3D</p> <li><p>Automatic documentation and CI</p> </ul> </blockquote> <hr /> <p><a id=content  class=anchor ></a> <strong>Content</strong></p> <div class=franklin-toc ><ol><li><a href="#lecture_11">Lecture 11</a><li><a href="#project_-_3d_thermal_porous_convection_on_multi-xpu">Project - 3D thermal porous convection on multi-xPU</a><ol><li><a href="#using_implicitglobalgridjl_continued">Using <code>ImplicitGlobalGrid.jl</code> &#40;continued&#41;</a><li><a href="#multi-xpu_3d_thermal_porous_convection">Multi-xPU 3D thermal porous convection</a></ol><li><a href="#exercises_-_lecture_11">Exercises - lecture 11</a><ol><li><a href="#exercise_1_multi-xpu_computing_projects">Exercise 1 ‚Äî <strong>Multi-xPU computing projects</strong></a><li><a href="#exercise_2_automatic_documentation_in_julia">Exercise 2 ‚Äî <strong>Automatic documentation in Julia</strong></a></ol><li><a href="#solving_partial_differential_equations_in_parallel_on_gpus_ii">Solving Partial Differential Equations in Parallel on GPUs II</a><ol><li><a href="#a_solving_one_of_the_proposed_pdes">A. Solving One of the Proposed PDEs</a><li><a href="#b_solving_a_pde_of_your_interest">B. Solving a PDE of Your Interest</a><li><a href="#c_advanced_optimisations">C. Advanced Optimisations</a></ol></ol></div> <p><a href="#exercises_-_lecture_11"><em>üëâ get started with exercises</em></a></p> <hr /> <h1 id=project_-_3d_thermal_porous_convection_on_multi-xpu ><a href="#project_-_3d_thermal_porous_convection_on_multi-xpu" class=header-anchor >Project - 3D thermal porous convection on multi-xPU</a></h1> <h3 id=the_goal_of_this_lecture_11 ><a href="#the_goal_of_this_lecture_11" class=header-anchor >The goal of this lecture 11:</a></h3> <ul> <li><p>Projects</p> <ul> <li><p>Create a multi-xPU version of the 3D thermal porous convection xPU code</p> <li><p>Combine <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl">ImplicitGlobalGrid.jl</a> and <a href="https://github.com/omlins/ParallelStencil.jl">ParallelStencil.jl</a></p> <li><p>Finalise the documentation of your project</p> </ul> <li><p>Automatic documentation and CI</p> </ul> <h2 id=using_implicitglobalgridjl_continued ><a href="#using_implicitglobalgridjl_continued" class=header-anchor >Using <code>ImplicitGlobalGrid.jl</code> &#40;continued&#41;</a></h2> <p>In previous Lecture 10, we introduced <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl">ImplicitGlobalGrid.jl</a>, which renders distributed parallelisation with GPU and CPU for HPC a very simple task.</p> <p>Also, ImplicitGlobalGrid.jl elegantly combines with <a href="https://github.com/omlins/ParallelStencil.jl">ParallelStencil.jl</a> to, e.g., hide communication behind computation.</p> <h2 id=multi-xpu_3d_thermal_porous_convection ><a href="#multi-xpu_3d_thermal_porous_convection" class=header-anchor >Multi-xPU 3D thermal porous convection</a></h2> <p>Let&#39;s step through the following content:</p> <ul> <li><p>Create a multi-xPU version of your thermal porous convection 3D xPU code you finalised in lecture 8</p> <li><p>Keep it xPU compatible using <code>ParallelStencil.jl</code></p> <li><p>Deploy it on multiple xPUs using <code>ImplicitGlobalGrid.jl</code></p> </ul> <p>üëâ You&#39;ll find a version of the <code>PorousConvection_3D_xpu.jl</code> code in the solutions folder on Moodle after exercises deadline if needed to get you started.</p> <h3 id=enable_multi-xpu_support ><a href="#enable_multi-xpu_support" class=header-anchor >Enable multi-xPU support</a></h3> <p>Only a few changes are required to enable multi-xPU support, namely:</p> <ol> <li><p>Copy your working <code>PorousConvection_3D_xpu.jl</code> code developed for the exercises in Lecture 8 and rename it <code>PorousConvection_3D_multixpu.jl</code>.</p> <li><p>Add at the beginning of the code</p> </ol> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> ImplicitGlobalGrid
<span class=hljs-keyword >import</span> MPI</code></pre> <ol start=3 > <li><p>Further, add global maximum computation using MPI reduction function to be used instead of <code>maximum&#40;&#41;</code></p> </ol> <pre><code class="julia hljs">max_g(A) = (max_l = maximum(A); MPI.Allreduce(max_l, MPI.MAX, MPI.COMM_WORLD))</code></pre>
<ol start=4 >
<li><p>In the <code># numerics</code> section, initialise the global grid right after defining <code>nx,ny,nz</code> and use now global grid <code>nx_g&#40;&#41;</code>,<code>ny_g&#40;&#41;</code> and <code>nz_g&#40;&#41;</code> for defining <code>maxiter</code> and <code>ncheck</code>, as well as in any other places when needed.</p>

</ol>
<pre><code class="julia hljs">nx,ny       = <span class=hljs-number >2</span> * (nz + <span class=hljs-number >1</span>) - <span class=hljs-number >1</span>, nz
me, dims    = init_global_grid(nx, ny, nz)  <span class=hljs-comment ># init global grid and more</span>
b_width     = (<span class=hljs-number >8</span>, <span class=hljs-number >8</span>, <span class=hljs-number >4</span>)                     <span class=hljs-comment ># for comm / comp overlap</span></code></pre>
<ol start=5 >
<li><p>Modify the temperature initialisation using ImplicitGlobalGrid&#39;s global coordinate helpers &#40;<code>x_g</code>, etc...&#41;, including one internal boundary condition update &#40;update halo&#41;:</p>

</ol>
<pre><code class="julia hljs">T  = <span class=hljs-meta >@zeros</span>(nx, ny, nz)
T .= Data.<span class=hljs-built_in >Array</span>([ŒîT * exp(-(x_g(ix, dx, T) + dx / <span class=hljs-number >2</span> - lx / <span class=hljs-number >2</span>)^<span class=hljs-number >2</span>
                          -(y_g(iy, dy, T) + dy / <span class=hljs-number >2</span> - ly / <span class=hljs-number >2</span>)^<span class=hljs-number >2</span>
                          -(z_g(iz, dz, T) + dz / <span class=hljs-number >2</span> - lz / <span class=hljs-number >2</span>)^<span class=hljs-number >2</span>) <span class=hljs-keyword >for</span> ix = <span class=hljs-number >1</span>:size(T, <span class=hljs-number >1</span>), iy = <span class=hljs-number >1</span>:size(T, <span class=hljs-number >2</span>), iz = <span class=hljs-number >1</span>:size(T, <span class=hljs-number >3</span>)])
T[:, :, <span class=hljs-number >1</span>  ] .=  ŒîT / <span class=hljs-number >2</span>
T[:, :, <span class=hljs-keyword >end</span>] .= -ŒîT / <span class=hljs-number >2</span>
update_halo!(T)
T_old = copy(T)</code></pre>
<ol start=6 >
<li><p>Prepare for visualisation, making sure only <code>me&#61;&#61;0</code> creates the output directory. Also, prepare an array for storing inner points only &#40;no halo&#41; <code>T_inn</code> as well as global array to gather subdomains <code>T_v</code></p>

</ol>
<pre><code class="julia hljs"><span class=hljs-keyword >if</span> do_viz
    <span class=hljs-literal >ENV</span>[<span class=hljs-string >&quot;GKSwstype&quot;</span>]=<span class=hljs-string >&quot;nul&quot;</span>
    <span class=hljs-keyword >if</span> (me==<span class=hljs-number >0</span>) <span class=hljs-keyword >if</span> isdir(<span class=hljs-string >&quot;viz3Dmpi_out&quot;</span>)==<span class=hljs-literal >false</span> mkdir(<span class=hljs-string >&quot;viz3Dmpi_out&quot;</span>) <span class=hljs-keyword >end</span>; loadpath=<span class=hljs-string >&quot;viz3Dmpi_out/&quot;</span>; anim=Animation(loadpath,<span class=hljs-built_in >String</span>[]); println(<span class=hljs-string >&quot;Animation directory: <span class=hljs-subst >$(anim.dir)</span>&quot;</span>) <span class=hljs-keyword >end</span>
    nx_v, ny_v, nz_v = (nx - <span class=hljs-number >2</span>) * dims[<span class=hljs-number >1</span>], (ny - <span class=hljs-number >2</span>) * dims[<span class=hljs-number >2</span>], (nz - <span class=hljs-number >2</span>) * dims[<span class=hljs-number >3</span>]
    (nx_v * ny_v * nz_v * sizeof(Data.<span class=hljs-built_in >Number</span>) &gt; <span class=hljs-number >0.8</span> * Sys.free_memory()) &amp;&amp; error(<span class=hljs-string >&quot;Not enough memory for visualization.&quot;</span>)
    T_v   = zeros(nx_v, ny_v, nz_v) <span class=hljs-comment ># global array for visu</span>
    T_inn = zeros(nx - <span class=hljs-number >2</span>, ny - <span class=hljs-number >2</span>, nz - <span class=hljs-number >2</span>) <span class=hljs-comment ># no halo local array for visu</span>
    xi_g, zi_g = <span class=hljs-built_in >LinRange</span>(-lx / <span class=hljs-number >2</span> + dx + dx / <span class=hljs-number >2</span>, lx / <span class=hljs-number >2</span> - dx - dx / <span class=hljs-number >2</span>, nx_v), <span class=hljs-built_in >LinRange</span>(-lz + dz + dz / <span class=hljs-number >2</span>, -dz - dz / <span class=hljs-number >2</span>, nz_v) <span class=hljs-comment ># inner points only</span>
    iframe = <span class=hljs-number >0</span>
<span class=hljs-keyword >end</span></code></pre>
<ol start=7 >
<li><p>Moving to the time loop, add halo update function <code>update_halo&#33;</code> after the kernel that computes the fluid fluxes. You can additionally wrap it in the <code>@hide_communication</code> block to enable communication/computation overlap &#40;using <code>b_width</code> defined above&#41;</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-meta >@hide_communication</span> b_width <span class=hljs-keyword >begin</span>
    <span class=hljs-meta >@parallel</span> compute_Dflux!(qDx, qDy, qDz, Pf, T, k_Œ∑f, _dx, _dy, _dz, Œ±œÅg, _1_Œ∏_dœÑ_D)
    update_halo!(qDx, qDy, qDz)
<span class=hljs-keyword >end</span></code></pre>
<ol start=8 >
<li><p>Apply a similar step to the temperature update, where you can also include boundary condition computation as following &#40;‚ö†Ô∏è no other construct is currently allowed&#41;</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-meta >@hide_communication</span> b_width <span class=hljs-keyword >begin</span>
    <span class=hljs-meta >@parallel</span> update_T!(T, qTx, qTy, qTz, dTdt, _dx, _dy, _dz, _1_dt_Œ≤_dœÑ_T)
    <span class=hljs-meta >@parallel</span> (<span class=hljs-number >1</span>:size(T, <span class=hljs-number >2</span>), <span class=hljs-number >1</span>:size(T, <span class=hljs-number >3</span>)) bc_x!(T)
    <span class=hljs-meta >@parallel</span> (<span class=hljs-number >1</span>:size(T, <span class=hljs-number >1</span>), <span class=hljs-number >1</span>:size(T, <span class=hljs-number >3</span>)) bc_y!(T)
    update_halo!(T)
<span class=hljs-keyword >end</span></code></pre>
<ol start=9 >
<li><p>Use now the <code>max_g</code> function instead of <code>maximum</code> to collect the global maximum among all local arrays spanning all MPI processes. Use it in the timestep <code>dt</code> definition and in the error calculation &#40;instead of <code>maximum</code>&#41;.</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-comment ># time step</span>
dt = <span class=hljs-keyword >if</span> it == <span class=hljs-number >1</span>
    <span class=hljs-number >0.1</span> * min(dx, dy, dz) / (Œ±œÅg * ŒîT * k_Œ∑f)
<span class=hljs-keyword >else</span>
    min(<span class=hljs-number >5.0</span> * min(dx, dy, dz) / (Œ±œÅg * ŒîT * k_Œ∑f), œï * min(dx / max_g(abs.(qDx)), dy / max_g(abs.(qDy)), dz / max_g(abs.(qDz))) / <span class=hljs-number >3.1</span>)
<span class=hljs-keyword >end</span></code></pre>
<ol start=10 >
<li><p>Make sure all printing statements are only executed by <code>me&#61;&#61;0</code> in order to avoid each MPI process to print to screen, and use <code>nx_g&#40;&#41;</code> instead of local <code>nx</code> in the printed statements when assessing the iteration per number of grid points.</p>

<li><p>Update the visualisation and output saving part</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-comment ># visualisation</span>
<span class=hljs-keyword >if</span> do_viz &amp;&amp; (it % nvis == <span class=hljs-number >0</span>)
    T_inn .= <span class=hljs-built_in >Array</span>(T)[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>, <span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>, <span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>]; gather!(T_inn, T_v)
    <span class=hljs-keyword >if</span> me == <span class=hljs-number >0</span>
        p1 = heatmap(xi_g, zi_g, T_v[:, ceil(<span class=hljs-built_in >Int</span>, ny_g() / <span class=hljs-number >2</span>), :]&#x27;; xlims=(xi_g[<span class=hljs-number >1</span>], xi_g[<span class=hljs-keyword >end</span>]), ylims=(zi_g[<span class=hljs-number >1</span>], zi_g[<span class=hljs-keyword >end</span>]), aspect_ratio=<span class=hljs-number >1</span>, c=:turbo)
        <span class=hljs-comment ># display(p1)</span>
        png(p1, <span class=hljs-meta >@sprintf</span>(<span class=hljs-string >&quot;viz3Dmpi_out/%04d.png&quot;</span>, iframe += <span class=hljs-number >1</span>))
        save_array(<span class=hljs-meta >@sprintf</span>(<span class=hljs-string >&quot;viz3Dmpi_out/out_T_%04d&quot;</span>, iframe), convert.(<span class=hljs-built_in >Float32</span>, T_v))
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre>
<ol start=12 >
<li><p>Finalise the global grid before returning from the main function</p>

</ol>
<pre><code class="julia hljs">finalize_global_grid()
<span class=hljs-keyword >return</span></code></pre>
<p>If you made it up to here, you should now be able to launch your <code>PorousConvection_3D_multixpu.jl</code> code on multiple GPUs. Let&#39;s give it a try üî•</p>
<p>Make sure to have set following parameters:</p>
<pre><code class="julia hljs">lx,ly,lz    = <span class=hljs-number >40.0</span>, <span class=hljs-number >20.0</span>, <span class=hljs-number >20.0</span>
Ra          = <span class=hljs-number >1000</span>
nz          = <span class=hljs-number >63</span>
nx,ny       = <span class=hljs-number >2</span> * (nz + <span class=hljs-number >1</span>) - <span class=hljs-number >1</span>, nz
b_width     = (<span class=hljs-number >8</span>, <span class=hljs-number >8</span>, <span class=hljs-number >4</span>) <span class=hljs-comment ># for comm / comp overlap</span>
nt          = <span class=hljs-number >500</span>
nvis        = <span class=hljs-number >50</span></code></pre>
<h3 id=benchmark_run ><a href="#benchmark_run" class=header-anchor >Benchmark run</a></h3>
<p>Then, launch the script on Piz Daint on 8 GPU nodes upon adapting the the <code>runme_mpi_daint.sh</code> or <code>sbatch sbatch_mpi_daint.sh</code> scripts &#40;see <a href="/software_install/#cuda-aware_mpi_on_piz_daint">here</a>&#41; üöÄ</p>
<p>The final 2D slice &#40;at <code>ny_g&#40;&#41;/2</code>&#41; produced should look as following and the code takes about 25min to run:</p>
<p><img src="../assets/literate_figures/l11_porous_convect_mpi_sl.png" alt="3D porous convection MPI" /></p>
<h3 id=3d_calculation ><a href="#3d_calculation" class=header-anchor >3D calculation</a></h3>
<p>Running the code at higher resolution &#40;<code>508x252x252</code> grid points&#41; and for 6000 timesteps produces the following result</p>

<center>
  <video width="90%" autoplay loop controls src="../assets/literate_figures/l9_porous_convection_mxpu.mp4"/>
</center>


<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<h1 id=exercises_-_lecture_11 ><a href="#exercises_-_lecture_11" class=header-anchor >Exercises - lecture 11</a></h1>
<div class=warning ><div class=title >‚ö†Ô∏è Warning&#33;</div>
<div class=messg ><strong>Exercise 1</strong> is the final step of your project - scripts and results should be added to the <code>PorousConvection</code> subfolder in your private GitHub repo. The git commit hash &#40;or SHA&#41; of the final push needs to be uploaded on Moodle &#40;<a href="/homework">more</a>&#41;.<br />From your <code>homework-8</code> branch, create a new git branch named <code>homework-11</code> in order to build upon work performed for homework 8.<br />The exercises from Lecture 11 include the last steps towards the completion of the project. Hand-in information can be found in <a href="/logistics/#project">Logistics</a>.</div></div>
<h2 id=exercise_1_multi-xpu_computing_projects ><a href="#exercise_1_multi-xpu_computing_projects" class=header-anchor >Exercise 1 ‚Äî <strong>Multi-xPU computing projects</strong></a></h2>
<p>üëâ See <a href="/logistics/#submission">Logistics</a> for submission details.</p>
<p>The goal of this exercise is to:</p>
<ul>
<li><p>Further familiarise with distributed computing</p>

<li><p>Combine <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl">ImplicitGlobalGrid.jl</a> and <a href="https://github.com/omlins/ParallelStencil.jl">ParallelStencil.jl</a></p>

<li><p>Learn about GPU MPI on the way</p>

</ul>
<p>In this exercise, you will:</p>
<ul>
<li><p>Create a multi-xPU version of the 3D thermal porous convection code from lecture 8</p>

<li><p>Keep it xPU compatible using <code>ParallelStencil.jl</code></p>

<li><p>Deploy it on multiple xPUs using <code>ImplicitGlobalGrid.jl</code></p>

</ul>
<p>üëâ You&#39;ll find a version of the <code>PorousConvection_3D_xpu.jl</code> code on Moodle after exercises deadline if needed to get you started.</p>
<ol>
<li><p>Copy the <code>PorousConvection_3D_xpu.jl</code> code from exercises in Lecture 8 and rename it <code>PorousConvection_3D_multixpu.jl</code>.</p>

<li><p>Refer to the steps outlined in the <a href="#multi-xpu_3d_thermal_porous_convection">Multi-xPU 3D thermal porous convection</a> section from the lecture to implement the changes needed to port the 3D single xPU code &#40;from Lecture 8&#41; to multi-xPU.</p>

<li><p>Upon completion, verify the script converges and produces expected output for following parameters:</p>

</ol>
<pre><code class="julia hljs">lx,ly,lz    = <span class=hljs-number >40.0</span>, <span class=hljs-number >20.0</span>, <span class=hljs-number >20.0</span>
Ra          = <span class=hljs-number >1000</span>
nz          = <span class=hljs-number >63</span>
nx,ny       = <span class=hljs-number >2</span> * (nz + <span class=hljs-number >1</span>) - <span class=hljs-number >1</span>, nz
b_width     = (<span class=hljs-number >8</span>, <span class=hljs-number >8</span>, <span class=hljs-number >4</span>) <span class=hljs-comment ># for comm / comp overlap</span>
nt          = <span class=hljs-number >500</span>
nvis        = <span class=hljs-number >50</span></code></pre>
<p>Use 8 GPUs on Daint.Alps adapting the <code>runme_mpi_daint.sh</code> or <code>sbatch sbatch_mpi_daint.sh</code> scripts &#40;see <a href="/software_install/#cuda-aware_mpi_on_piz_daint">here</a>&#41; to use CUDA-aware MPI üöÄ</p>
<p>The final 2D slice &#40;at <code>ny_g&#40;&#41;/2</code>&#41; produced should look similar as the figure depicted in <a href="#benchmark_run">Lecture 11</a>.</p>
<h3 id=task ><a href="#task" class=header-anchor >Task</a></h3>
<p>Now that you made sure the code runs as expected, launch <code>PorousConvection_3D_multixpu.jl</code> for 2000 steps on 8 GPUs at higher resolution &#40;global grid of <code>508x252x252</code>&#41; setting:</p>
<pre><code class="julia hljs">nz          = <span class=hljs-number >127</span>
nx,ny       = <span class=hljs-number >2</span> * (nz + <span class=hljs-number >1</span>) - <span class=hljs-number >1</span>, nz
nt          = <span class=hljs-number >2000</span>
nvis        = <span class=hljs-number >100</span></code></pre>
<p>and keeping other parameters unchanged.</p>
<p>Use <code>sbtach</code> command to launch a non-interactive job which may take about 5h30-6h to execute.</p>
<p>Produce a figure or animation showing the final stage of temperature distribution in 3D and add it to a new section titled <code>## Porous convection 3D MPI</code> in the <code>PorousConvection</code> project subfolder&#39;s <code>README.md</code>. You can use the Makie visualisation helper script from Lecture 8 for this purpose &#40;making sure to adapt the resolution and other input params if needed&#41;.</p>

<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<hr />
<h2 id=exercise_2_automatic_documentation_in_julia ><a href="#exercise_2_automatic_documentation_in_julia" class=header-anchor >Exercise 2 ‚Äî <strong>Automatic documentation in Julia</strong></a></h2>
<p>üëâ See <a href="/logistics/#submission">Logistics</a> for submission details.</p>
<p>The goal of this exercise is to:</p>
<ul>
<li><p>write some documentation</p>
<ul>
<li><p>using <a href="https://docs.julialang.org/en/v1/manual/documentation/">doc-strings</a></p>

<li><p>using <a href="https://github.com/fredrikekre/Literate.jl"><code>Literate.jl</code></a></p>

</ul>

</ul>
<p>One task you&#39;ve already done, namely to update the <code>README.md</code> of this set of exercises&#33;</p>
<p>Tasks:</p>
<ol>
<li><p>Add doc-string to the functions of following scripts:</p>
<ul>
<li><p><code>PorousConvection_3D_xpu.jl</code></p>

<li><p><code>PorousConvection_3D_multixpu.jl</code></p>

</ul>

<li><p>Add to the <code>PorousConvection</code> folder  a <code>Literate.jl</code> script called <code>bin_io_script.jl</code> that contains and documents following <code>save_array</code> and <code>load_array</code> functions you may have used in your 3D script</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-string >&quot;&quot;&quot;
Some docstring
&quot;&quot;&quot;</span>
<span class=hljs-keyword >function</span> save_array(Aname,A)
    fname = string(Aname,<span class=hljs-string >&quot;.bin&quot;</span>)
    out = open(fname,<span class=hljs-string >&quot;w&quot;</span>); write(out,A); close(out)
<span class=hljs-keyword >end</span>

<span class=hljs-string >&quot;&quot;&quot;
Some docstring
&quot;&quot;&quot;</span>
<span class=hljs-keyword >function</span> load_array(Aname,A)
    fname = string(Aname,<span class=hljs-string >&quot;.bin&quot;</span>)
    fid=open(fname,<span class=hljs-string >&quot;r&quot;</span>); read!(fid,A); close(fid)
<span class=hljs-keyword >end</span></code></pre>
<p>Add to the <code>bin_io_script.jl</code> a <code>main&#40;&#41;</code> function that will:</p>
<ul>
<li><p>generate a <code>3x3</code> array <code>A</code> of random numbers</p>

<li><p>initialise a second array <code>B</code> to hold the read-in results</p>

<li><p>call the <code>save_array</code> function and save the random number array</p>

<li><p>call the <code>load_array</code> function and read the random number array in <code>B</code></p>

<li><p>return B</p>

<li><p>call the main function making and plotting as following</p>

</ul>
<pre><code class="julia hljs">B = main()
heatmap(B)</code></pre>
<ol start=3 >
<li><p>Make the Literate-based workflow to automatically build on GitHub using GitHub Actions. For this, you need to add to the <code>.github/workflow</code> folder &#40;the one containing your <code>CI.yml</code> for testing&#41; the <code>Literate.yml</code> script which we saw in this lecture&#39;s section <a href="#documentation_tools_automating_literatejl">Documentation tools: Automating Literate.jl</a>.</p>

<li><p>That&#39;s all&#33; Head to the <a href="/logistics/#project">Project section in Logistics</a> for a check-list about what you should hand in for this project.</p>

</ol>

<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<hr />
<h1 id=solving_partial_differential_equations_in_parallel_on_gpus_ii ><a href="#solving_partial_differential_equations_in_parallel_on_gpus_ii" class=header-anchor >Solving Partial Differential Equations in Parallel on GPUs II</a></h1>
<p>Enjoyed the course and want more? In the upcoming Spring Semester, we will offer Part II, in which your task will be to develop a new numerical code <strong>from scratch</strong>, using the knowledge acquired in this course. You will choose one project from three possible directions &#40;A, B, or C&#41;:</p>
<h2 id=a_solving_one_of_the_proposed_pdes ><a href="#a_solving_one_of_the_proposed_pdes" class=header-anchor >A. Solving One of the Proposed PDEs</a></h2>
<h3 id=multi-gpu_navierstokes_in_3d ><a href="#multi-gpu_navierstokes_in_3d" class=header-anchor ><ol>
<li><p>Multi-GPU Navier‚ÄìStokes in 3D</p>

</ol>
</a></h3>
<p>Implement a simple Navier‚ÄìStokes solver using Chorin‚Äôs projection method and an advection scheme based on the method of characteristics. For the projection step, you will implement a Poisson solver for the pressure. You may reuse the pseudo-transient solver from class or implement your own &#40;e.g., a multigrid solver or a Fourier-transform-based spectral solver&#41;. The only requirement is that your solver must run on GPUs and scale well. Feel free to take inspiration from the <a href="https://github.com/utkinis/NavierStokes.jl">2D reference implementation</a>.</p>
<h3 id=ol_start2_free_convection_simulation ><a href="#ol_start2_free_convection_simulation" class=header-anchor ><ol start=2 >
<li><p>Free Convection Simulation</p>

</ol>
</a></h3>
<p>Use the <a href="https://github.com/omlins/ParallelStencil.jl#thermo-mechanical-convection-2-d-app"><code>ParallelStencil.jl</code> miniapp</a> as a starting point to implement your own <strong>3D multi-GPU mantle convection solver</strong>.</p>
<h3 id=ol_start3_hydro-mechanical_flow_localisation ><a href="#ol_start3_hydro-mechanical_flow_localisation" class=header-anchor ><ol start=3 >
<li><p>Hydro-Mechanical Flow Localisation</p>

</ol>
</a></h3>
<p>Use the <a href="https://github.com/omlins/ParallelStencil.jl#hydro-mechanical-porosity-waves-2-d-app"><code>ParallelStencil.jl</code> miniapp</a> as a starting point to implement your own <strong>3D multi-GPU hydro-mechanical ‚Äútwo-phase flow‚Äù solver</strong> capable of capturing the formation and propagation of solitary porosity waves.</p>
<h3 id=ol_start4_wave_physics ><a href="#ol_start4_wave_physics" class=header-anchor ><ol start=4 >
<li><p>Wave Physics</p>

</ol>
</a></h3>
<p>Elastic wave propagation is central to computational seismology, as it enables imaging of the subsurface, and it also has applications far beyond geosciences. Implement your 3D elastic wave solver, using the acoustic wave solver from the <a href="https://github.com/omlins/ParallelStencil.jl#acoustic-wave-3-d-app"><code>ParallelStencil.jl</code> miniapp</a> as a starting point. Alternatively, you may implement Maxwell‚Äôs equations to simulate the propagation of electromagnetic fields.</p>
<h2 id=b_solving_a_pde_of_your_interest ><a href="#b_solving_a_pde_of_your_interest" class=header-anchor >B. Solving a PDE of Your Interest</a></h2>
<p>Show your creativity by proposing your own PDE-based problem‚Äîperhaps related to another project or to a future research direction. We‚Äôll do our best to help you implement it. Relativistic MHD? Phase separation in alloys? Electromagnetic wave propagation? Spectral methods for PDEs? Name your topic&#33; Ideally, bring relevant papers and equations.</p>
<h2 id=c_advanced_optimisations ><a href="#c_advanced_optimisations" class=header-anchor >C. Advanced Optimisations</a></h2>
<p>If you are interested in GPU code optimisation, you can work through the advanced optimisation material &#40;Lecture 12&#41; and, for example, add shared-memory support and manual register queuing to accelerate the 3D thermal porous convection solver from class. Alternatively, pick one of the <a href="https://github.com/omlins/ParallelStencil.jl#acoustic-wave-3-d-app"><code>ParallelStencil.jl</code> miniapps</a> and optimise it further.</p>
<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<div class=page-foot >
  <div class=copyright >
    <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a><br>
    Last modified: November 25, 2025. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div>