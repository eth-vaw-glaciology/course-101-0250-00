{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "_Lecture 9_\n",
    "# Projects - 3D thermal porous convection on multi-xPU"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The goal of this lecture 9:\n",
    "\n",
    "- Projects\n",
    "    - Create a multi-xPU version of the 3D thermal porous convection xPU code\n",
    "    - Combine [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl) and [ParallelStencil.jl](https://github.com/omlins/ParallelStencil.jl)\n",
    "    - Finalise the documentation of your project\n",
    "- Automatic documentation and CI"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using `ImplicitGlobalGrid.jl` (continued)\n",
    "\n",
    "In previous Lecture 8, we introduced [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl), which renders distributed parallelisation with GPU and CPU for HPC a very simple task.\n",
    "\n",
    "Also, ImplicitGlobalGrid.jl elegantly combines with [ParallelStencil.jl](https://github.com/omlins/ParallelStencil.jl) to, e.g., hide communication behind computation."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a rapid tour of [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl)'s' documentation before using it to turn the 3D thermal porous diffusion solver into a multi-xPU solver."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-xPU 3D thermal porous convection\n",
    "\n",
    "Let's step through the following content:\n",
    "- Create a multi-xPU version of your thermal porous convection 3D xPU code you finalised in lecture 7\n",
    "- Keep it xPU compatible using `ParallelStencil.jl`\n",
    "- Deploy it on multiple xPUs using `ImplicitGlobalGrid.jl`\n",
    "\n",
    "üëâ You'll find a version of the `PorousConvection_3D_xpu.jl` code in the solutions folder on Polybox after exercises deadline if needed to get you started."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Enable multi-xPU support\n",
    "Only a few changes are required to enable multi-xPU support, namely:\n",
    "\n",
    "1. Copy your working `PorousConvection_3D_xpu.jl` code developed for the exercises in Lecture 7 and rename it `PorousConvection_3D_multixpu.jl`."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Add at the beginning of the code"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ImplicitGlobalGrid\n",
    "import MPI"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Also add global maximum computation using MPI reduction function"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "max_g(A) = (max_l = maximum(A); MPI.Allreduce(max_l, MPI.MAX, MPI.COMM_WORLD))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. In the `# numerics` section, initialise the global grid right after defining `nx,ny,nz` and use now global grid `nx_g()`,`ny_g()` and `nz_g()` for defining `maxiter` and `ncheck`, as well as in any other places when needed."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nx,ny       = 2*(nz+1)-1,nz\n",
    "me, dims    = init_global_grid(nx, ny, nz)  # init global grid and more\n",
    "b_width     = (8,8,4)                       # for comm / comp overlap"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Modify the temperature initialisation using ImplicitGlobalGrid's global coordinate helpers (`x_g`, etc...), including one internal boundary condition update (update halo):"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "T           = @zeros(nx  ,ny  ,nz  )\n",
    "T          .= Data.Array([ŒîT*exp(-(x_g(ix,dx,T)+dx/2-lx/2)^2\n",
    "                                 -(y_g(iy,dy,T)+dy/2-ly/2)^2\n",
    "                                 -(z_g(iz,dz,T)+dz/2-lz/2)^2) for ix=1:size(T,1),iy=1:size(T,2),iz=1:size(T,3)])\n",
    "T[:,:,1].=ŒîT/2; T[:,:,end].=-ŒîT/2\n",
    "update_halo!(T)\n",
    "T_old       = copy(T)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Prepare for visualisation, making sure only `me==0` creates the output directory. Also, prepare an array for storing inner points only (no halo) `T_inn` as well as global array to gather subdomains `T_v`"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if do_viz\n",
    "    ENV[\"GKSwstype\"]=\"nul\"\n",
    "    if (me==0) if isdir(\"viz3Dmpi_out\")==false mkdir(\"viz3Dmpi_out\") end; loadpath=\"viz3Dmpi_out/\"; anim=Animation(loadpath,String[]); println(\"Animation directory: $(anim.dir)\") end\n",
    "    nx_v,ny_v,nz_v = (nx-2)*dims[1],(ny-2)*dims[2],(nz-2)*dims[3]\n",
    "    if (nx_v*ny_v*nz_v*sizeof(Data.Number) > 0.8*Sys.free_memory()) error(\"Not enough memory for visualization.\") end\n",
    "    T_v   = zeros(nx_v, ny_v, nz_v) # global array for visu\n",
    "    T_inn = zeros(nx-2, ny-2, nz-2) # no halo local array for visu\n",
    "    xi_g,zi_g = LinRange(-lx/2+dx+dx/2, lx/2-dx-dx/2, nx_v), LinRange(-lz+dz+dz/2, -dz-dz/2, nz_v) # inner points only\n",
    "    iframe = 0\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Use the `max_g` function in the timestep `dt` definition (instead of `maximum`) as one now needs to gather the global maximum among all MPI processes."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. Moving to the time loop, add halo update function `update_halo!` after the kernel that computes the fluid fluxes. You can additionally wrap it in the `@hide_communication` block to enable communication/computation overlap (using `b_width` defined above)"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@hide_communication b_width begin\n",
    "    @parallel compute_Dflux!(qDx,qDy,qDz,Pf,T,k_Œ∑f,_dx,_dy,_dz,Œ±œÅg,_1_Œ∏_dœÑ_D)\n",
    "    update_halo!(qDx,qDy,qDz)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. Apply a similar step to the temperature update, where you can also include boundary condition computation as following (‚ö†Ô∏è no other construct is currently allowed)"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@hide_communication b_width begin\n",
    "    @parallel update_T!(T,qTx,qTy,qTz,dTdt,_dx,_dy,_dz,_1_dt_Œ≤_dœÑ_T)\n",
    "    @parallel (1:size(T,2),1:size(T,3)) bc_x!(T)\n",
    "    @parallel (1:size(T,1),1:size(T,3)) bc_y!(T)\n",
    "    update_halo!(T)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Use now the `max_g` function instead of `maximum` to collect the global maximum among all local arrays spanning all MPI processes."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# time step\n",
    "dt = if it == 1\n",
    "    0.1*min(dx,dy,dz)/(Œ±œÅg*ŒîT*k_Œ∑f)\n",
    "else\n",
    "    min(5.0*min(dx,dy,dz)/(Œ±œÅg*ŒîT*k_Œ∑f),œï*min(dx/max_g(abs.(qDx)), dy/max_g(abs.(qDy)), dz/max_g(abs.(qDz)))/3.1)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "11. Make sure all printing statements are only executed by `me==0` in order to avoid each MPI process to print to screen, and use `nx_g()` instead of local `nx` in the printed statements when assessing the iteration per number of grid points."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "12. Update the visualisation and output saving part"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# visualisation\n",
    "if do_viz && (it % nvis == 0)\n",
    "    T_inn .= Array(T)[2:end-1,2:end-1,2:end-1]; gather!(T_inn, T_v)\n",
    "    if me==0\n",
    "        p1=heatmap(xi_g,zi_g,T_v[:,ceil(Int,ny_g()/2),:]';xlims=(xi_g[1],xi_g[end]),ylims=(zi_g[1],zi_g[end]),aspect_ratio=1,c=:turbo)\n",
    "        # display(p1)\n",
    "        png(p1,@sprintf(\"viz3Dmpi_out/%04d.png\",iframe+=1))\n",
    "        save_array(@sprintf(\"viz3Dmpi_out/out_T_%04d\",iframe),convert.(Float32,T_v))\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "13. Finalise the global grid before returning from the main function"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "finalize_global_grid()\n",
    "return"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you made it up to here, you should now be able to launch your `PorousConvection_3D_multixpu.jl` code on multiple GPUs. Let's give it a try üî•"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure to have set following parameters:"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lx,ly,lz    = 40.0,20.0,20.0\n",
    "Ra          = 1000\n",
    "nz          = 63\n",
    "nx,ny       = 2*(nz+1)-1,nz\n",
    "b_width     = (8,8,4) # for comm / comp overlap\n",
    "nt          = 500\n",
    "nvis        = 50"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Benchmark run\n",
    "Then, launch the script on Piz Daint on 8 GPU nodes upon adapting the the `runme_mpi_daint.sh` or `sbatch sbatch_mpi_daint.sh` scripts (see [here](/software_install/#cuda-aware_mpi_on_piz_daint)) using CUDA-aware MPI üöÄ"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The final 2D slice (at `ny_g()/2`) produced should look as following and take about 25min to run:\n",
    "\n",
    "![3D porous convection MPI](./figures/l9_porous_convect_mpi_sl.png)"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3D calculation\n",
    "Running the code at higher resolution (`508x252x252` grid points) and for 6000 timesteps produces the following result"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center>\n",
    "  <video width=\"90%\" autoplay loop controls src=\"./figures/l9_porous_convection_mxpu.mp4\"/>\n",
    "</center>"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "kernelspec": {
   "name": "julia-1.8",
   "display_name": "Julia 1.8.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
