{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "_Lecture 9_\n",
    "# Projects - 3D thermal porous convection on multi-xPU"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The goal of this lecture 9:\n",
    "\n",
    "- Projects\n",
    "    - Create a multi-xPU version of the 3D thermal porous convection xPU code\n",
    "    - Combine [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl) and [ParallelStencil.jl](https://github.com/omlins/ParallelStencil.jl)\n",
    "    - Finalise the documentation of your project\n",
    "- Automatic documentation and CI"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using `ImplicitGlobalGrid.jl` (continued)\n",
    "\n",
    "In previous Lecture 8, we introduced [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl), which renders distributed parallelisation with GPU and CPU for HPC a very simple task.\n",
    "\n",
    "Also, ImplicitGlobalGrid.jl elegantly combines with [ParallelStencil.jl](https://github.com/omlins/ParallelStencil.jl) to, e.g., hide communication behind computation."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a rapid tour of [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl)'s' documentation before using it to turn the 3D thermal porous diffusion solver into a multi-xPU solver."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-xPU 3D thermal porous convection\n",
    "\n",
    "Let's step through the following content:\n",
    "- Create a multi-xPU version of your thermal porous convection 3D xPU code you finalised in lecture 7\n",
    "- Keep it xPU compatible using `ParallelStencil.jl`\n",
    "- Deploy it on multiple xPUs using `ImplicitGlobalGrid.jl`\n",
    "\n",
    "üëâ You'll find a version of the `PorousConvection_3D_xpu.jl` code in the solutions folder on Moodle after exercises deadline if needed to get you started."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Enable multi-xPU support\n",
    "Only a few changes are required to enable multi-xPU support, namely:\n",
    "\n",
    "1. Copy your working `PorousConvection_3D_xpu.jl` code developed for the exercises in Lecture 7 and rename it `PorousConvection_3D_multixpu.jl`."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Add at the beginning of the code"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ImplicitGlobalGrid\n",
    "import MPI"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Further, add global maximum computation using MPI reduction function to be used instead of `maximum()`"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "max_g(A) = (max_l = maximum(A); MPI.Allreduce(max_l, MPI.MAX, MPI.COMM_WORLD))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. In the `# numerics` section, initialise the global grid right after defining `nx,ny,nz` and use now global grid `nx_g()`,`ny_g()` and `nz_g()` for defining `maxiter` and `ncheck`, as well as in any other places when needed."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nx,ny       = 2 * (nz + 1) - 1, nz\n",
    "me, dims    = init_global_grid(nx, ny, nz)  # init global grid and more\n",
    "b_width     = (8, 8, 4)                     # for comm / comp overlap"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Modify the temperature initialisation using ImplicitGlobalGrid's global coordinate helpers (`x_g`, etc...), including one internal boundary condition update (update halo):"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "T  = @zeros(nx, ny, nz)\n",
    "T .= Data.Array([ŒîT * exp(-(x_g(ix, dx, T) + dx / 2 - lx / 2)^2\n",
    "                          -(y_g(iy, dy, T) + dy / 2 - ly / 2)^2\n",
    "                          -(z_g(iz, dz, T) + dz / 2 - lz / 2)^2) for ix = 1:size(T, 1), iy = 1:size(T, 2), iz = 1:size(T, 3)])\n",
    "T[:, :, 1  ] .=  ŒîT / 2\n",
    "T[:, :, end] .= -ŒîT / 2\n",
    "update_halo!(T)\n",
    "T_old = copy(T)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Prepare for visualisation, making sure only `me==0` creates the output directory. Also, prepare an array for storing inner points only (no halo) `T_inn` as well as global array to gather subdomains `T_v`"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if do_viz\n",
    "    ENV[\"GKSwstype\"]=\"nul\"\n",
    "    if (me==0) if isdir(\"viz3Dmpi_out\")==false mkdir(\"viz3Dmpi_out\") end; loadpath=\"viz3Dmpi_out/\"; anim=Animation(loadpath,String[]); println(\"Animation directory: $(anim.dir)\") end\n",
    "    nx_v, ny_v, nz_v = (nx - 2) * dims[1], (ny - 2) * dims[2], (nz - 2) * dims[3]\n",
    "    (nx_v * ny_v * nz_v * sizeof(Data.Number) > 0.8 * Sys.free_memory()) && error(\"Not enough memory for visualization.\")\n",
    "    T_v   = zeros(nx_v, ny_v, nz_v) # global array for visu\n",
    "    T_inn = zeros(nx - 2, ny - 2, nz - 2) # no halo local array for visu\n",
    "    xi_g, zi_g = LinRange(-lx / 2 + dx + dx / 2, lx / 2 - dx - dx / 2, nx_v), LinRange(-lz + dz + dz / 2, -dz - dz / 2, nz_v) # inner points only\n",
    "    iframe = 0\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Moving to the time loop, add halo update function `update_halo!` after the kernel that computes the fluid fluxes. You can additionally wrap it in the `@hide_communication` block to enable communication/computation overlap (using `b_width` defined above)"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@hide_communication b_width begin\n",
    "    @parallel compute_Dflux!(qDx, qDy, qDz, Pf, T, k_Œ∑f, _dx, _dy, _dz, Œ±œÅg, _1_Œ∏_dœÑ_D)\n",
    "    update_halo!(qDx, qDy, qDz)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. Apply a similar step to the temperature update, where you can also include boundary condition computation as following (‚ö†Ô∏è no other construct is currently allowed)"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@hide_communication b_width begin\n",
    "    @parallel update_T!(T, qTx, qTy, qTz, dTdt, _dx, _dy, _dz, _1_dt_Œ≤_dœÑ_T)\n",
    "    @parallel (1:size(T, 2), 1:size(T, 3)) bc_x!(T)\n",
    "    @parallel (1:size(T, 1), 1:size(T, 3)) bc_y!(T)\n",
    "    update_halo!(T)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. Use now the `max_g` function instead of `maximum` to collect the global maximum among all local arrays spanning all MPI processes. Use it in the timestep `dt` definition and in the error calculation (instead of `maximum`)."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# time step\n",
    "dt = if it == 1\n",
    "    0.1 * min(dx, dy, dz) / (Œ±œÅg * ŒîT * k_Œ∑f)\n",
    "else\n",
    "    min(5.0 * min(dx, dy, dz) / (Œ±œÅg * ŒîT * k_Œ∑f), œï * min(dx / max_g(abs.(qDx)), dy / max_g(abs.(qDy)), dz / max_g(abs.(qDz))) / 3.1)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Make sure all printing statements are only executed by `me==0` in order to avoid each MPI process to print to screen, and use `nx_g()` instead of local `nx` in the printed statements when assessing the iteration per number of grid points."
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "11. Update the visualisation and output saving part"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# visualisation\n",
    "if do_viz && (it % nvis == 0)\n",
    "    T_inn .= Array(T)[2:end-1, 2:end-1, 2:end-1]; gather!(T_inn, T_v)\n",
    "    if me == 0\n",
    "        p1 = heatmap(xi_g, zi_g, T_v[:, ceil(Int, ny_g() / 2), :]'; xlims=(xi_g[1], xi_g[end]), ylims=(zi_g[1], zi_g[end]), aspect_ratio=1, c=:turbo)\n",
    "        # display(p1)\n",
    "        png(p1, @sprintf(\"viz3Dmpi_out/%04d.png\", iframe += 1))\n",
    "        save_array(@sprintf(\"viz3Dmpi_out/out_T_%04d\", iframe), convert.(Float32, T_v))\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "12. Finalise the global grid before returning from the main function"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "finalize_global_grid()\n",
    "return"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you made it up to here, you should now be able to launch your `PorousConvection_3D_multixpu.jl` code on multiple GPUs. Let's give it a try üî•"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure to have set following parameters:"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "fragment"
    }
   }
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lx,ly,lz    = 40.0, 20.0, 20.0\n",
    "Ra          = 1000\n",
    "nz          = 63\n",
    "nx,ny       = 2 * (nz + 1) - 1, nz\n",
    "b_width     = (8, 8, 4) # for comm / comp overlap\n",
    "nt          = 500\n",
    "nvis        = 50"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Benchmark run\n",
    "Then, launch the script on Piz Daint on 8 GPU nodes upon adapting the the `runme_mpi_daint.sh` or `sbatch sbatch_mpi_daint.sh` scripts (see [here](/software_install/#cuda-aware_mpi_on_piz_daint)) üöÄ"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The final 2D slice (at `ny_g()/2`) produced should look as following and the code takes about 25min to run:\n",
    "\n",
    "![3D porous convection MPI](./figures/l9_porous_convect_mpi_sl.png)"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3D calculation\n",
    "Running the code at higher resolution (`508x252x252` grid points) and for 6000 timesteps produces the following result"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center>\n",
    "  <video width=\"90%\" autoplay loop controls src=\"./figures/l9_porous_convection_mxpu.mp4\"/>\n",
    "</center>"
   ],
   "metadata": {
    "name": "A slide ",
    "slideshow": {
     "slide_type": "slide"
    }
   }
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
