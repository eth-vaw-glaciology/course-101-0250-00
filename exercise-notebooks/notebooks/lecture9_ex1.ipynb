{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1 - **Multi-xPU computing projects**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of this exercise is to:\n",
    "- Further familiarise with distributed computing\n",
    "- Combine [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl) and [ParallelStencil.jl](https://github.com/omlins/ParallelStencil.jl)\n",
    "- Learn about GPU MPI on the way"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise, you will:\n",
    "- Create a multi-xPU version of the 3D thermal porous convection code from lecture 7\n",
    "- Keep it xPU compatible using `ParallelStencil.jl`\n",
    "- Deploy it on multiple xPUs using `ImplicitGlobalGrid.jl`\n",
    "\n",
    "ðŸ‘‰ You'll find a version of the `PorousConvection_3D_xpu.jl` code in the solutions folder on Polybox after exercises deadline if needed to get you started.\n",
    "\n",
    "1. Copy the `PorousConvection_3D_xpu.jl` code from exercises in Lecture 7 and rename it `PorousConvection_3D_multixpu.jl`.\n",
    "\n",
    "2. Refer to the steps outlined in the [Multi-xPU 3D thermal porous convection](#multi-xpu_3d_thermal_porous_convection) section from Lecture 9 to implement the changes needed to port the 3D single xPU code (from Lecture 7) to multi-xPU.\n",
    "\n",
    "3. Upon completion, verify the script converges and produces expected output for following parameters:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "lx,ly,lz    = 40.0,20.0,20.0\n",
    "Ra          = 1000\n",
    "nz          = 63\n",
    "nx,ny       = 2*(nz+1)-1,nz\n",
    "b_width     = (8,8,4) # for comm / comp overlap\n",
    "nt          = 500\n",
    "nvis        = 50"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use 8 GPUs on Piz Daint adapting the `runme_mpi_daint.sh` or `sbatch sbatch_mpi_daint.sh` scripts (see [here](/software_install/#cuda-aware_mpi_on_piz_daint)) to use CUDA-aware MPI ðŸš€\n",
    "\n",
    "The final 2D slice (at `ny_g()/2`) produced should look similar as the figure depicted in [Lecture 9](#benchmark_run).\n",
    "\n",
    "### Task\n",
    "\n",
    "Now that you made sure the code runs as expected, launch `PorousConvection_3D_multixpu.jl` for 2000 steps on 8 GPUs at higher resolution (global grid of `508x252x252`) setting:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nz          = 127\n",
    "nx,ny       = 2*(nz+1)-1,nz\n",
    "nt          = 2000\n",
    "nvis        = 100"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and keeping other parameters unchanged.\n",
    "\n",
    "Use `sbtach` command to launch a non-interactive job which may take about 5h30-6h to execute.\n",
    "\n",
    "Produce a figure or animation showing the final stage of temperature distribution in 3D and add it to a new section titled `## Porous convection 3D MPI` in the `PorousConvection` project subfolder's `README.md`. You can use the Makie visualisation helper script from Lecture 7 for this purpose (making sure to adapt the resolution and other input params if needed)."
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "kernelspec": {
   "name": "julia-1.8",
   "display_name": "Julia 1.8.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
