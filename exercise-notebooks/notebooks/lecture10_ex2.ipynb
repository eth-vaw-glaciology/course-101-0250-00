{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2 - **Advanced data transfer optimisations (part 1)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of this exercise is to:\n",
    "- learn how to use shared memory (on-chip) to avoid main memory accesses.\n",
    "\n",
    "Prerequisites:\n",
    "- the introduction notebook *Benchmarking memory copy and establishing peak memory access performance* ([`l6_1-gpu-memcopy.ipynb`](https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/slide-notebooks/notebooks/l6_1-gpu-memcopy.ipynb))\n",
    "- the *Data transfer optimisation notebook* ([`lecture6_ex1.ipynb`](https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/exercise-notebooks/notebooks/lecture6_ex1.ipynb))\n",
    "\n",
    "[*This content is distributed under MIT licence. Authors: S. Omlin (CSCS), L. RÃ¤ss (ETHZ).*](https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/LICENSE.md)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting started\n",
    "\n",
    "ðŸ‘‰ Download the [`lecture10_ex2.ipynb`](https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/exercise-notebooks/notebooks/lecture10_ex2.ipynb) notebook and edit it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will again use the packages `CUDA`, `BenchmarkTools` and `Plots` to create a little performance laboratory:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "] activate ."
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "] instantiate"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using CUDA\n",
    "using BenchmarkTools\n",
    "using Plots"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us consider the same 2-D heat diffusion solver as in the second part of the first *Data transfer optimisation notebook* ([`lecture6_ex1.ipynb`](https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/exercise-notebooks/notebooks/lecture6_ex1.ipynb)):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function diffusion2D()\n",
    "    # Physics\n",
    "    lam      = 1.0                                          # Thermal conductivity\n",
    "    c0       = 2.0                                          # Heat capacity\n",
    "    lx, ly   = 10.0, 10.0                                   # Length of computational domain in dimension x and y\n",
    "\n",
    "    # Numerics\n",
    "    nx, ny   = 32*2, 32*2                                   # Number of gridpoints in dimensions x and y\n",
    "    nt       = 100                                          # Number of time steps\n",
    "    dx       = lx/(nx-1)                                    # Space step in x-dimension\n",
    "    dy       = ly/(ny-1)                                    # Space step in y-dimension\n",
    "    _dx, _dy = 1.0/dx, 1.0/dy\n",
    "\n",
    "    # Array initializations\n",
    "    T    = CUDA.zeros(Float64, nx, ny)                      # Temperature\n",
    "    T2   = CUDA.zeros(Float64, nx, ny)                      # 2nd array for Temperature\n",
    "    Ci   = CUDA.zeros(Float64, nx, ny)                      # 1/Heat capacity\n",
    "\n",
    "    # Initial conditions\n",
    "    Ci .= 1/c0                                              # 1/Heat capacity (could vary in space)\n",
    "    T  .= CuArray([10.0*exp(-(((ix-1)*dx-lx/2)/2)^2-(((iy-1)*dy-ly/2)/2)^2) for ix=1:size(T,1), iy=1:size(T,2)]) # Initialization of Gaussian temperature anomaly\n",
    "    T2 .= T;                                                 # Assign also T2 to get correct boundary conditions.\n",
    "\n",
    "    # Time loop\n",
    "    dt  = min(dx^2,dy^2)/lam/maximum(Ci)/4.1                # Time step for 2D Heat diffusion\n",
    "    opts = (aspect_ratio=1, xlims=(1, nx), ylims=(1, ny), clims=(0.0, 10.0), c=:davos, xlabel=\"Lx\", ylabel=\"Ly\") # plotting options\n",
    "    @gif for it = 1:nt\n",
    "        diffusion2D_step!(T2, T, Ci, lam, dt, _dx, _dy)     # Diffusion time step.\n",
    "        heatmap(Array(T)'; opts...)                         # Visualization\n",
    "        T, T2 = T2, T                                       # Swap the aliases T and T2 (does not perform any array copy)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function diffusion2D_step!(T2, T, Ci, lam, dt, _dx, _dy)\n",
    "    threads = (32, 8)\n",
    "    blocks  = (size(T2,1)Ã·threads[1], size(T2,2)Ã·threads[2])\n",
    "    @cuda blocks=blocks threads=threads update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    if (ix>1 && ix<size(T2,1) && iy>1 && iy<size(T2,2))\n",
    "        @inbounds T2[ix,iy] = T[ix,iy] + dt*Ci[ix,iy]*(\n",
    "                              - ((-lam*(T[ix+1,iy] - T[ix,iy])*_dx) - (-lam*(T[ix,iy] - T[ix-1,iy])*_dx))*_dx\n",
    "                              - ((-lam*(T[ix,iy+1] - T[ix,iy])*_dy) - (-lam*(T[ix,iy] - T[ix,iy-1])*_dy))*_dy\n",
    "                              )\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, for benchmarking activities, we will require again the following arrays and scalars (use again the `nx=ny` found best in the introduction notebook; you can modify the value if it is not right for you):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nx = ny = 512*32\n",
    "T    = CUDA.rand(Float64, nx, ny);\n",
    "T2   = CUDA.rand(Float64, nx, ny);\n",
    "Ci   = CUDA.rand(Float64, nx, ny);\n",
    "lam = _dx = _dy = dt = rand();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the introduction notebook, we determined how the performance of memory copy behaved with in function of the number of threads per blocks. We will do the same now for the temperature update kernel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ðŸ’¡ Hint: Make sure to have no other notebook **kernel** running; array sizes are close to device DRAM max and you may get an out-of-mem error otherwise."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1 (Performance evaluation)\n",
    "\n",
    "Determine the effective memory throughput, `T_eff`, of the kernel `update_temperature!` in function of the number of threads, fixing the number of threads in x dimension to 32.\n",
    "> ðŸ’¡ Hint: you can base yourself on the corresponding activity in the introduction notebook (remember to compute now `T_eff` rather than `T_tot`)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# solution\n",
    "max_threads  = attribute(device(),CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)\n",
    "thread_count = []\n",
    "throughputs  = []\n",
    "for pow = 0:Int(log2(max_threads/32))\n",
    "    threads = (32, 2^pow)\n",
    "    blocks  = #...\n",
    "    t_it = @belapsed begin @cuda #...\n",
    "    T_eff = #...\n",
    "    push!(thread_count, prod(threads))\n",
    "    push!(throughputs, T_eff)\n",
    "    println(\"(threads=$threads) T_eff = $(T_eff)\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the best thread/block configuration measured for reusing it later (adapt the code if your variable names above do not match):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "T_tot_max, index = findmax(throughputs)\n",
    "threads = (32, thread_count[index]Ã·32)\n",
    "blocks  = (nxÃ·threads[1], nyÃ·threads[2])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You could probably observe that this kernel is more sensitive to the thread/block configuration than the memory copy kernel. The reason is that the thread/block configuration directly influences the way the fast memory situated on-chip (here high-level cache and registers) is used in order to avoid redundant main memory accesses. We will now explicitly control part of the the on-chip memory usage, using so called \"shared memory\", which is repurposed high-level cache. This will give some insights on how certain parameters relate to on-chip memory usage. However, we will not implement a diffusion kernel with shared memory at once, but in little steps.\n",
    "\n",
    "Let us start with relating the `update_temperature!` kernel back to the triad memory copy kernel investigated in the introduction notebook. We can observe that if we remove the derivatives from the `update_temperature!` kernel then we end up with a simple triad memory copy kernel, except for an additional if-statement to avoid updating the boundary values (for simplicity, we do not remove the unused function arguments which we will use again in the next experiments):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    if (ix>1 && ix<size(T2,1) && iy>1 && iy<size(T2,2))\n",
    "        @inbounds T2[ix,iy] = T[ix,iy] + dt*Ci[ix,iy]\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This kernel should consequently achieve a `T_tot` of essentially the value of `T_peak` (if an optimal thread/block configuration is used). Moreover, for this case `T_eff = T_tot`. Let us verify quickly that `T_eff` is essentially equal `T_peak` here (measured 561 GB/s with the Tesla P100 GPU):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads update_temperature!($T2, $T, $Ci, $lam, $dt, $_dx, $_dy); synchronize() end\n",
    "T_eff = (2*1+1)*1/1e9*nx*ny*sizeof(Float64)/t_it"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will do now our first shared memory experiment with this simple triad kernel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2 (Shared memory basics)\n",
    "\n",
    "Modify the above `update_temperature!` kernel (which now does just triad memory copy) as follows: read the values of the temperature array `T` into shared memory; then, subsequently, read the temperature values from there when updating `T2`.\n",
    "To help you, the structure of the kernel is already given; you only need to complete the unfinished lines.\n",
    "> ðŸ’¡ Hint: use [`@cuDynamicSharedMem`](https://juliagpu.gitlab.io/CUDA.jl/api/kernel/#CUDA.@cuDynamicSharedMem) to allocate the required shared memory\n",
    ">\n",
    "> ðŸ’¡ Hint: shared memory is block-local, i.e., shared between the threads of a same block.\n",
    ">\n",
    "> ðŸ’¡ Note that shared memory as well as registers are a very limited resource and the amount a kernel needs increases normally with the number of threads launched per block. As a result, the maximum number of threads launchable per block can be restricted by the needed on-chip resources to a value less than the general limit of the device (attribute `CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK`). The CUDA occupancy API lets query the maximum number of threads possible for a given kernel (see [`maxthreads`](https://cuda.juliagpu.org/stable/api/compiler/#CUDA.maxthreads))."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# hint\n",
    "function update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    tx = # local thread id, x dimension\n",
    "    ty = # local thread id, y dimension\n",
    "    T_l = # allocation of a block-local temperature array (in shared memory)\n",
    "    @inbounds T_l[tx,ty] = # read the values of the temperature array `T` into shared memory\n",
    "    if (ix>1 && ix<size(T2,1) && iy>1 && iy<size(T2,2))\n",
    "        @inbounds T2[ix,iy] = #=read temperature values from shared memory=#  + dt*Ci[ix,iy]\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 3 (Shared memory basics)\n",
    "\n",
    "Launch the kernel requesting the required amount of shared memory; compute the `T_eff` achieved.\n",
    "> ðŸ’¡ Hint: the `@cuda` macro supports the keyword `shmem` to request the required amount of shared memory; note that it must be indicated in bytes (use sizeof() to get the number of bytes used by the datatype used)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# solution"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should not observe any significant change in `T_eff` compared to the previous kernel (measured as before 561 GB/s with the Tesla P100 GPU).\n",
    "\n",
    "When we will add back the derivatives later, then each thread will read values on the left, right, bottom and top of it. We will want the threads to read the temperature values from the block-local array `T_l`, not from `T` anymore. However, right now each thread maps directly to a cell of `T_l`; thus, the threads at the boundary of the block would read out-of-bounds when reading the \"neighbour cells\". We therefore need to add a \"halo\" to `T_l` that will contain the required values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 4 (Shared memory)\n",
    "\n",
    "Modify the `update_temperature!` kernel from Task 2 as follows: add a \"halo\" of size `1` to `T_l` on each side, i.e. on the left, right, bottom and top. To this purpose, you need to modify the allocation of `T_l` and adapt the local thread ids `tx` and `ty` accordingly. Then, launch the new kernel adjusting the required amount of shared memory and compute `T_eff`.\n",
    "To help you, the structure of the kernel is already given; you only need to complete the unfinished lines."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# hint\n",
    "function update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    tx =  # adjust the local thread id in y dimension\n",
    "    ty =  # adjust the local thread id in y dimension\n",
    "    T_l = # adjust the shared memory allocation\n",
    "    @inbounds T_l[tx,ty] = T[ix,iy]\n",
    "    if (ix>1 && ix<size(T2,1) && iy>1 && iy<size(T2,2))\n",
    "        @inbounds T2[ix,iy] = T_l[tx,ty] + dt*Ci[ix,iy]\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads shmem=#=adjust the shared memory=# update_temperature!($T2, $T, $Ci, $lam, $dt, $_dx, $_dy); synchronize() end\n",
    "T_eff = (2*1+1)*1/1e9*nx*ny*sizeof(Float64)/t_it"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`T_eff` did certainly not significantly change, as you probably expected as we did not access more data than before (measured as before 561 GB/s with the Tesla P100 GPU)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 5 (Shared memory)\n",
    "\n",
    "Modify the `update_temperature!` kernel from Task 4 as follows: read the required values into the newly added halo of `T_l`. Then, compute again `T_eff`.\n",
    "To help you, the structure of the kernel is already given; you only need to complete the unfinished lines."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# hint\n",
    "function update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    tx = threadIdx().x+1\n",
    "    ty = threadIdx().y+1\n",
    "    T_l = @cuDynamicSharedMem(eltype(T), (blockDim().x+2, blockDim().y+2))\n",
    "    @inbounds T_l[tx,ty] = T[ix,iy]\n",
    "    if (ix>1 && ix<size(T2,1) && iy>1 && iy<size(T2,2))\n",
    "        @inbounds if (threadIdx().x == 1)            #=read the required values to the left halo of `T_l`=# end\n",
    "        @inbounds if (threadIdx().x == blockDim().x) #=read the required values to the right halo of `T_l`=# end\n",
    "        @inbounds if                                 #=read the required values to the bottom halo of `T_l`=# end\n",
    "        @inbounds if                                 #=read the required values to the top halo of `T_l`=# end\n",
    "        @inbounds T2[ix,iy] = T_l[tx,ty] + dt*Ci[ix,iy]\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads shmem=prod($threads.+2)*sizeof(Float64) update_temperature!($T2, $T, $Ci, $lam, $dt, $_dx, $_dy); synchronize() end\n",
    "T_eff = (2*1+1)*1/1e9*nx*ny*sizeof(Float64)/t_it"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`T_eff` certainly decreased a bit due to the additional read-in of the halo of `T_l` (measured 538 GB/s with the Tesla P100 GPU), except if the compiler would have understood that the halo is never used and therefore never done these additional reads. In order to create the 2-D diffusion kernel using shared memory, the last step is to add back the derivatives."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 6 (Shared memory)\n",
    "\n",
    "Modify the `update_temperature!` kernel from Task 5 as follows: add back the derivatives that we removed at the beginning of the notebook and modify them to read the temperature from `T_l` rather then from `T`. Then, verify that the diffusion works as expected and compute again `T_eff`.\n",
    "\n",
    "To help you, the structure of the kernel is already given; you only need to complete the unfinished lines."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# hint\n",
    "function update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)\n",
    "    ix = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y-1) * blockDim().y + threadIdx().y\n",
    "    tx = threadIdx().x+1\n",
    "    ty = threadIdx().y+1\n",
    "    T_l = @cuDynamicSharedMem(eltype(T), (blockDim().x+2, blockDim().y+2))\n",
    "    @inbounds T_l[tx,ty] = T[ix,iy]\n",
    "    if (ix>1 && ix<size(T2,1) && iy>1 && iy<size(T2,2))\n",
    "        @inbounds if (threadIdx().x == 1)            T_l[tx-1,ty] = T[ix-1,iy] end\n",
    "        @inbounds if (threadIdx().x == blockDim().x) T_l[tx+1,ty] = T[ix+1,iy] end\n",
    "        @inbounds if (threadIdx().y == 1)            T_l[tx,ty-1] = T[ix,iy-1] end\n",
    "        @inbounds if (threadIdx().y == blockDim().y) T_l[tx,ty+1] = T[ix,iy+1] end\n",
    "        sync_threads()\n",
    "        @inbounds T2[ix,iy] = T_l[tx,ty] + dt*Ci[ix,iy]*(\n",
    "                    # add the computation of the derivatives\n",
    "                    # ...\n",
    "                    )\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "diffusion2D()\n",
    "\n",
    "t_it = @belapsed begin @cuda blocks=$blocks threads=$threads shmem=prod($threads.+2)*sizeof(Float64) update_temperature!($T2, $T, $Ci, $lam, $dt, $_dx, $_dy); synchronize() end\n",
    "T_eff = (2*1+1)*1/1e9*nx*ny*sizeof(Float64)/t_it"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ðŸ’¡ Note that the we have added a call to `sync_threads()` at the end of all reads into shared memory (i.e. `T_l`) in order to ensure that no thread tries to read a from a \"neighboring cell\" before it contains the required value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`T_eff` should not have decreased significantly when adding back the derivatives (measured, as in Task 5, 538 GB/s with the Tesla P100 GPU) even though they constitute the major part of the computations! This confirms one more time empirically that the performance of solvers as the above is essentially defined by how much we can avoid redundant main memory accesses."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 7 (Performance evaluation)\n",
    "\n",
    "Compute by how much percent you can improve the performance of the solver at most."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# solution"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congratulations! You have implemented a 2-D diffusion solver using shared memory!"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.5",
   "language": "julia"
  }
 },
 "nbformat": 4
}
