<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <script src="/libs/lunr/lunr.min.js"></script> <script src="/libs/lunr/lunr_index.js"></script> <script src="/libs/lunr/lunrclient.min.js"></script> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/poole_hyde.css"> <link rel=stylesheet  href="/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=icon  href="/assets/favicon.png"> <title>Lecture 7</title> <style> .content {max-width: 50rem} </style> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/assets/vaw_logo.png" style="width: 180px; height: auto; display: inline"> <div style="font-weight: margin-bottom: 0.5em"><a href="/"> Fall 2022</a> <span style="opacity: 0.7;">| <a href="http://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2022W&ansicht=KATALOGDATEN&lerneinheitId=162403&lang=en"> ETHZ 101-0250-00</a></span></div> <br> <h1><a href="/">Solving partial differential equations in parallel on GPUs</a></h1> <div style="line-height:18px; font-size: 15px; opacity: 0.85">by &nbsp; <a href="https://vaw.ethz.ch/en/people/person-detail.MjcwOTYw.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ludovic R√§ss</a>, &nbsp; <a href="https://vaw.ethz.ch/en/personen/person-detail.html?persid=124402">Mauro Werder</a>, &nbsp; <a href="https://www.cscs.ch/about/staff/">Samuel Omlin</a> & <br> <a href="https://vaw.ethz.ch/en/people/person-detail.MzAwMjIy.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ivan Utkin</a> </div> </div> <br> <style> </style> <nav class=sidebar-nav  style="opacity: 0.9; margin-bottom: 1.2cm;"> <a class="sidebar-nav-item " href="/"><b>Welcome</b></a> <a class="sidebar-nav-item " href="/logistics/">Logistics</a> <a class="sidebar-nav-item " href="/homework/">Homework</a> <a class="sidebar-nav-item " href="/software_install/">Software install</a> <a class="sidebar-nav-item " href="/extras/">Extras</a> <br> <div class=course-section >Part 1 - Introduction</div> <a class="sidebar-nav-item " href="/lecture1/">Lecture 1 - Why Julia GPU</a> <a class="sidebar-nav-item " href="/lecture2/">Lecture 2 - PDEs & physical processes</a> <a class="sidebar-nav-item " href="/lecture3/">Lecture 3 - Solving elliptic PDEs</a> <div class=course-section >Part 2 - Solving PDEs on GPUs</div> <a class="sidebar-nav-item " href="/lecture4/">Lecture 4 - Porous convection</a> <a class="sidebar-nav-item " href="/lecture5/">Lecture 5 - Parallel computing</a> <a class="sidebar-nav-item " href="/lecture6/">Lecture 6 - GPU computing</a> <div class=course-section >Part 3 - Multi-GPU computing (projects)</div> <a class="sidebar-nav-item active" href="/lecture7/">Lecture 7 - xPU computing</a> <a class="sidebar-nav-item " href="/lecture8/">Lecture 8 - Julia MPI & multi-xPU</a> <a class="sidebar-nav-item " href="/lecture9/">Lecture 9 - Advanced optimisations</a> <div class=course-section >Final Projects</div> <a class="sidebar-nav-item " href="/lecture10/">Lecture 10 - Final projects</a> </nav> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/search/index.html"> </form> <br> <br> </div> </div> <div class="content container"> <div class=franklin-content > <h1 id=lecture_7 ><a href="#lecture_7" class=header-anchor >Lecture 7</a></h1> <blockquote> <p><strong>Agenda</strong><br />üìö The &quot;two-language problem&quot;, <code>ParallelStencil.jl</code> xPU implementation<br />üíª Reference testing, GitHub CI and workflows<br />üöß Exercises - &#40;Project 1&#41;:</p> <ul> <li><p>xPU codes for 2D thermal porous convection</p> <li><p>2D and 3D xPU implementation</p> <li><p>CI workflows</p> </ul> </blockquote> <hr /> <p><a id=content  class=anchor ></a> <strong>Content</strong></p> <div class=franklin-toc ><ol><li><a href="#lecture_7">Lecture 7</a><li><a href="#julia_xpu_the_two-language_solution">Julia xPU: the two-language solution</a><ol><li><a href="#the_two-language_problem">The two-language problem</a><li><a href="#backend_portable_xpu_implementation">Backend portable xPU implementation</a><li><a href="#towards_3d_thermal_porous_convection">Towards 3D thermal porous convection</a></ol><li><a href="#continuous_integration_ci_and_github_actions">Continuous Integration &#40;CI&#41; and GitHub Actions</a><li><a href="#exercises_-_lecture_7">Exercises - lecture 7</a><ol><li><a href="#infos_about_projects">Infos about projects</a><li><a href="#exercise_1_-_2d_thermal_porous_convection_xpu_implementation">Exercise 1 - <strong>2D thermal porous convection xPU implementation</strong></a><li><a href="#exercise_2_-_3d_thermal_porous_convection_xpu_implementation">Exercise 2 - <strong>3D thermal porous convection xPU implementation</strong></a><li><a href="#exercise_3_-_ci_and_github_actions">Exercise 3 - <strong>CI and GitHub Actions</strong></a></ol></ol></div> <p><a href="#exercises_-_lecture_7"><em>üëâ get started with exercises</em></a></p> <hr /> <h1 id=julia_xpu_the_two-language_solution ><a href="#julia_xpu_the_two-language_solution" class=header-anchor >Julia xPU: the two-language solution</a></h1> <h3 id=the_goal_of_this_lecture_7 ><a href="#the_goal_of_this_lecture_7" class=header-anchor >The goal of this lecture 7:</a></h3> <ul> <li><p>Address the <strong><em>two-language problem</em></strong></p> <li><p>Backend portable xPU implementation</p> <li><p>Towards 3D porous convection</p> <li><p>Reference testing, GitHub CI and workflows</p> </ul> <h2 id=the_two-language_problem ><a href="#the_two-language_problem" class=header-anchor >The two-language problem</a></h2> <p>Combining CPU and GPU implementation within a single code.</p> <p>You may certainly be familiar with this situation in scientific computing:</p> <p><img src="../assets/literate_figures/l7_2lang_1.png" alt="two-lang problem" /></p> <p>Which may turn out into a costly cycle:</p> <p><img src="../assets/literate_figures/l7_2lang_2.png" alt="two-lang problem" /></p> <p>This situation is referred to as the <strong><em>two-language problem</em></strong>.</p> <p>Multi-language/software environment leads to:</p> <ul> <li><p>Translation errors</p> <li><p>Large development time &#40;overhead&#41;</p> <li><p>Non-portable solutions</p> </ul> <p>Good news&#33; Julia is a perfect candidate to solve the <strong><em>two-language problem</em></strong> as Julia code is:</p> <ul> <li><p><strong><em>simple</em></strong>, high-level, interactive &#40;low development costs&#41;</p> <li><p><strong><em>fast</em></strong>, compiled just ahead of time &#40;before one uses it for the first time&#41;</p> </ul> <div class=img-med ><img src="../assets/literate_figures/l7_2lang_3.png" alt="two-lang problem" /></div> <p>Julia provides a <strong><em>portable</em></strong> solution in many aspects &#40;beyond performance portability&#41;.</p> <p>As you may have started to experience, GPUs deliver great performance but may not be present in every laptop or workstation. Also, powerful GPUs require to be hosted in servers, especially when multiple GPUs are needed to perform high-resolution calculations.</p> <p>Wouldn&#39;t it be great to have <strong>single code that both executes on CPU and GPU?</strong></p> <blockquote> <p>Using the CPU &quot;backend&quot; for prototyping and debugging, and switching to the GPU &quot;backend&quot; for production purpose.</p> </blockquote> <p>Wouldn&#39;t it be great? ... <strong>YES</strong>, and there is a Julia solution&#33;</p> <div class=img-med ><img src="../assets/literate_figures/l7_ps_logo.png" alt=ParallelStencil  /></div> <h2 id=backend_portable_xpu_implementation ><a href="#backend_portable_xpu_implementation" class=header-anchor >Backend portable xPU implementation</a></h2> <p>Let&#39;s get started with <a href="https://github.com/omlins/ParallelStencil.jl">ParallelStencil.jl</a></p> <h3 id=getting_started_with_parallelstencil ><a href="#getting_started_with_parallelstencil" class=header-anchor >Getting started with ParallelStencil</a></h3> <p>ParallelStencil enables to:</p> <ul> <li><p>Write architecture-agnostic high-level code</p> <li><p>Parallel high-performance stencil computations on GPUs and CPUs</p> </ul> <p>ParallelStencil relies on the native kernel programming capabilities of:</p> <ul> <li><p><a href="https://cuda.juliagpu.org/stable/">CUDA.jl</a> for high-performance computations on Nvidia GPUs</p> <li><p><a href="https://docs.julialang.org/en/v1/base/multi-threading/#Base.Threads">Base.Threads</a> for high-performance computations on CPUs</p> <li><p>And <em>to be released soon</em> <a href="https://amdgpu.juliagpu.org/stable/">AMDGPU.jl</a> for high-performance computations on AMD GPUs</p> </ul> <h3 id=short_tour_of_parallelstencils_readme ><a href="#short_tour_of_parallelstencils_readme" class=header-anchor >Short tour of ParallelStencil&#39;s <code>README</code></a></h3> <p>Before we start our exercises, let&#39;s have a rapid tour of <a href="https://github.com/omlins/ParallelStencil.jl">ParallelStencil</a>&#39;s repo and <a href="https://github.com/omlins/ParallelStencil.jl"><code>README</code></a>.</p> <p><em>So, how does it work?</em></p> <p>As first hands-on for this lecture, let&#39;s <em><strong>merge</strong></em> the 2D fluid pressure diffusion solvers <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/"><code>diffusion_2D_perf_loop_fun.jl</code></a> and the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/"><code>diffusion_2D_perf_gpu.jl</code></a> into a single <em><strong>xPU</strong></em> code using ParallelStencil.</p> <div class=note ><div class=title >üí° Note</div> <div class=messg >Two approaches are possible &#40;we&#39;ll implement both&#41;. Parallelisation using stencil computations with 1&#41; math-close notation; 2&#41; more explicit kernel programming approach.</div></div> <h3 id=stencil_computations_with_math-close_notation ><a href="#stencil_computations_with_math-close_notation" class=header-anchor >Stencil computations with math-close notation</a></h3> <p>Let&#39;s get started with using the ParallelStencil.jl module and the <code>ParallelStencil.FiniteDifferences2D</code> submodule to enable math-close notation.</p> <p>üíª We&#39;ll start from the <code>Pf_diffusion_2D_perf_gpu.jl</code> &#40;available later in the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/">scripts/</a> folder in case you don&#39;t have it from lecture 6&#41; to create the <code>Pf_diffusion_2D_xpu.jl</code> script.</p> <p>The first step is to handle the packages:</p> <pre><code class="julia hljs"><span class=hljs-keyword >const</span> USE_GPU = <span class=hljs-literal >false</span>
<span class=hljs-keyword >using</span> ParallelStencil
<span class=hljs-keyword >using</span> ParallelStencil.FiniteDifferences2D
<span class=hljs-meta >@static</span> <span class=hljs-keyword >if</span> USE_GPU
    <span class=hljs-meta >@init_parallel_stencil</span>(CUDA, <span class=hljs-built_in >Float64</span>, <span class=hljs-number >2</span>)
<span class=hljs-keyword >else</span>
    <span class=hljs-meta >@init_parallel_stencil</span>(Threads, <span class=hljs-built_in >Float64</span>, <span class=hljs-number >2</span>)
<span class=hljs-keyword >end</span>
<span class=hljs-keyword >using</span> Plots,Plots.Measures,Printf</code></pre> <p>Then, we need to update the two compute functions , <code>compute_flux&#33;</code> and <code>update_Pf&#33;</code>.</p> <p>Let&#39;s start with <code>compute_flux&#33;</code>.</p> <p>ParallelStencil&#39;s <code>FiniteDifferences2D</code> submodule provides macros we need: <code>@inn_x&#40;&#41;</code>, <code>@inn_y&#40;&#41;</code>, <code>@d_xa&#40;&#41;</code>, <code>@d_ya&#40;&#41;</code>.</p> <p>The macros used in this example are described in the Module documentation callable from the Julia REPL / IJulia:</p> <pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class=language-julia > <span class=hljs-keyword >using</span> ParallelStencil.FiniteDifferences2D
</span>
<span class="hljs-meta prompt_">julia&gt;</span><span class=language-julia >?
</span>
help?&gt; @inn_x
  @inn_x(A): Select the inner elements of A in dimension x. Corresponds to A[2:end-1,:].</code></pre> <p>This would, e.g., give you more infos about the <code>@inn_x</code> macro.</p> <p>So, back to our compute function &#40;kernel&#41;. The <code>compute_flux&#33;</code> function gets the <code>@parallel</code> macro in its definition and returns nothing.</p> <p>Inside, we define the flux definition as following:</p> <pre><code class="julia hljs"><span class=hljs-meta >@parallel</span> <span class=hljs-keyword >function</span> compute_flux!(qDx,qDy,Pf,k_Œ∑f_dx,k_Œ∑f_dy,_1_Œ∏_dœÑ)
    <span class=hljs-meta >@inn_x</span>(qDx) = <span class=hljs-meta >@inn_x</span>(qDx) - (<span class=hljs-meta >@inn_x</span>(qDx) + k_Œ∑f_dx*<span class=hljs-meta >@d_xa</span>(Pf))*_1_Œ∏_dœÑ
    <span class=hljs-meta >@inn_y</span>(qDy) = <span class=hljs-meta >@inn_y</span>(qDy) - (<span class=hljs-meta >@inn_y</span>(qDy) + k_Œ∑f_dy*<span class=hljs-meta >@d_ya</span>(Pf))*_1_Œ∏_dœÑ
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span></code></pre> <p>Note that currently the shorthand <code>-&#61;</code> notation is not supported and we need to explicitly write out the equality. Now that we&#39;re done with <code>compute_flux&#33;</code>, your turn&#33;</p> <p>By analogy, update <code>update_Pf&#33;</code>.</p> <pre><code class="julia hljs"><span class=hljs-meta >@parallel</span> <span class=hljs-keyword >function</span> update_Pf!(Pf,qDx,qDy,_dx,_dy,_Œ≤_dœÑ)
    Pf = ...
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span></code></pre> <p>So far so good. We are done with the kernels. Let&#39;s see what changes are needed in the main part of the script.</p> <p>In the <code># numerics</code> section, <code>threads</code> and <code>blocks</code> are no longer needed; the kernel launch parameters being now automatically adapted:</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> Pf_diffusion_2D(;do_check=<span class=hljs-literal >false</span>)
    <span class=hljs-comment ># physics</span>
    <span class=hljs-comment ># [...]</span>
    <span class=hljs-comment ># numerics</span>
    nx, ny  = <span class=hljs-number >16</span>*<span class=hljs-number >32</span>, <span class=hljs-number >16</span>*<span class=hljs-number >32</span> <span class=hljs-comment ># number of grid points</span>
    maxiter = <span class=hljs-number >500</span>
    <span class=hljs-comment ># [...]</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span></code></pre> <p>In the <code># array initialisation</code> section, we need to wrap the Gaussian by <code>Data.Array</code> &#40;instead of <code>CuArray</code>&#41; and use the <code>@zeros</code> to initialise the other arrays:</p> <pre><code class="julia hljs"><span class=hljs-comment ># [...]</span>
<span class=hljs-comment ># array initialisation</span>
Pf      = Data.<span class=hljs-built_in >Array</span>( @. exp(-(xc-lx/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span> -(yc&#x27;-ly/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span>) )
qDx     = <span class=hljs-meta >@zeros</span>(nx+<span class=hljs-number >1</span>,ny  )
qDy     = <span class=hljs-meta >@zeros</span>(nx  ,ny+<span class=hljs-number >1</span>)
r_Pf    = <span class=hljs-meta >@zeros</span>(nx  ,ny  )
<span class=hljs-comment ># [...]</span></code></pre> <p>In the <code># iteration loop</code>, only the kernel call needs to be worked out. We can here re-use the single <code>@parallel</code> macro which now serves to launch the computations on the chosen backend:</p> <pre><code class="julia hljs"><span class=hljs-comment ># [...]</span>
<span class=hljs-comment ># iteration loop</span>
iter = <span class=hljs-number >1</span>; err_Pf = <span class=hljs-number >2</span>œµtol
t_tic = <span class=hljs-number >0.0</span>; niter = <span class=hljs-number >0</span>
<span class=hljs-keyword >while</span> err_Pf &gt;= œµtol &amp;&amp; iter &lt;= maxiter
    <span class=hljs-keyword >if</span> (iter==<span class=hljs-number >11</span>) t_tic = Base.time(); niter = <span class=hljs-number >0</span> <span class=hljs-keyword >end</span>
    <span class=hljs-meta >@parallel</span> compute_flux!(qDx,qDy,Pf,k_Œ∑f_dx,k_Œ∑f_dy,_1_Œ∏_dœÑ)
    <span class=hljs-meta >@parallel</span> update_Pf!(Pf,qDx,qDy,_dx,_dy,_Œ≤_dœÑ)
    <span class=hljs-keyword >if</span> do_check &amp;&amp; (iter%ncheck == <span class=hljs-number >0</span>)
        <span class=hljs-comment >#  [...]</span>
    <span class=hljs-keyword >end</span>
    iter += <span class=hljs-number >1</span>; niter += <span class=hljs-number >1</span>
<span class=hljs-keyword >end</span>
<span class=hljs-comment ># [...]</span></code></pre> <p>The performance evaluation section remaining unchanged, we are all set&#33;</p> <p><strong>Wrap-up tasks</strong></p> <ul> <li><p>Let&#39;s execute the code having the <code>USE_GPU &#61; false</code> flag set. We are running on multi-threading CPU backend with multi-threading enabled.</p> <li><p>Changing the <code>USE_GPU</code> flag to <code>true</code> &#40;having first relaunched a Julia session&#41; will make the application running on a GPU. On the GPU, you can reduce <code>ttot</code> and increase <code>nx, ny</code> in order achieve higher <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi mathvariant=normal >e</mi><mi mathvariant=normal >f</mi><mi mathvariant=normal >f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_\mathrm{eff}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span></span></span></span></span><span class=vlist-s >‚Äã</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</p> </ul> <div class=note ><div class=title >üí° Note</div> <div class=messg >Curious to see how it works under the hood? Feel free to <a href="https://github.com/omlins/ParallelStencil.jl/blob/cd59a5b0d1fd32ceaecbf7fc922ab87a24257781/src/ParallelKernel/parallel.jl#L263">explore the source code</a>. Another nice bit of open source software &#40;and the fact that Julia&#39;s meta programming rocks üöÄ&#41;.</div></div> <h3 id=stencil_computations_with_more_explicit_kernel_programming_approach ><a href="#stencil_computations_with_more_explicit_kernel_programming_approach" class=header-anchor >Stencil computations with more explicit kernel programming approach</a></h3> <p>ParallelStencil also allows for more explicit kernel programming, enabled by <code>@parallel_indices</code> kernel definitions. In style, the codes are closer to the initial plain GPU version we started from, <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/"><code>diffusion_2D_perf_gpu.jl</code></a>.</p> <p>As the macro name suggests, kernels defined using <code>@parallel_indices</code> allow for explicit indices handling within the kernel operations. This approach is <em><strong>currently</strong></em> slightly more performant than using <code>@parallel</code> kernel definitions.</p> <p>As second step, let&#39;s transform the <code>Pf_diffusion_2D_xpu.jl</code> into <code>Pf_diffusion_2D_perf_xpu.jl</code>.</p> <p>üíª We&#39;ll need bits from both <code>Pf_diffusion_2D_perf_gpu.jl</code> and <code>Pf_diffusion_2D_xpu.jl</code>.</p> <p>We can keep the package handling and initialisation identical to what we implemented in the <code>Pf_diffusion_2D_xpu.jl</code> script, but start again from the <code>Pf_diffusion_2D_perf_gpu.jl</code> script.</p> <p>Then, we can modify the <code>compute_flux&#33;</code> function definition from the <code>diffusion_2D_perf_gpu.jl</code> script, removing the <code>ix</code>, <code>iy</code> indices as those are now handled by ParallelStencil. The function definition takes however the <code>@parallel_indices</code> macro and the <code>&#40;ix,iy&#41;</code> tuple:</p> <pre><code class="julia hljs"><span class=hljs-keyword >macro</span> d_xa(A)  esc(:( $A[ix+<span class=hljs-number >1</span>,iy]-$A[ix,iy] )) <span class=hljs-keyword >end</span>
<span class=hljs-keyword >macro</span> d_ya(A)  esc(:( $A[ix,iy+<span class=hljs-number >1</span>]-$A[ix,iy] )) <span class=hljs-keyword >end</span>

<span class=hljs-meta >@parallel_indices</span> (ix,iy) <span class=hljs-keyword >function</span> compute_flux!(qDx,qDy,Pf,k_Œ∑f_dx,k_Œ∑f_dy,_1_Œ∏_dœÑ)
    nx,ny=size(Pf)
    <span class=hljs-keyword >if</span> (ix&lt;=nx-<span class=hljs-number >1</span> &amp;&amp; iy&lt;=ny  )  qDx[ix+<span class=hljs-number >1</span>,iy] -= (qDx[ix+<span class=hljs-number >1</span>,iy] + k_Œ∑f_dx*<span class=hljs-meta >@d_xa</span>(Pf))*_1_Œ∏_dœÑ  <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >if</span> (ix&lt;=nx   &amp;&amp; iy&lt;=ny-<span class=hljs-number >1</span>)  qDy[ix,iy+<span class=hljs-number >1</span>] -= (qDy[ix,iy+<span class=hljs-number >1</span>] + k_Œ∑f_dy*<span class=hljs-meta >@d_ya</span>(Pf))*_1_Œ∏_dœÑ  <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span></code></pre> <p>The <code># physics</code> section remains unchanged, and the <code># numerics section</code> is identical to the previous <code>xpu</code> script, i.e., no need for explicit block and thread definition.</p> <div class=warning ><div class=title >‚ö†Ô∏è Warning&#33;</div> <div class=messg >ParallelStencil computes the GPU kernel launch parameters based on optimal heuristics. Recalling lecture 6, multiple of 32 are most optimal; number of grid points should thus be chosen accordingly, i.e. as multiple of 32.</div></div> <p>We can then keep the scalar preprocessing in the <code># derived numerics</code> section.</p> <p>In the <code># array initialisation</code>, make sure to wrap the Gaussian by <code>Data.Array</code>, initialise zeros with the <code>@zeros</code> macro and remove information about precision &#40;<code>Float64</code>&#41;from there.</p> <p>The <code># iteration loop</code> remains concise; xPU kernels are launched here also with <code>@parallel</code> macro &#40;that implicitly includes <code>synchronize&#40;&#41;</code> statement&#41;:</p> <pre><code class="julia hljs"><span class=hljs-comment ># [...]</span>
<span class=hljs-comment ># iteration loop</span>
iter = <span class=hljs-number >1</span>; err_Pf = <span class=hljs-number >2</span>œµtol
t_tic = <span class=hljs-number >0.0</span>; niter = <span class=hljs-number >0</span>
<span class=hljs-keyword >while</span> err_Pf &gt;= œµtol &amp;&amp; iter &lt;= maxiter
    <span class=hljs-keyword >if</span> (iter==<span class=hljs-number >11</span>) t_tic = Base.time(); niter = <span class=hljs-number >0</span> <span class=hljs-keyword >end</span>
    <span class=hljs-meta >@parallel</span> compute_flux!(qDx,qDy,Pf,k_Œ∑f_dx,k_Œ∑f_dy,_1_Œ∏_dœÑ)
    <span class=hljs-meta >@parallel</span> update_Pf!(Pf,qDx,qDy,_dx,_dy,_Œ≤_dœÑ)
    <span class=hljs-keyword >if</span> do_check &amp;&amp; (iter%ncheck == <span class=hljs-number >0</span>)
        <span class=hljs-comment ># [...]</span>
    <span class=hljs-keyword >end</span>
    iter += <span class=hljs-number >1</span>; niter += <span class=hljs-number >1</span>
<span class=hljs-keyword >end</span>
<span class=hljs-comment ># [...]</span></code></pre> <p>Here we go üöÄ The <code>Pf_diffusion_2D_perf_xpu.jl</code> code is ready and should squeeze the performance out of your CPU or GPU, running as fast as the exclusive Julia multi-threaded or Julia GPU implementations, respectively.</p> <h3 id=multi-xpu_support ><a href="#multi-xpu_support" class=header-anchor >Multi-xPU support</a></h3> <p><em>What about multi-xPU support and distributed memory parallelisation?</em></p> <p>ParallelStencil is seamlessly interoperable with <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl"><code>ImplicitGlobalGrid.jl</code></a>, which enables distributed parallelisation of stencil-based xPU applications on a regular staggered grid and enables close to ideal weak scaling of real-world applications on thousands of GPUs.</p> <p>Moreover, ParallelStencil enables hiding communication behind computation with a simple macro call and without any particular restrictions on the package used for communication.</p> <p><em>This will be material for next lectures.</em></p> <div class=note ><div class=title >üí° Note</div> <div class=messg >Head to ParallelStencil&#39;s <a href="https://github.com/omlins/ParallelStencil.jl#concise-singlemulti-xpu-miniapps">miniapp section</a> if you are curious about various domain science applications featrued there.</div></div> <h2 id=towards_3d_thermal_porous_convection ><a href="#towards_3d_thermal_porous_convection" class=header-anchor >Towards 3D thermal porous convection</a></h2> <p>WIP</p> <p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p> <h1 id=continuous_integration_ci_and_github_actions ><a href="#continuous_integration_ci_and_github_actions" class=header-anchor >Continuous Integration &#40;CI&#41; and GitHub Actions</a></h1> <p>Last lecture we learned how to make and run tests for a Julia project.</p> <p>This lecture we will learn how to run those tests on GitHub automatically after you push to it. This will make sure that</p> <ul> <li><p>tests are always run</p> <li><p>you will be alerted by email when a test fails</p> </ul> <p><em>You may start to wonder why we&#39;re doing all of these tooling shenanigans...</em></p> <p>One requirement for the final project will be that it contains tests, which are run via GitHub Actions CI. Additionally, you&#39;ll have to write your project report as &quot;documentation&quot; for the package which could be deployed to its website, via GitHub Actions.</p> <p><strong>These days it is expected of good numerical software that it is well tested and documented.</strong></p> <h3 id=github_actions ><a href="#github_actions" class=header-anchor >GitHub Actions</a></h3> <p>GitHub Actions are a generic way to run computations when you interact with the repository. There is extensive <a href="https://docs.github.com/en/actions">documentation</a> for it &#40;no need for you to read it&#41;.</p> <p>For instance the course&#39;s <a href="https://pde-on-gpu.vaw.ethz.ch">website</a> is generated from the markdown input files upon pushing to the repo:</p> <ul> <li><p><a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/tree/main/website">https://github.com/eth-vaw-glaciology/course-101-0250-00/tree/main/website</a> contains the source</p> <li><p>the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/.github/workflows/Deploy.yml">https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/.github/workflows/Deploy.yml</a> is the GitHub Actions script which tells it to run Franklin.jl to</p> <li><p>create the website and deploy it on a specific URL <a href="https://pde-on-gpu.vaw.ethz.ch">https://pde-on-gpu.vaw.ethz.ch</a></p> </ul> <h3 id=github_actions_for_ci ><a href="#github_actions_for_ci" class=header-anchor >GitHub Actions for CI</a></h3> <p>How do we use GitHub Actions for CI?</p> <ol> <li><p>create a Julia project and add some tests</p> <li><p>make a suitable GitHub Actions scrip &#40;that <code>.yml</code> file&#41;</p> <li><p>pushing to GitHub will now run the tests &#40;maybe you need to activate Actions in <code>Setting</code> -&gt; <code>Actions</code> -&gt; <code>Allow all actions</code>&#41;</p> </ol> <div class=note ><div class=title >üí° Note</div> <div class=messg >There are other providers of CI, e.g. Travis, Appveyor, etc. Here we&#39;ll only look at GitHub actions.</div></div> <h4 id=example_from_last_lecture_continued ><a href="#example_from_last_lecture_continued" class=header-anchor >Example from last lecture continued</a></h4> <p>In the last lecture we&#39;ve setup a <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00-L6Testing.jl">project</a> to illustrate how unit-testing works.</p> <p>Let&#39;s now add CI to this:</p> <ol> <li><p>create a Julia project and add some tests <strong>&#91;done&#93;</strong></p> <li><p>make a suitable GitHub Actions scrip &#40;that <code>.yml</code> file&#41;</p> <li><p>pushing to GitHub will now run the tests &#40;maybe you need to activate Actions in <code>Setting</code> -&gt; <code>Actions</code> -&gt; <code>Allow all actions</code>&#41;</p> </ol> <p>For step 2 we follow the documentation on <a href="https://github.com/julia-actions/julia-runtest">https://github.com/julia-actions/julia-runtest</a>.</p> <div class=note ><div class=title >üí° Note</div> <div class=messg ><a href="https://github.com/invenia/PkgTemplates.jl">PkgTemplates.jl</a> is a handy package, which can generate a suitable Github Actions file.</div></div> <h4 id=example_from_last_lecture_continued_yml_magic ><a href="#example_from_last_lecture_continued_yml_magic" class=header-anchor >Example from last lecture continued: YML magic</a></h4> <p>The <code>.yml</code> file, adapted from the <code>README</code> of <a href="https://github.com/julia-actions/julia-runtest">julia-runtest</a>:</p> <pre><code class="yml hljs"><span class=hljs-attr >name:</span> <span class=hljs-string >Run</span> <span class=hljs-string >tests</span>

<span class=hljs-attr >on:</span> [<span class=hljs-string >push</span>, <span class=hljs-string >pull_request</span>]

<span class=hljs-attr >jobs:</span>
  <span class=hljs-attr >test:</span>
    <span class=hljs-attr >runs-on:</span> <span class=hljs-string >${{</span> <span class=hljs-string >matrix.os</span> <span class=hljs-string >}}</span>
    <span class=hljs-attr >strategy:</span>
      <span class=hljs-attr >matrix:</span>
        <span class=hljs-attr >julia-version:</span> [<span class=hljs-string >&#x27;1.8&#x27;</span>]
        <span class=hljs-attr >julia-arch:</span> [<span class=hljs-string >x64</span>]
        <span class=hljs-attr >os:</span> [<span class=hljs-string >ubuntu-latest</span>]

    <span class=hljs-attr >steps:</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >actions/checkout@v2</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >julia-actions/setup-julia@v1</span>
        <span class=hljs-attr >with:</span>
          <span class=hljs-attr >version:</span> <span class=hljs-string >${{</span> <span class=hljs-string >matrix.julia-version</span> <span class=hljs-string >}}</span>
          <span class=hljs-attr >arch:</span> <span class=hljs-string >${{</span> <span class=hljs-string >matrix.julia-arch</span> <span class=hljs-string >}}</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >julia-actions/julia-buildpkg@v1</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >julia-actions/julia-runtest@v1</span></code></pre> <h4 id=where_is_my_badge ><a href="#where_is_my_badge" class=header-anchor >Where is my BADGE&#33;&#33;&#33;</a></h4> <p>The CI will create a badge &#40;a small picture&#41; which reflects the status of the Action. Typically added to the <code>README.md</code>:</p> <p><img src="../assets/literate_figures/l7_ci-badge.png" alt=ci-badge  /></p> <p>It can be found under</p> <pre><code class="julia hljs">https://github.com/&lt;USER&gt;/&lt;REPO&gt;/actions/workflows/CI.yml/badge.svg</code></pre>
<p>and should be added to the near the top of <code>README</code> like so:</p>
<pre><code class="julia hljs">[![CI action](https://github.com/&lt;USER&gt;/&lt;REPO&gt;/actions/workflows/CI.yml/badge.svg)](https://github.com/&lt;USER&gt;/&lt;REPO&gt;/actions/workflows/CI.yml)</code></pre>
<p>&#40;this also sets the link to the Actions which gets open upon clicking on it&#41;</p>
<p>üëâ <em><strong>All together</strong></em> on <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00-L6Testing.jl">https://github.com/eth-vaw-glaciology/course-101-0250-00-L6Testing.jl</a></p>
<h4 id=wait_a_second_we_submit_our_homework_as_subfolders_of_our_github_repo ><a href="#wait_a_second_we_submit_our_homework_as_subfolders_of_our_github_repo" class=header-anchor >Wait a second, we submit our homework as subfolders of our GitHub repo...</a></h4>
<p>This makes the <code>.yml</code> a bit more complicated:</p>
<pre><code class="yml hljs"><span class=hljs-attr >name:</span> <span class=hljs-string >CI</span>
<span class=hljs-attr >on:</span>
  [<span class=hljs-string >push</span>, <span class=hljs-string >pull_request</span>]
<span class=hljs-attr >jobs:</span>
  <span class=hljs-attr >test:</span>
    <span class=hljs-attr >name:</span> <span class=hljs-string >Julia</span> <span class=hljs-string >${{</span> <span class=hljs-string >matrix.julia-version</span> <span class=hljs-string >}}</span> <span class=hljs-bullet >-</span> <span class=hljs-string >${{</span> <span class=hljs-string >matrix.os</span> <span class=hljs-string >}}</span> <span class=hljs-bullet >-</span> <span class=hljs-string >${{</span> <span class=hljs-string >matrix.julia-arch</span> <span class=hljs-string >}}</span> <span class=hljs-bullet >-</span> <span class=hljs-string >${{</span> <span class=hljs-string >github.event_name</span> <span class=hljs-string >}}</span>
    <span class=hljs-attr >runs-on:</span> <span class=hljs-string >${{</span> <span class=hljs-string >matrix.os</span> <span class=hljs-string >}}</span>
    <span class=hljs-attr >strategy:</span>
      <span class=hljs-attr >fail-fast:</span> <span class=hljs-literal >false</span>
      <span class=hljs-attr >matrix:</span>
        <span class=hljs-attr >julia-version:</span> [<span class=hljs-string >&#x27;1.8&#x27;</span>]
        <span class=hljs-attr >julia-arch:</span> [<span class=hljs-string >x64</span>]
        <span class=hljs-attr >os:</span> [<span class=hljs-string >ubuntu-latest</span>]
    <span class=hljs-attr >steps:</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >actions/checkout@v2</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >julia-actions/setup-julia@v1</span>
        <span class=hljs-attr >with:</span>
          <span class=hljs-attr >version:</span> <span class=hljs-string >${{</span> <span class=hljs-string >matrix.julia-version</span> <span class=hljs-string >}}</span>
          <span class=hljs-attr >arch:</span> <span class=hljs-string >${{</span> <span class=hljs-string >matrix.julia-arch</span> <span class=hljs-string >}}</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >actions/cache@v1</span>
        <span class=hljs-attr >env:</span>
          <span class=hljs-attr >cache-name:</span> <span class=hljs-string >cache-artifacts</span>
        <span class=hljs-attr >with:</span>
          <span class=hljs-attr >path:</span> <span class=hljs-string >~/.julia/artifacts</span>
          <span class=hljs-attr >key:</span> <span class=hljs-string >${{</span> <span class=hljs-string >runner.os</span> <span class=hljs-string >}}-test-${{</span> <span class=hljs-string >env.cache-name</span> <span class=hljs-string >}}-${{</span> <span class=hljs-string >hashFiles(&#x27;**/Project.toml&#x27;)</span> <span class=hljs-string >}}</span>
          <span class=hljs-attr >restore-keys:</span> <span class=hljs-string >|
            ${{ runner.os }}-test-${{ env.cache-name }}-
            ${{ runner.os }}-test-
            ${{ runner.os }}-
</span>      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >julia-actions/julia-buildpkg@v1</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >run:</span> <span class=hljs-string >julia</span> <span class=hljs-string >--check-bounds=yes</span> <span class=hljs-string >--color=yes</span> <span class=hljs-string >-e</span> <span class=hljs-string >&#x27;cd(&quot;&lt;subfolder-of-julia-project&gt;&quot;); import Pkg; Pkg.activate(&quot;.&quot;); Pkg.test()&#x27;</span></code></pre>
<p>Note that you have to <em><strong>adjust</strong></em> the bit: <code>cd&#40;&quot;&lt;subfolder-of-julia-project&gt;&quot;&#41;</code>.</p>
<p>üëâ The <em><strong>example</strong></em> is in <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00-L6Testing-subfolder.jl">course-101-0250-00-L6Testing-subfolder.jl</a>.</p>
<h4 id=a_final_note ><a href="#a_final_note" class=header-anchor >A final note</a></h4>
<p>GitHub Actions are limited to 2000min per month per user for private repositories.</p>

<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<h1 id=exercises_-_lecture_7 ><a href="#exercises_-_lecture_7" class=header-anchor >Exercises - lecture 7</a></h1>
<h2 id=infos_about_projects ><a href="#infos_about_projects" class=header-anchor >Infos about projects</a></h2>
<p>Starting from this lecture &#40;and until to lecture 9&#41;, homework will contribute to the course&#39;s first project. Make sure to carefully follow the instructions from the Project section in <a href="/logistics#project">Logistics</a> as well as the specific steps listed hereafter.</p>
<div class=warning ><div class=title >‚ö†Ô∏è Warning&#33;</div>
<div class=messg >This project being identical to all students. We ask you to strictly follow the demanded structure and steps as this will be part of the evaluation criteria, besides running 3D codes.</div></div>
<h3 id=preparing_the_project_folder_in_your_github_repo ><a href="#preparing_the_project_folder_in_your_github_repo" class=header-anchor >Preparing the project folder in your GitHub repo</a></h3>
<p>For the project, you will have to create a <code>PorousConvection</code> folder <strong>within</strong> your <code>pde-on-gpu-&lt;lastname&gt;</code> shared private GitHub repo. To do so, you can use <a href="https://github.com/JuliaCI/PkgTemplates.jl"><code>PkgTemplates.jl</code></a>.</p>
<ol>
<li><p>Within Julia, run following command while <strong>being in the root</strong> of your <code>pde-on-gpu-&lt;lastname&gt;</code> folder:</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> PkgTemplates
Template(; dir=<span class=hljs-string >&quot;.&quot;</span>, plugins=[Git(; ssh=<span class=hljs-literal >true</span>), GitHubActions(; x86=<span class=hljs-literal >true</span>)],)(<span class=hljs-string >&quot;PorousConvection&quot;</span>)</code></pre>
<ol start=2 >
<li><p>From the automatically generated files and folders, you can remove the <code>.git</code> since we are already in a git folder, as well as the <code>.github/workflows/CompatHelper.yml</code> and <code>.github/workflows/TagBot.yml</code> files as we won&#39;t use them.</p>

<li><p>This should give you the basic structure. Then edit the <code>.gitignore</code> file to include <code>Manifest.toml</code> and <code>.DS_Store</code> for mac users.</p>

<li><p>Also, add following folders to the repo: <code>docs</code>, <code>scripts</code>. You will place all assets linked from the <code>README.md</code> in <code>docs</code>, and add your scripts to <code>scripts</code>. We won&#39;t touch <code>src</code>.</p>

<li><p>Your final structure should be as following:</p>

</ol>
<pre><code class="julia hljs">PorousConvection
|-- .github
|   <span class=hljs-string >`-- workflows
|       `</span>-- CI.yml
|-- .gitignore
|-- LICENSE
|-- Manifest.toml
|-- Project.toml
|-- README.md
|-- docs
|-- scripts
|-- src
|   <span class=hljs-string >`-- PorousConvection.jl
`</span>-- test
    <span class=hljs-string >`-- runtests.jl</span></code></pre>
<p>In the next 3 lectures &#40;7,8,9&#41;, we will populate the <code>scripts</code> folder with 2D and 3D porous convection applications, add tests and use the <code>README.md</code> as main &quot;documentation&quot;.</p>
<p>You should now be all set and ready to get started üöÄ</p>
<h2 id=exercise_1_-_2d_thermal_porous_convection_xpu_implementation ><a href="#exercise_1_-_2d_thermal_porous_convection_xpu_implementation" class=header-anchor >Exercise 1 - <strong>2D thermal porous convection xPU implementation</strong></a></h2>
<p>üëâ See <a href="/logistics/#submission">Logistics</a> for submission details.</p>
<p>The goal of this exercise is to:</p>
<ul>
<li><p>Finalise the xPU implementation of the 2D fluid diffusion solver started in class</p>

<li><p>Familiarise with xPU programming, <code>@parallel</code> and <code>@parallel_indices</code></p>

<li><p>Port your 2D thermal porous convection code to xPU implementation</p>

<li><p>Start populating the project repository</p>

</ul>
<p>In this exercise, you will finalise the 2D fluid diffusion solver started during lecture 7 and use the new xPU scripts as starting point to port your 2D thermal porous convection code.</p>

<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<hr />
<h2 id=exercise_2_-_3d_thermal_porous_convection_xpu_implementation ><a href="#exercise_2_-_3d_thermal_porous_convection_xpu_implementation" class=header-anchor >Exercise 2 - <strong>3D thermal porous convection xPU implementation</strong></a></h2>
<p>üëâ See <a href="/logistics/#submission">Logistics</a> for submission details.</p>
<p>The goal of this exercise is to:</p>
<ul>
<li><p>Create a 3D xPU implementation of the 2D thermal porous convection code</p>

<li><p>Familiarise with 3D and xPU programming, <code>@parallel</code> and <code>@parallel_indices</code></p>

<li><p>Include 3D visualisation using <a href="https://docs.makie.org/stable/"><code>Makie.jl</code></a></p>

</ul>

<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<hr />
<h2 id=exercise_3_-_ci_and_github_actions ><a href="#exercise_3_-_ci_and_github_actions" class=header-anchor >Exercise 3 - <strong>CI and GitHub Actions</strong></a></h2>
<p>üëâ See <a href="/logistics/#submission">Logistics</a> for submission details.</p>
<p>The goal of this exercise is to:</p>
<ul>
<li><p>setup Continuous Integration with GitHub Actions</p>

</ul>
<h3 id=tasks ><a href="#tasks" class=header-anchor >Tasks</a></h3>
<ol>
<li><p>Add CI setup to your <code>PorousConvection</code> project to run <strong>one unit and one reference test</strong> for both the 2D and 3D thermal porous convection scripts.</p>

<li><p>Follow/revisit the lecture and in particular look at the example at <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00-L6Testing-subfolder.jl">https://github.com/eth-vaw-glaciology/course-101-0250-00-L6Testing-subfolder.jl</a> to setup CI for a folder that is part of another Git repo &#40;your <code>PorousConvection</code> folder is part of your <code>pde-on-gpu-&lt;username&gt;</code> git repo&#41;.</p>

<li><p>Push to GitHub and make sure the CI runs and passes</p>

<li><p>Add the CI-badge to the <code>README.md</code> file from your <code>PorousConvection</code> folder, right below the title &#40;as it is commonly done&#41;.</p>

</ol>

<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<div class=page-foot >
  <div class=copyright >
    <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a><br>
    Last modified: October 31, 2022. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div>