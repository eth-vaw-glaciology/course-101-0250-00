<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/course-101-0250-00/libs/katex/katex.min.css"> <link rel=stylesheet  href="/course-101-0250-00/libs/highlight/github.min.css"> <link rel=stylesheet  href="/course-101-0250-00/css/franklin.css"> <link rel=stylesheet  href="/course-101-0250-00/css/poole_hyde.css"> <link rel=stylesheet  href="/course-101-0250-00/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=icon  href="/course-101-0250-00/assets/favicon.png"> <title>Lecture 7</title> <style> .content {max-width: 50rem} </style> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/course-101-0250-00/assets/vaw_logo.png" style="width: 180px; height: auto; display: inline"> <div style="font-weight: margin-bottom: 0.5em"><a href="/course-101-0250-00/"> Fall 2021</a> <span style="opacity: 0.7;">| <a href="http://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2021W&ansicht=KATALOGDATEN&lerneinheitId=155538&lang=en"> ETHZ 101-0250-00</a></span></div> <br> <h1><a href="/course-101-0250-00/">Solving partial differential equations in parallel on GPUs</a></h1> <div style="line-height:18px; font-size: 15px; opacity: 0.85">by <a href="https://vaw.ethz.ch/en/people/person-detail.MjcwOTYw.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ludovic R√§ss</a>, <a href="https://vaw.ethz.ch/en/personen/person-detail.html?persid=124402">Mauro Werder</a> & <a href="https://www.cscs.ch/about/staff/">Samuel Omlin</a> </div> </div> <br> <style> </style> <nav class=sidebar-nav  style="opacity: 0.9"> <a class="sidebar-nav-item " href="/course-101-0250-00/"><b>Welcome</b></a> <a class="sidebar-nav-item " href="/course-101-0250-00/logistics/">Logistics</a> <a class="sidebar-nav-item " href="/course-101-0250-00/homework/">Homework</a> <a class="sidebar-nav-item " href="/course-101-0250-00/software_install/">Software install</a> <a class="sidebar-nav-item " href="/course-101-0250-00/extras/">Extras</a> <br> <div class=course-section >Part 1 - Introduction</div> <a class="sidebar-nav-item " href="/course-101-0250-00/lecture1/">Lecture 1</a> <a class="sidebar-nav-item " href="/course-101-0250-00/lecture2/">Lecture 2</a> <a class="sidebar-nav-item " href="/course-101-0250-00/lecture3/">Lecture 3</a> <a class="sidebar-nav-item " href="/course-101-0250-00/lecture4/">Lecture 4</a> <div class=course-section >Part 2 - Solving PDEs on GPUs</div> <a class="sidebar-nav-item " href="/course-101-0250-00/lecture5/">Lecture 5</a> <a class="sidebar-nav-item " href="/course-101-0250-00/lecture6/">Lecture 6</a> <a class="sidebar-nav-item active" href="/course-101-0250-00/lecture7/">Lecture 7</a> <div class=course-section >Part 3 - Projects</div> <br> </nav> </div> </div> <div class="content container"> <div class=franklin-content > <h1 id=lecture_7 ><a href="#lecture_7" class=header-anchor >Lecture 7</a></h1> <blockquote> <p><strong>Agenda</strong><br />üìö The &quot;two-language problem&quot;, XPU implementation with <code>ParallelStencil.jl</code><br />üíª Reference testing, GitHub CI and workflows<br />üöß Exercises:</p> <ul> <li><p>Data transfer optimisations on GPUs</p> <li><p>GPU codes for diffusion / acoustic waves 2D</p> <li><p>Reference testing in Julia</p> </ul> </blockquote> <p><hr /> </p> <p><a id=content  class=anchor ></a> <strong>Content</strong></p> <div class=franklin-toc ><ol><li><a href="#lecture_7">Lecture 7</a><li><a href="#solving_the_two-language_problem_xpu-implementation">Solving the two-language problem: XPU-implementation</a><ol><li><a href="#the_two-language_problem">The two-language problem</a><li><a href="#backend_portable_xpu_implementation">Backend portable XPU implementation</a><li><a href="#towards_stokes_i_acoustic_to_elastic">Towards Stokes I: acoustic to elastic</a></ol><li><a href="#reference_testing_github_ci_and_workflows">Reference testing, GitHub CI and workflows</a><li><a href="#exercises_-_lecture_7">Exercises - lecture 7</a><ol><li><a href="#exercise_1_-_title">Exercise 1 - <strong>Title</strong></a></ol></ol></div> <p><a href="#exercises_-_lecture_6"><em>üëâ get started with exercises</em></a></p> <hr /> <h1 id=solving_the_two-language_problem_xpu-implementation ><a href="#solving_the_two-language_problem_xpu-implementation" class=header-anchor >Solving the two-language problem: XPU-implementation</a></h1> <h3 id=the_goal_of_this_lecture_7 ><a href="#the_goal_of_this_lecture_7" class=header-anchor >The goal of this lecture 7:</a></h3> <ul> <li><p>Address the <strong><em>two-language problem</em></strong></p> <li><p>Backend portable XPU implementation</p> <li><p>Towards Stokes I: acoustic to elastic</p> <li><p>Reference testing, GitHub CI and workflows</p> </ul> <h2 id=the_two-language_problem ><a href="#the_two-language_problem" class=header-anchor >The two-language problem</a></h2> <p>Combining CPU and GPU implementation within a single code.</p> <p>You may certainly be familiar with this situation in scientific computing:</p> <p><img src="../assets/literate_figures/l7-2lang_1.png" alt="two-lang problem" /></p> <p>Which may turn out into a costly cycle:</p> <p><img src="../assets/literate_figures/l7-2lang_2.png" alt="two-lang problem" /></p> <p>This situation is referred to as the <strong><em>two-language problem</em></strong>.</p> <p>Multi-language/software environment leads to:</p> <ul> <li><p>Translation errors</p> <li><p>Large development time &#40;overhead&#41;</p> <li><p>Non-portable solutions</p> </ul> <p>Good news&#33; Julia is a perfect candidate to solve the <strong><em>two-language problem</em></strong> as Julia code is:</p> <ul> <li><p><strong><em>simple</em></strong>, high-level, interactive &#40;low development costs&#41;</p> <li><p><strong><em>fast</em></strong>, compiled just ahead of time &#40;before one use it for the first time&#41;</p> </ul> <div class=img-med ><img src="../assets/literate_figures/l7-2lang_3.png" alt="two-lang problem" /></div> <p>Julia provides a <strong><em>portable</em></strong> solution in many aspects &#40;beyond performance portability&#41;.</p> <p>As you may have started to experience, GPUs deliver great performance but may not be present in every laptop or workstation. Also, powerful GPUs require servers to run on, especially when multiple GPUs are needed to perform high-resolution calculations.</p> <p>Wouldn&#39;t it be great to have <strong>single code that both executes on CPU and GPU?</strong></p> <blockquote> <p>Using the CPU &quot;backend&quot; for prototyping and debugging, and switching to the GPU &quot;backend&quot; for production purpose.</p> </blockquote> <p>Wouldn&#39;t it be great? ... <strong>YES</strong>, and there is a Julia solution&#33;</p> <div class=img-med ><img src="../assets/literate_figures/ps_logo.png" alt=ParallelStencil  /></div> <h2 id=backend_portable_xpu_implementation ><a href="#backend_portable_xpu_implementation" class=header-anchor >Backend portable XPU implementation</a></h2> <p>Let&#39;s get started with <a href="https://github.com/omlins/ParallelStencil.jl"><code>ParallelStencil.jl</code></a></p> <h3 id=getting_started_with_parallelstencil ><a href="#getting_started_with_parallelstencil" class=header-anchor >Getting started with ParallelStencil</a></h3> <p>ParallelStencil enables to</p> <ul> <li><p>Write architecture-agnostic high-level code</p> <li><p>Parallel high-performance stencil computations on GPUs and CPUs</p> </ul> <p>ParallelStencil relies on the native kernel programming capabilities of</p> <ul> <li><p><a href="https://cuda.juliagpu.org/stable/">CUDA.jl</a> for high-performance computations on GPUs</p> <li><p><a href="https://docs.julialang.org/en/v1/base/multi-threading/#Base.Threads">Base.Threads</a> for high-performance computations on CPUs</p> </ul> <h3 id=short_tour_of_parallelstencils_readme ><a href="#short_tour_of_parallelstencils_readme" class=header-anchor >Short tour of ParallelStencil&#39;s README</a></h3> <p>Before we start our first push-up exercise, let&#39;s have a rapid tour of <a href="https://github.com/omlins/ParallelStencil.jl"><code>ParallelStencil.jl</code></a>&#39;s repo and <a href="https://github.com/omlins/ParallelStencil.jl"><code>README</code></a>.</p> <p><em>So, how does it work?</em></p> <p>As first hands-on for this lecture, let&#39;s <em><strong>merge</strong></em> the diffusion 2D solver <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/"><code>diffusion_2D_perf_loop_fun.jl</code></a> and the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/"><code>diffusion_2D_perf_gpu.jl</code></a> into a single <em><strong>XPU</strong></em> code using ParallelStencil.</p> <div class=note ><div class=title >üí° Note</div> <div class=messg >Two approaches are possible &#40;we&#39;ll implement both&#41;. Parallelisation using stencil computations with 1&#41; math-close notation; 2&#41; more explicit kernel programming approach.</div></div> <h3 id=stencil_computations_with_math-close_notation ><a href="#stencil_computations_with_math-close_notation" class=header-anchor >Stencil computations with math-close notation</a></h3> <p>Let&#39;s get started with using the <code>ParallelStencil</code> module and the <code>ParallelStencil.FiniteDifferences2D</code> submodule to enable math-close notation.</p> <p>üíª We&#39;ll start from the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/"><code>diffusion_2D_perf_gpu.jl</code></a> &#40;available in the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/">scripts/</a> folder in case you don&#39;t have it at hand from lecture 6&#41; to create the <code>diffusion_2D_xpu.jl</code> script.</p> <p>The first step is to handle the packages:</p> <pre><code class="julia hljs"><span class=hljs-keyword >const</span> USE_GPU = <span class=hljs-literal >false</span>
<span class=hljs-keyword >using</span> ParallelStencil
<span class=hljs-keyword >using</span> ParallelStencil.FiniteDifferences2D
<span class=hljs-meta >@static</span> <span class=hljs-keyword >if</span> USE_GPU
    <span class=hljs-meta >@init_parallel_stencil</span>(CUDA, <span class=hljs-built_in >Float64</span>, <span class=hljs-number >2</span>)
<span class=hljs-keyword >else</span>
    <span class=hljs-meta >@init_parallel_stencil</span>(Threads, <span class=hljs-built_in >Float64</span>, <span class=hljs-number >2</span>)
<span class=hljs-keyword >end</span>
<span class=hljs-keyword >using</span> Plots, Printf</code></pre> <p>Then, we need to create two compute functions , <code>compute_q&#33;</code> to compute the fluxes, and <code>compute_C&#33;</code> for computing the update of <code>C</code>, the quantity we diffusion &#40;e.g. concentration&#41;.</p> <p>Let&#39;s start with <code>compute_q&#33;</code>. There we want to program the following fluxes</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><msub><mi>q</mi><mi>x</mi></msub><mo>=</mo><mo>‚àí</mo><mi>D</mi><mfrac><mrow><mi mathvariant=normal >‚àÇ</mi><mi>C</mi></mrow><mrow><mi mathvariant=normal >‚àÇ</mi><mi>x</mi></mrow></mfrac><mtext> </mtext><mo separator=true >,</mo><mtext> </mtext><msub><mi>q</mi><mi>y</mi></msub><mo>=</mo><mo>‚àí</mo><mi>D</mi><mfrac><mrow><mi mathvariant=normal >‚àÇ</mi><mi>C</mi></mrow><mrow><mi mathvariant=normal >‚àÇ</mi><mi>y</mi></mrow></mfrac><mtext> </mtext><mi mathvariant=normal >.</mi></mrow><annotation encoding="application/x-tex"> q_x = -D\frac{‚àÇC}{‚àÇx} ~,~~ q_y = -D\frac{‚àÇC}{‚àÇy} ~.</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.19444em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s >‚Äã</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:2.05744em;vertical-align:-0.686em;"></span><span class=mord >‚àí</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.37144em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord  style="margin-right:0.05556em;">‚àÇ</span><span class="mord mathnormal">x</span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord  style="margin-right:0.05556em;">‚àÇ</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span><span class=vlist-s >‚Äã</span></span><span class=vlist-r ><span class=vlist  style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace nobreak"> </span><span class=mpunct >,</span><span class="mspace nobreak"> </span><span class="mspace nobreak"> </span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class=vlist-s >‚Äã</span></span><span class=vlist-r ><span class=vlist  style="height:0.286108em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:2.25188em;vertical-align:-0.8804400000000001em;"></span><span class=mord >‚àí</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.37144em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord  style="margin-right:0.05556em;">‚àÇ</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord  style="margin-right:0.05556em;">‚àÇ</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span><span class=vlist-s >‚Äã</span></span><span class=vlist-r ><span class=vlist  style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace nobreak"> </span><span class=mord >.</span></span></span></span></span> <p><code>ParallelStencil</code>&#39;s <code>FiniteDifferences2D</code> submodule provides macros we need: <code>@all&#40;&#41;</code>, <code>@d_xi&#40;&#41;</code>, <code>@d_yi&#40;&#41;</code>.</p> <p>The macros used in this example are described in the Module documentation callable from the Julia REPL / IJulia:</p> <pre><code class="julia hljs">julia&gt; <span class=hljs-keyword >using</span> ParallelStencil.FiniteDifferences2D

julia&gt;?

help?&gt; <span class=hljs-meta >@all</span></code></pre> <p>This would give you more infos about the <code>@all</code> macro.</p> <p>So, back to our compute function &#40;kernel&#41;. The <code>compute_q&#33;</code> function gets the <code>@parallel</code> macro in its definition and returns nothing.</p> <p>Inside, we define the flux definition as following:</p> <pre><code class="julia hljs"><span class=hljs-meta >@parallel</span> <span class=hljs-keyword >function</span> compute_q!(qx, qy, C, D, dx, dy)
    <span class=hljs-meta >@all</span>(qx) = -D*<span class=hljs-meta >@d_xi</span>(C)/dx
    <span class=hljs-meta >@all</span>(qy) = -D*<span class=hljs-meta >@d_yi</span>(C)/dy
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span></code></pre> <p>Now that we&#39;re done with <code>compute_q&#33;</code>, your turn&#33;</p> <p>By analogy, update <code>compute_C&#33;</code>.</p> <pre><code class="julia hljs"><span class=hljs-meta >@parallel</span> <span class=hljs-keyword >function</span> compute_C!(C, qx, qy, dt, dx, dy)
   <span class=hljs-comment ># C = C - dt * (‚àÇqx/dx + ‚àÇqy/dy)</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span></code></pre> <p>So far so good. We are done with the kernels. Let&#39;s see what changes are needed in the main part of the script.</p> <p>We can keep the <code># Physics</code> section as such. The <code># Numerics</code> only needs <code>nx</code>, <code>ny</code> and <code>nout</code>; the kernel launch parameters being now automatically adapted</p> <pre><code class="julia hljs"><span class=hljs-meta >@views</span> <span class=hljs-keyword >function</span> diffusion_2D(; do_visu=<span class=hljs-literal >false</span>)
    <span class=hljs-comment ># Physics</span>
    Lx, Ly  = <span class=hljs-number >10.0</span>, <span class=hljs-number >10.0</span>
    D       = <span class=hljs-number >1.0</span>
    ttot    = <span class=hljs-number >1e-4</span>
    <span class=hljs-comment ># Numerics</span>
    nx, ny  = <span class=hljs-number >32</span>*<span class=hljs-number >16</span>, <span class=hljs-number >32</span>*<span class=hljs-number >16</span> <span class=hljs-comment ># number of grid points</span>
    nout    = <span class=hljs-number >50</span>
    <span class=hljs-comment ># [...]</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span></code></pre> <p>In the <code># Derived numerics</code>, we can skip the scalar pre-processing, keeping only</p> <pre><code class="julia hljs"><span class=hljs-comment ># [...]</span>
<span class=hljs-comment ># Derived numerics</span>
dx, dy  = Lx/nx, Ly/ny
dt      = min(dx, dy)^<span class=hljs-number >2</span>/D/<span class=hljs-number >4.1</span>
nt      = cld(ttot, dt)
xc, yc  = <span class=hljs-built_in >LinRange</span>(dx/<span class=hljs-number >2</span>, Lx-dx/<span class=hljs-number >2</span>, nx), <span class=hljs-built_in >LinRange</span>(dy/<span class=hljs-number >2</span>, Ly-dy/<span class=hljs-number >2</span>, ny)
<span class=hljs-comment ># [...]</span></code></pre> <p>In the <code># Array initialisation</code> section, we need to wrap the Gaussian by <code>Data.Array</code> &#40;instead of <code>CuArray</code>&#41; and initialise the flux arrays:</p> <pre><code class="julia hljs"><span class=hljs-comment ># [...]</span>
<span class=hljs-comment ># Array initialisation</span>
C       = Data.<span class=hljs-built_in >Array</span>(exp.(.-(xc .- Lx/<span class=hljs-number >2</span>).^<span class=hljs-number >2</span> .-(yc&#x27; .- Ly/<span class=hljs-number >2</span>).^<span class=hljs-number >2</span>))
qx      = <span class=hljs-meta >@zeros</span>(nx-<span class=hljs-number >1</span>,ny-<span class=hljs-number >2</span>)
qy      = <span class=hljs-meta >@zeros</span>(nx-<span class=hljs-number >2</span>,ny-<span class=hljs-number >1</span>)
<span class=hljs-comment ># [...]</span></code></pre> <p>In the <code># Time loop</code>, only the kernel call needs to be worked out. We can here re-use the single <code>@parallel</code> macro which now serves to launch the computations on the chosen backend:</p> <pre><code class="julia hljs"><span class=hljs-comment ># [...]</span>
t_tic = <span class=hljs-number >0.0</span>; niter = <span class=hljs-number >0</span>
<span class=hljs-comment ># Time loop</span>
<span class=hljs-keyword >for</span> it = <span class=hljs-number >1</span>:nt
    <span class=hljs-keyword >if</span> (it==<span class=hljs-number >11</span>) t_tic = Base.time(); niter = <span class=hljs-number >0</span> <span class=hljs-keyword >end</span>
    <span class=hljs-meta >@parallel</span> compute_q!(qx, qy, C, D, dx, dy)
    <span class=hljs-meta >@parallel</span> compute_C!(C, qx, qy, dt, dx, dy)
    niter += <span class=hljs-number >1</span>
    <span class=hljs-keyword >if</span> do_visu &amp;&amp; (it % nout == <span class=hljs-number >0</span>)
        <span class=hljs-comment ># visualisation unchanged</span>
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span>
<span class=hljs-comment ># [...]</span></code></pre> <p>The performance evaluation section remaining unchanged, we are all set&#33;</p> <p><strong>Wrap-up tasks</strong></p> <ul> <li><p>Let&#39;s execute the code having the <code>USE_GPU &#61; false</code> flag set. We are running on multi-threading CPU backend with multi-threading enabled.</p> <li><p>Changing the <code>USE_GPU</code> flag to <code>true</code> &#40;having first relaunched a Julia session&#41; will make the application running on a GPU.</p> </ul> <div class=note ><div class=title >üí° Note</div> <div class=messg >Curious to see how it works under the hood? Feel free to <a href="https://github.com/omlins/ParallelStencil.jl/blob/cd59a5b0d1fd32ceaecbf7fc922ab87a24257781/src/ParallelKernel/parallel.jl#L263">explore the source code</a>. Another nice bit of open source software &#40;and the fact that Julia&#39;s meta programming rocks üöÄ&#41;.</div></div> <h3 id=stencil_computations_with_more_explicit_kernel_programming_approach ><a href="#stencil_computations_with_more_explicit_kernel_programming_approach" class=header-anchor >Stencil computations with more explicit kernel programming approach</a></h3> <p>ParallelStencil also allows for more explicit kernel programming, enabled by <code>@parallel_indices</code> kernel definitions. In style, the codes are closer to the initial plain GPU version we started from, <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/"><code>diffusion_2D_perf_gpu.jl</code></a>.</p> <p>As the macro name suggests, kernels defined using <code>@parallel_indices</code> allow for explicit indices handling within the kernel operations. This approach is <em><strong>currently</strong></em> more performant than using <code>@parallel</code> kernel definitions.</p> <p>As second push-up, let&#39;s transform the <code>diffusion_2D_xpu.jl</code> into <code>diffusion_2D_perf_xpu.jl</code>.</p> <p>üíª We&#39;ll need bits from both <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/"><code>diffusion_2D_perf_gpu.jl</code></a> and <code>diffusion_2D_xpu.jl</code>.</p> <p>We can keep the package handling and initialisation identical to what we implemented in the <code>diffusion_2D_xpu.jl</code> script.</p> <p>Then, we can start from the flux macro an compute function definition from the <code>diffusion_2D_perf_gpu.jl</code> script, removing the <code>ix</code>, <code>iy</code> indices as those are now handled by ParallelStencil. The function definition takes however the <code>@parallel_indices</code> macro and the <code>&#40;ix,iy&#41;</code> tuple:</p> <pre><code class="julia hljs"><span class=hljs-comment ># macros to avoid array allocation</span>
<span class=hljs-keyword >macro</span> qx(ix,iy)  esc(:( -D_dx*(C[$ix+<span class=hljs-number >1</span>,$iy+<span class=hljs-number >1</span>] - C[$ix,$iy+<span class=hljs-number >1</span>]) )) <span class=hljs-keyword >end</span>
<span class=hljs-keyword >macro</span> qy(ix,iy)  esc(:( -D_dy*(C[$ix+<span class=hljs-number >1</span>,$iy+<span class=hljs-number >1</span>] - C[$ix+<span class=hljs-number >1</span>,$iy]) )) <span class=hljs-keyword >end</span>

<span class=hljs-meta >@parallel_indices</span> (ix,iy) <span class=hljs-keyword >function</span> compute!(C2, C, D_dx, D_dy, dt, _dx, _dy, size_C1_2, size_C2_2)
    <span class=hljs-keyword >if</span> (ix&lt;=size_C1_2 &amp;&amp; iy&lt;=size_C2_2)
        C2[ix+<span class=hljs-number >1</span>,iy+<span class=hljs-number >1</span>] = C[ix+<span class=hljs-number >1</span>,iy+<span class=hljs-number >1</span>] - dt*( (<span class=hljs-meta >@qx</span>(ix+<span class=hljs-number >1</span>,iy) - <span class=hljs-meta >@qx</span>(ix,iy))*_dx + (<span class=hljs-meta >@qy</span>(ix,iy+<span class=hljs-number >1</span>) - <span class=hljs-meta >@qy</span>(ix,iy))*_dy )
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span></code></pre> <p>The <code># Physics</code> section remains unchanged, and the <code># Numerics section</code> is identical to the previous <code>xpu</code> script, i.e., no need for explicit block and thread definition.</p> <div class=warning ><div class=title >‚ö†Ô∏è Warning&#33;</div> <div class=messg >ParallelStencil computes the GPU kernel launch parameters based on optimal heuristics. Recalling lecture 6, multiple of 32 are most optimal; number of grid points should thus be chosen accordingly, i.e. as multiple of 32.</div></div> <p>We can then keep the scalar preprocessing &#40;<code>D_dx</code>, <code>D_dy</code>, <code>_dx</code>, <code>_dy</code>&#41; in the <code># Derived numerics</code> section.</p> <p>In the <code># Array initialisation</code>, make sure wrapping the Gaussian by <code>Data.Array</code>. The <code>cuthreads</code> and <code>cublocks</code> tuples are no longer needed.</p> <p>The <code># Time loop</code> gets very concise; XPU kernels are launched here also with <code>@parallel</code> macro &#40;that implicitly includes <code>synchronize&#40;&#41;</code> statement&#41;:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Time loop</span>
<span class=hljs-keyword >for</span> it = <span class=hljs-number >1</span>:nt
    <span class=hljs-keyword >if</span> (it==<span class=hljs-number >11</span>) t_tic = Base.time(); niter = <span class=hljs-number >0</span> <span class=hljs-keyword >end</span>
    <span class=hljs-meta >@parallel</span> compute!(C2, C, D_dx, D_dy, dt, _dx, _dy, size_C1_2, size_C2_2)
    C, C2 = C2, C <span class=hljs-comment ># pointer swap</span>
    niter += <span class=hljs-number >1</span>
    <span class=hljs-keyword >if</span> do_visu &amp;&amp; (it % nout == <span class=hljs-number >0</span>)
        <span class=hljs-comment ># visu unchanged</span>
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre> <p>Here we go üöÄ The <code>diffusion_2D_perf_xpu.jl</code> code is ready and should squeeze the performance out of your CPU or GPU, running as fast as the exclusive Julia multi-threaded or Julia GPU implementations, respectively.</p> <h3 id=multi-xpu_support ><a href="#multi-xpu_support" class=header-anchor >Multi-XPU support</a></h3> <p><em>What about multi-XPU support and distributed memory parallelisation?</em></p> <p>ParallelStencil is seamlessly interoperable with <a href=""><code>ImplicitGlobalGrid.jl</code></a>, which enables distributed parallelisation of stencil-based XPU applications on a regular staggered grid and enables close to ideal weak scaling of real-world applications on thousands of GPUs.</p> <p>Moreover, ParallelStencil enables hiding communication behind computation with a simple macro call and without any particular restrictions on the package used for communication.</p> <p><em>This will be material for next lectures.</em></p> <div class=note ><div class=title >üí° Note</div> <div class=messg >Head to ParallelStencil&#39;s <a href="https://github.com/omlins/ParallelStencil.jl#concise-singlemulti-xpu-miniapps">miniapp section</a> if you are curious about various domain science application featrued there.</div></div> <h2 id=towards_stokes_i_acoustic_to_elastic ><a href="#towards_stokes_i_acoustic_to_elastic" class=header-anchor >Towards Stokes I: acoustic to elastic</a></h2> <p>From acoustic to elastic wave propagation; stress, strain and elastic rheology</p> <p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p> <h1 id=reference_testing_github_ci_and_workflows ><a href="#reference_testing_github_ci_and_workflows" class=header-anchor >Reference testing, GitHub CI and workflows</a></h1> <p>ready to getting started</p> <p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p> <h1 id=exercises_-_lecture_7 ><a href="#exercises_-_lecture_7" class=header-anchor >Exercises - lecture 7</a></h1> <div class=warning ><div class=title >‚ö†Ô∏è Warning&#33;</div> <div class=messg >Exercises have to be handed in as monolithic Julia scripts &#40;one code per script&#41; and uploaded to your private &#40;shared&#41; GitHub repository, in a <strong>specific folder for each lecture</strong>. The git commit hash &#40;or SHA&#41; of the final push needs to be uploaded on Moodle &#40;<a href="/course-101-0250-00/homework">more</a>&#41;.</div></div> <h2 id=exercise_1_-_title ><a href="#exercise_1_-_title" class=header-anchor >Exercise 1 - <strong>Title</strong></a></h2> <p>üëâ See <a href="/course-101-0250-00/logistics/#submission">Logistics</a> for submission details.</p> <p>The goal of this exercise is to:</p> <ul> <li> </ul> <h3 id=getting_started ><a href="#getting_started" class=header-anchor >Getting started</a></h3> <p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p> <hr /> <div class=page-foot > <div class=copyright > <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a><br> Last modified: November 01, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div>