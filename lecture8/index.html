<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <script src="/libs/lunr/lunr.min.js"></script> <script src="/libs/lunr/lunr_index.js"></script> <script src="/libs/lunr/lunrclient.min.js"></script> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/poole_hyde.css"> <link rel=stylesheet  href="/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=icon  href="/assets/favicon.png"> <title>Lecture 8</title> <style> .content {max-width: 50rem} </style> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/assets/vaw_logo.png" style="width: 180px; height: auto; display: inline"> <div style="font-weight: margin-bottom: 0.5em"><a href="/"> Fall 2022</a> <span style="opacity: 0.7;">| <a href="http://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2022W&ansicht=KATALOGDATEN&lerneinheitId=162403&lang=en"> ETHZ 101-0250-00</a></span></div> <br> <h1><a href="/">Solving partial differential equations in parallel on GPUs</a></h1> <div style="line-height:18px; font-size: 15px; opacity: 0.85">by &nbsp; <a href="https://vaw.ethz.ch/en/people/person-detail.MjcwOTYw.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ludovic Räss</a>, &nbsp; <a href="https://vaw.ethz.ch/en/personen/person-detail.html?persid=124402">Mauro Werder</a>, &nbsp; <a href="https://www.cscs.ch/about/staff/">Samuel Omlin</a> & <br> <a href="https://vaw.ethz.ch/en/people/person-detail.MzAwMjIy.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ivan Utkin</a> </div> </div> <br> <style> </style> <nav class=sidebar-nav  style="opacity: 0.9; margin-bottom: 1.2cm;"> <a class="sidebar-nav-item " href="/"><b>Welcome</b></a> <a class="sidebar-nav-item " href="/logistics/">Logistics</a> <a class="sidebar-nav-item " href="/homework/">Homework</a> <a class="sidebar-nav-item " href="/software_install/">Software install</a> <a class="sidebar-nav-item " href="/extras/">Extras</a> <br> <div class=course-section >Part 1 - Introduction</div> <a class="sidebar-nav-item " href="/lecture1/">Lecture 1 - Why Julia GPU</a> <a class="sidebar-nav-item " href="/lecture2/">Lecture 2 - PDEs & physical processes</a> <a class="sidebar-nav-item " href="/lecture3/">Lecture 3 - Solving elliptic PDEs</a> <div class=course-section >Part 2 - Solving PDEs on GPUs</div> <a class="sidebar-nav-item " href="/lecture4/">Lecture 4 - Porous convection</a> <a class="sidebar-nav-item " href="/lecture5/">Lecture 5 - Parallel computing</a> <a class="sidebar-nav-item " href="/lecture6/">Lecture 6 - GPU computing</a> <div class=course-section >Part 3 - Multi-GPU computing (projects)</div> <a class="sidebar-nav-item " href="/lecture7/">Lecture 7 - xPU computing</a> <a class="sidebar-nav-item active" href="/lecture8/">Lecture 8 - Julia MPI & multi-xPU</a> <a class="sidebar-nav-item " href="/lecture9/">Lecture 9 - Advanced optimisations</a> <div class=course-section >Final Projects</div> <a class="sidebar-nav-item " href="/lecture10/">Lecture 10 - Final projects</a> </nav> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/search/index.html"> </form> <br> <br> </div> </div> <div class="content container"> <div class=franklin-content > <h1 id=lecture_8 ><a href="#lecture_8" class=header-anchor >Lecture 8</a></h1> <blockquote> <p><strong>Agenda</strong><br />📚 Distributed multi-xPU computing, MPI, multi-xPU thermal porous convection 3D<br />💻 Automatic documentation<br />🚧 Exercises:</p> <ul> <li><p>Fake-parallelisation, Julia MPI, <code>ImplicitGlobalGrid.jl</code></p> <li><p>Multi-xPU thermal porous convection 3D</p> <li><p>Automatic documentation and CI</p> </ul> </blockquote> <hr /> <p><a id=content  class=anchor ></a> <strong>Content</strong></p> <div class=franklin-toc ><ol><li><a href="#lecture_8">Lecture 8</a><li><a href="#distributed_computing_in_julia">Distributed computing in Julia</a><ol><li><a href="#new_to_distributed_computing">New to distributed computing?</a><li><a href="#fake_parallelisation">Fake parallelisation</a><li><a href="#julia_and_mpi">Julia and MPI</a><li><a href="#using_implicitglobalgridjl">Using <code>ImplicitGlobalGrid.jl</code></a></ol><li><a href="#documenting_your_code">Documenting your code</a><li><a href="#exercises_-_lecture_8">Exercises - lecture 8</a><ol><li><a href="#exercise_1_-_towards_distributed_memory_computing_on_gpus">Exercise 1 - <strong>Towards distributed memory computing on GPUs</strong></a><li><a href="#exercise_2_-_multi-xpu_computing">Exercise 2 - <strong>Multi-xPU computing</strong></a><li><a href="#exercise_3_-_automatic_documentation_in_julia">Exercise 3 - <strong>Automatic documentation in Julia</strong></a></ol></ol></div> <p><a href="#exercises_-_lecture_8"><em>👉 get started with exercises</em></a></p> <hr /> <h1 id=distributed_computing_in_julia ><a href="#distributed_computing_in_julia" class=header-anchor >Distributed computing in Julia</a></h1> <h3 id=the_goal_of_this_lecture_8 ><a href="#the_goal_of_this_lecture_8" class=header-anchor >The goal of this lecture 8:</a></h3> <ul> <li><p>Distributed computing</p> <ul> <li><p>Fake parallelisation</p> <li><p>Julia MPI &#40;CPU &#43; GPU&#41;</p> <li><p>Using <code>ParallelStencil.jl</code> together with <code>ImplicitGlobalGrid.jl</code></p> <li><p>Thermal porous convection 3D using GPU MPI</p> </ul> <li><p>Automatic documentation and CI</p> </ul> <h2 id=new_to_distributed_computing ><a href="#new_to_distributed_computing" class=header-anchor >New to distributed computing?</a></h2> <p><em>If this is the case or not - hold-on, we certainly have some good stuff for everyone</em></p> <h3 id=distributed_computing ><a href="#distributed_computing" class=header-anchor >Distributed computing</a></h3> <p>Adds one additional layer of parallelisation:</p> <ul> <li><p>Global problem does no longer &quot;fit&quot; within a single compute node &#40;or GPU&#41;</p> <li><p>Local resources &#40;mainly memory&#41; are finite, e.g.,</p> <ul> <li><p>CPUs: increase the number of cores beyond what a single CPU can offer</p> <li><p>GPUs: overcome the device memory limitation</p> </ul> </ul> <p>Simply said:</p> <p><em>If one CPU or GPU is not sufficient to solve a problem, then use more than one and solve a subset of the global problem on each.</em></p> <p>Distributed &#40;memory&#41; computing permits to take advantage of computing &quot;clusters&quot;, many similar compute nodes interconnected by high-throughput network. That&#39;s also what supercomputers are.</p> <h3 id=parallel_scaling ><a href="#parallel_scaling" class=header-anchor >Parallel scaling</a></h3> <p>So here we go. Let&#39;s assume we want to solve a certain problem, which we will call the &quot;global problem&quot;. This global problem, we split then into several local problems that execute concurrently.</p> <p>Two scaling approaches exist:</p> <ul> <li><p>strong scaling</p> <li><p>weak scaling</p> </ul> <p>Increasing the amount of computing resources to resolve the same global problem would increase parallelism and may result in faster execution &#40;wall-time&#41;. This parallelisation is called <em><strong>strong scaling</strong></em>; the resources are increased but the global problem size does not change, resulting in an increase in the number of &#40;smaller&#41; local problems that can be solved in parallel.</p> <p>The <em><strong>strong scaling</strong></em> approach is often used when parallelising legacy CPU codes, as increasing the number of parallel local problems can lead to some speed-up, reaching an optimum beyond which additional local processes is no longer be beneficial.</p> <p>However, we won&#39;t follow that path when developing parallel multi-GPU applications from scratch. Why?</p> <p><em>Because GPUs&#39; performance is very sensitive to the local problem size as we experienced when trying to tune the kernel launch parameters &#40;threads, blocks, i.e., the local problem size&#41;.</em></p> <p>When developing multi-GPU applications from scratch, it is likely more suitably to approach distributed parallelisation from a <em><strong>weak scaling</strong></em> perspective; defining first the optimal local problem size to resolve on a single GPU and then increasing the number of optimal local problems &#40;and the number of GPUs&#41; until reaching the global problem one originally wants to solve.</p> <h3 id=implicit_global_grid ><a href="#implicit_global_grid" class=header-anchor >Implicit Global Grid</a></h3> <p>We can thus replicate a local problem multiple times in each dimension of the Cartesian space to obtain a global grid, which is therefore defined implicitly. Local problems define each others local boundary conditions by exchanging internal boundary values using intra-node communication &#40;e.g., message passing interface - <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>&#41;, as depicted on the <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl">figure</a> hereafter:</p> <p><img src="../assets/literate_figures/l8_igg.png" alt=IGG  /></p> <h3 id=distributing_computations_-_challenges ><a href="#distributing_computations_-_challenges" class=header-anchor >Distributing computations - challenges</a></h3> <p>Many things could potentially go wrong in distributed computing. However, the ultimate goal &#40;at least for us&#41; is to keep up with <em><strong>parallel efficiency</strong></em>.</p> <p>The parallel efficiency defines as the speed-up divided by the number of processors. The speed-up defines as the execution time using an increasing number of processors normalised by the single processor execution time. We will use the parallel efficiency in a weak scaling configuration.</p> <p>Ideally, the parallel efficiency should stay close to 1 while increasing the number of computing resources proportionally with the global problem size &#40;i.e. keeping the constant local problem sizes&#41;, meaning no time is lost &#40;no overhead&#41; in due to, e.g., inter-process communication, network congestion, congestion of shared filesystem, etc... as shown in the <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl">figure</a> hereafter:</p> <p><img src="../assets/literate_figures/l8_par_eff.png" alt="Parallel scaling" /></p> <hr /> <h3 id=lets_get_started ><a href="#lets_get_started" class=header-anchor >Let&#39;s get started</a></h3> <p>we will explore distributed computing with Julia&#39;s MPI wrapper <a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a>. This will enable our codes to run on multiple CPUs and GPUs in order to scale on modern multi-CPU/GPU nodes, clusters and supercomputers. In the proposed approach, each MPI process handles one CPU or GPU.</p> <p>We&#39;re going to work out the following steps to tackle distributed parallelisation in this lecture &#40;in 5 tasks&#41;:</p> <ul> <li><p><a href="#fake_parallelisation"><strong>Fake parallelisation</strong> as proof-of-concept</a></p> <li><p><a href="#julia_and_mpi"><strong>Julia and MPI</strong></a></p> </ul> <h2 id=fake_parallelisation ><a href="#fake_parallelisation" class=header-anchor >Fake parallelisation</a></h2> <p>As a first step, we will look at the below 1-D diffusion code which solves the linear diffusion equations using a &quot;fake-parallelisation&quot; approach. We split the calculation on two distinct left and right domains, which requires left and right <code>C</code> arrays, <code>CL</code> and <code>CR</code>, respectively.</p> <p>In this &quot;fake parallelisation&quot; code, the computations for the left and right domain are performed sequentially on one process, but they could be computed on two distinct processes if the needed boundary update &#40;often referred to as halo update in literature&#41; was done with MPI.</p> <p><img src="../assets/literate_figures/l8_1D_global_grid.png" alt="1D Global grid" /></p> <p>The idea of this fake parallelisation approach is the following:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Compute physics locally</span>
CL[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>] .= CL[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>] .+ dt*D*diff(diff(CL)/dx)/dx
CR[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>] .= CR[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>] .+ dt*D*diff(diff(CR)/dx)/dx
<span class=hljs-comment ># Update boundaries (MPI)</span>
CL[<span class=hljs-keyword >end</span>] = ...
CR[<span class=hljs-number >1</span>]   = ...
<span class=hljs-comment ># Global picture</span>
C .= [CL[<span class=hljs-number >1</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>]; CR[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>]]</code></pre> <p>We see that a correct boundary update will be the critical part for a successful implementation. In our approach, we need an overlap of 2 cells between <code>CL</code> and <code>CR</code> in order to avoid any wrong computations at the transition between the left and right domains.</p> <h3 id=task_1_fake_parallelisation_with_2_fake_processes ><a href="#task_1_fake_parallelisation_with_2_fake_processes" class=header-anchor >Task 1 &#40;fake parallelisation with 2 fake processes&#41;</a></h3> <p>Run the &quot;fake parallelisation&quot; 1-D diffusion code <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_diffusion_1D_2procs.jl</code></a>, which is missing the boundary updates of the 2 fake processes and describe what you see in the visualisation.</p> <p>Then, add the required boundary update:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Update boundaries (MPI)</span>
CL[<span class=hljs-keyword >end</span>] = ...
CR[<span class=hljs-number >1</span>]   = ...</code></pre> <p>in order make the code work properly and run it again. Note what has changed in the visualisation.</p> <center> <video width="60%" autoplay loop controls src="../assets/literate_figures/l8_diff_1D_2procs.mp4"/> </center> <p>The next step will be to generalise the fake parallelisation with <code>2</code> fake processes to work with <code>n</code> fake processes. The idea of this generalised fake parallelisation approach is the following:</p> <pre><code class="julia hljs"><span class=hljs-keyword >for</span> ip = <span class=hljs-number >1</span>:np <span class=hljs-comment ># compute physics locally</span>
    C[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>,ip] .= C[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>,ip] .+ dt*D*diff(diff(C[:,ip])/dxg)/dxg
<span class=hljs-keyword >end</span>
<span class=hljs-keyword >for</span> ip = <span class=hljs-number >1</span>:np-<span class=hljs-number >1</span> <span class=hljs-comment ># update boundaries</span>
   <span class=hljs-comment ># ...</span>
<span class=hljs-keyword >end</span>
<span class=hljs-keyword >for</span> ip = <span class=hljs-number >1</span>:np <span class=hljs-comment ># global picture</span>
    i1 = <span class=hljs-number >1</span> + (ip-<span class=hljs-number >1</span>)*(nx-<span class=hljs-number >2</span>)
    Cg[i1:i1+nx-<span class=hljs-number >2</span>] .= C[<span class=hljs-number >1</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>,ip]
<span class=hljs-keyword >end</span></code></pre> <p>The array <code>C</code> contains now <code>n</code> local domains where each domain belongs to one fake process, namely the fake process indicated by the second index of <code>C</code> &#40;ip&#41;. The boundary updates are to be adapted accordingly. All the physical calculations happen on the local chunks of the arrays. We only need &quot;global&quot; knowledge in the definition of the initial condition.</p> <p>The previous simple initial conditions can be easily defined without computing any Cartesian coordinates. To define other initial conditions we often need to compute global coordinates. In the code below, which serves to define a Gaussian anomaly in the centre of the domain, Cartesian coordinates can be computed for each cell based on the process ID &#40;<code>ip</code>&#41;, the cell ID &#40;<code>ix</code>&#41;, the array size &#40;<code>nx</code>&#41;, the overlap of the local domains &#40;<code>2</code>&#41; and the grid spacing of the global grid &#40;<code>dxg</code>&#41;; moreover, the origin of the coordinate system can be moved to any position using the global domain length &#40;<code>lx</code>&#41;:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Initial condition</span>
<span class=hljs-keyword >for</span> ip = <span class=hljs-number >1</span>:np
    <span class=hljs-keyword >for</span> ix = <span class=hljs-number >1</span>:nx
        x[ix,ip] = ...
        C[ix,ip] = exp(-x[ix,ip]^<span class=hljs-number >2</span>)
    <span class=hljs-keyword >end</span>
    i1 = <span class=hljs-number >1</span> + (ip-<span class=hljs-number >1</span>)*(nx-<span class=hljs-number >2</span>)
    xt[i1:i1+nx-<span class=hljs-number >2</span>] .= x[<span class=hljs-number >1</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>,ip]; <span class=hljs-keyword >if</span> (ip==np) xt[i1+nx-<span class=hljs-number >1</span>] = x[<span class=hljs-keyword >end</span>,ip] <span class=hljs-keyword >end</span>
    Ct[i1:i1+nx-<span class=hljs-number >2</span>] .= C[<span class=hljs-number >1</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>,ip]; <span class=hljs-keyword >if</span> (ip==np) Ct[i1+nx-<span class=hljs-number >1</span>] = C[<span class=hljs-keyword >end</span>,ip] <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre> <h3 id=task_2_fake_parallelisation_with_n_fake_processes ><a href="#task_2_fake_parallelisation_with_n_fake_processes" class=header-anchor >Task 2 &#40;fake parallelisation with <code>n</code> fake processes&#41;</a></h3> <p>Modify the initial condition in the 1-D diffusion code <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_diffusion_1D_nprocs.jl</code></a> to a centred <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><msub><mi>L</mi><mi>x</mi></msub><mi mathvariant=normal >/</mi><mn>2</mn><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(L_x/2)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal">L</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord >/</span><span class=mord >2</span><span class=mclose >)</span></span></span></span> Gaussian anomaly.</p> <p>Then run this code which is missing the boundary updates of the <code>n</code> fake processes and describe what you see in the visualisation. Then, add the required boundary update in order make the code work properly and run it again. Note what has changed in the visualisation.</p> <center> <video width="60%" autoplay loop controls src="../assets/literate_figures/l8_diff_1D_nprocs.mp4"/> </center> <h2 id=julia_and_mpi ><a href="#julia_and_mpi" class=header-anchor >Julia and MPI</a></h2> <p>We are now ready to write a code that will truly distribute calculations on different processors using <a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a> for inter-process communication.</p> <div class=note ><div class=title >💡 Note</div> <div class=messg >At this point, make sure to have a working Julia MPI environment. Head to <a href="/software_install/#julia_mpi">Julia MPI install</a> to set-up Julia MPI. See <a href="/software_install/#julia_mpi_gpu_on_piz_daint">Julia MPI GPU on Piz Daint</a> for detailed information on how to run MPI GPU &#40;multi-GPU&#41; applications on Piz Daint.</div></div> <p>Let us see what are the somewhat minimal requirements that will allow us to write a distributed code in Julia using MPI.jl. We will solve the following linear diffusion physics:</p> <pre><code class="julia hljs"><span class=hljs-keyword >for</span> it = <span class=hljs-number >1</span>:nt
    qx         .= .-D*diff(C)/dx
    C[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>] .= C[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>] .- dt*diff(qx)/dx
<span class=hljs-keyword >end</span></code></pre> <p>To enable distributed parallelisation, we will do the following steps:</p> <ol> <li><p>Initialise MPI and set up a Cartesian communicator</p> <li><p>Implement a boundary exchange routine</p> <li><p>Create a &quot;global&quot; initial condition</p> <li><p>Finalise MPI</p> </ol> <p>To &#40;1.&#41; initialise MPI and prepare the Cartesian communicator, we do:</p> <pre><code class="julia hljs">MPI.Init()
dims        = [<span class=hljs-number >0</span>]
comm        = MPI.COMM_WORLD
nprocs      = MPI.Comm_size(comm)
MPI.Dims_create!(nprocs, dims)
comm_cart   = MPI.Cart_create(comm, dims, [<span class=hljs-number >0</span>], <span class=hljs-number >1</span>)
me          = MPI.Comm_rank(comm_cart)
coords      = MPI.Cart_coords(comm_cart)
neighbors_x = MPI.Cart_shift(comm_cart, <span class=hljs-number >0</span>, <span class=hljs-number >1</span>)</code></pre> <p>where <code>me</code> represents the process ID unique to each MPI process &#40;the analogue to <code>ip</code> in the fake parallelisation&#41;.</p> <p>Then, we need to &#40;2.&#41; implement a boundary update routine, which can have the following structure:</p> <pre><code class="julia hljs"><span class=hljs-meta >@views</span> <span class=hljs-keyword >function</span> update_halo(A, neighbors_x, comm)
    <span class=hljs-comment ># Send to / receive from neighbour 1 (&quot;left neighbor&quot;)</span>
    <span class=hljs-keyword >if</span> neighbors_x[<span class=hljs-number >1</span>] != MPI.MPI_PROC_NULL
        <span class=hljs-comment ># ...</span>
    <span class=hljs-keyword >end</span>
    <span class=hljs-comment ># Send to / receive from neighbour 2 (&quot;right neighbor&quot;)</span>
    <span class=hljs-keyword >if</span> neighbors_x[<span class=hljs-number >2</span>] != MPI.MPI_PROC_NULL
        <span class=hljs-comment ># ...</span>
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span></code></pre> <p>Then, we &#40;3.&#41; initialize <code>C</code> with a &quot;global&quot; initial Gaussian anomaly that spans correctly over all local domains. This can be achieved, e.g., as given here:</p> <pre><code class="julia hljs">x0    = coords[<span class=hljs-number >1</span>]*(nx-<span class=hljs-number >2</span>)*dx
xc    = [x0 + ix*dx - dx/<span class=hljs-number >2</span> - <span class=hljs-number >0.5</span>*lx  <span class=hljs-keyword >for</span> ix=<span class=hljs-number >1</span>:nx]
C     = exp.(.-xc.^<span class=hljs-number >2</span>)</code></pre> <p>where <code>x0</code> represents the first global x-coordinate on every process &#40;computed in function of <code>coords</code>&#41; and <code>xc</code> represents the local chunk of the global coordinates on each local process &#40;this is analogue to the initialisation in the fake parallelisation&#41;.</p> <p>Last, we need to &#40;4.&#41; finalise MPI prior to returning from the main function:</p> <pre><code class="julia hljs">MPI.Finalize()</code></pre>
<p>All the above described is found in the code <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_diffusion_1D_mpi.jl</code></a>, except for the boundary updates &#40;see 2.&#41;.</p>
<h3 id=task_3_1-d_parallelisation_with_mpi ><a href="#task_3_1-d_parallelisation_with_mpi" class=header-anchor >Task 3 &#40;1-D parallelisation with MPI&#41;</a></h3>
<p>Run the code <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_diffusion_1D_mpi.jl</code></a> which is still missing the boundary updates three times: with 1, 2 and 4 processes &#40;replacing <code>np</code> by the number of processes&#41;:</p>
<pre><code class="sh hljs">mpiexecjl -n &lt;np&gt; julia --project &lt;my_script.jl&gt;</code></pre>
<p>Visualise the results after each run with the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_vizme1D_mpi.jl</code></a> code &#40;<em><strong>adapt the variable <code>nprocs</code>&#33;</strong></em>&#41;. Describe what you see in the visualisation. Then, add the required boundary update in order make the code work properly and run it again. Note what has changed in the visualisation.</p>
<div class=note ><div class=title >💡 Note</div>
<div class=messg >For the boundary updates, you can use the following approach for the communication with each neighbour: 1&#41; create a <code>sendbuffer</code> and receive buffer, storing the right value in the send buffer; 2&#41; use <code>MPI.Send</code> and <code>MPI.Recv&#33;</code> to send/receive the data; 3&#41; store the received data in the right position in the array.</div></div>
<p>Congratulations&#33; You just did a distributed memory diffusion solver in only 70 lines of code.</p>
<p>Let us now do the same in 2D: there is not much new there, but it may be interesting to work out how boundary update routines can be defined in 2D as one now needs to exchange vectors instead of single values.</p>
<h3 id=task_4_2-d_parallelisation_with_mpi ><a href="#task_4_2-d_parallelisation_with_mpi" class=header-anchor >Task 4 &#40;2-D parallelisation with MPI&#41;</a></h3>
<p>Run the code <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_diffusion_2D_mpi.jl</code></a> which is still missing the boundary updates three times: with 1, 2 and 4 processes.</p>
<p>Visualise the results after each run with the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_vizme2D_mpi.jl</code></a> code &#40;adapt the variable <code>nprocs</code>&#33;&#41;. Describe what you see in the visualisation. Then, add the required boundary update in order make the code work properly and run it again. Note what has changed in the visualisation.</p>
<div class=img-med ><img src="../assets/literate_figures/l8_diff_2D_mpi.png" alt="diffusion 2D MPI" /></div>
<p>The last step is to create a multi-GPU solver out of the above multi-CPU solver. CUDA-aware MPI is of great help in this task, because it allows to directly pass GPU arrays to the MPI functions.</p>
<p>Besides facilitating the programming, it can leverage Remote Direct Memory Access &#40;RDMA&#41; which can be of great benefit in many HPC scenarios.</p>
<h3 id=task_5_multi-gpu_homework ><a href="#task_5_multi-gpu_homework" class=header-anchor >Task 5 &#40;multi-GPU&#41; <em><strong>HOMEWORK</strong></em></a></h3>
<p>Translate the code <code>diffusion_2D_mpi.jl</code> from Task 4 to GPU using GPU array programming. Note what changes were needed to go from CPU to GPU in this distributed solver.</p>
<div class=note ><div class=title >💡 Note</div>
<div class=messg >You can use a similar approach as in the CPU code to perform the boundary updates. You can use <code>copyto&#33;</code> function in order to copy the data from the GPU memory into the send buffers &#40;CPU memory&#41; or to copy the receive buffer data to the GPU array.</div></div>
<p>Head to the <a href="#exercises_-_lecture_8">exercise section</a> for further directions on this task which is part of this week&#39;s homework assignments.</p>
<div class=note ><div class=title >💡 Note</div>
<div class=messg >As alternative, one could use the same approach as in the CPU code to perform the boundary updates thanks to CUDA-aware MPI &#40;it allows to pass GPU arrays directly to the MPI functions&#41;. However, this requires MPI being specifically built for that purpose.</div></div>
<p>This completes the introduction to distributed parallelisation with Julia.</p>
<p>Note that high-level Julia packages as for example <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl">ImplicitGlobalGrid.jl</a> can render distributed parallelisation with GPU and CPU for HPC a very simple task. Let&#39;s check it out&#33;</p>
<h2 id=using_implicitglobalgridjl ><a href="#using_implicitglobalgridjl" class=header-anchor >Using <code>ImplicitGlobalGrid.jl</code></a></h2>
<p>Let&#39;s have look at <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl">ImplicitGlobalGrid.jl</a>&#39;s repository.</p>
<p>ImplicitGlobalGrid.jl can render distributed parallelisation with GPU and CPU for HPC a very simple task. Moreover, ImplicitGlobalGrid.jl elegantly combines with <a href="https://github.com/omlins/ParallelStencil.jl">ParallelStencil.jl</a>.</p>
<p>Finally, the cool part: using both packages together enables to <a href="https://github.com/omlins/ParallelStencil.jl#seamless-interoperability-with-communication-packages-and-hiding-communication">hide communication behind computation</a>. This feature enables a parallel efficiency close to 1.</p>
<p>For this demo, we&#39;ll start from the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_diffusion_2D_perf_xpu.jl</code></a> code.</p>
<p>Only a few changes are required to enable multi-xPU execution, namely:</p>
<ol>
<li><p>initialise the implicit global grid</p>

<li><p>use global coordinates to compute the initial condition</p>

<li><p>update halo &#40;and overlap communication with computation&#41;</p>

<li><p>finalise the global grid</p>

<li><p>tune visualisation</p>

</ol>
<p>But before we start programming the multi-xPU implementation, let&#39;s get setup with GPU MPI on Piz Daint. Follow steps are needed:</p>
<ul>
<li><p>Launch a <code>salloc</code> on 4 nodes</p>

<li><p>Install the required MPI-related packages</p>

<li><p>Test your setup running <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_hello_mpi.jl</code></a> and <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_hello_mpi_gpu.jl</code></a> scripts on 1-4 nodes</p>

</ul>
<div class=note ><div class=title >💡 Note</div>
<div class=messg >See <a href="/software_install/#julia_mpi_gpu_on_piz_daint">Julia MPI GPU on Piz Daint</a> for detailed information on how to run MPI GPU &#40;multi-GPU&#41; applications on Piz Daint.</div></div>
<p>To &#40;1.&#41; initialise the global grid, one first needs to use the package</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> ImplicitGlobalGrid</code></pre>
<p>Then, one can add the global grid initialisation in the <code># Derived numerics</code> section</p>
<pre><code class="julia hljs">me, dims = init_global_grid(nx, ny, <span class=hljs-number >1</span>)  <span class=hljs-comment ># Initialization of MPI and more...</span>
dx, dy  = Lx/nx_g(), Ly/ny_g()</code></pre>
<div class=note ><div class=title >💡 Note</div>
<div class=messg >Have a look at the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_hello_mpi_gpu.jl</code></a> code to get an idea on how to select a GPU based on node-local MPI infos.</div></div>
<p>Then, for &#40;2.&#41;, one can use <code>x_g&#40;&#41;</code> and <code>y_g&#40;&#41;</code> to compute the global coordinates in the initialisation &#40;to correctly spread the Gaussian distribution over all local processes&#41;</p>
<pre><code class="julia hljs">C       = <span class=hljs-meta >@zeros</span>(nx,ny)
C      .= Data.<span class=hljs-built_in >Array</span>([exp(-(x_g(ix,dx,C)+dx/<span class=hljs-number >2</span> -Lx/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span> -(y_g(iy,dy,C)+dy/<span class=hljs-number >2</span> -Ly/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span>) <span class=hljs-keyword >for</span> ix=<span class=hljs-number >1</span>:size(C,<span class=hljs-number >1</span>), iy=<span class=hljs-number >1</span>:size(C,<span class=hljs-number >2</span>)])</code></pre>
<p>The halo update &#40;3.&#41; can be simply performed adding following line after the <code>compute&#33;</code> kernel</p>
<pre><code class="julia hljs">update_halo!(C)</code></pre>
<p>Now, when running on GPUs, it is possible to hide MPi communication behind computations&#33; This option implements as:</p>
<pre><code class="julia hljs"><span class=hljs-meta >@hide_communication</span> (<span class=hljs-number >8</span>, <span class=hljs-number >2</span>) <span class=hljs-keyword >begin</span>
    <span class=hljs-meta >@parallel</span> compute!(C2, C, D_dx, D_dy, dt, _dx, _dy, size_C1_2, size_C2_2)
    C, C2 = C2, C <span class=hljs-comment ># pointer swap</span>
    update_halo!(C)
<span class=hljs-keyword >end</span></code></pre>
<p>The <code>@hide_communication &#40;8, 2&#41;</code> will first compute the first and last 8 and 2 grid points in x and y dimension, respectively. Then, while exchanging boundaries the rest of the local domains computations will be perform &#40;overlapping the MPI communication&#41;.</p>
<p>To &#40;4.&#41; finalise the global grid,</p>
<pre><code class="julia hljs">finalize_global_grid()</code></pre>
<p>needs to be added before the <code>return</code> of the &quot;main&quot;.</p>
<p>The last changes to take car of is to &#40;5.&#41; handle visualisation in an appropriate fashion. Here, several options exists.</p>
<ul>
<li><p>One approach would for each local process to dump the local domain results to a file &#40;with process ID <code>me</code> in the filename&#41; in order to reconstruct to global grid with a post-processing visualisation script &#40;as done in the previous examples&#41;. Libraries like, e.g., <a href="https://adios2.readthedocs.io/en/latest">ADIOS2</a> may help out there.</p>

<li><p>Another approach would be to gather the global grid results on a master process before doing further steps as disk saving or plotting.</p>

</ul>
<p>To implement the latter and generate a <code>gif</code>, one needs to define a global array for visualisation:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >if</span> do_visu
    <span class=hljs-keyword >if</span> (me==<span class=hljs-number >0</span>) <span class=hljs-literal >ENV</span>[<span class=hljs-string >&quot;GKSwstype&quot;</span>]=<span class=hljs-string >&quot;nul&quot;</span>; <span class=hljs-keyword >if</span> isdir(<span class=hljs-string >&quot;viz2D_mxpu_out&quot;</span>)==<span class=hljs-literal >false</span> mkdir(<span class=hljs-string >&quot;viz2D_mxpu_out&quot;</span>) <span class=hljs-keyword >end</span>; loadpath = <span class=hljs-string >&quot;./viz2D_mxpu_out/&quot;</span>; anim = Animation(loadpath,<span class=hljs-built_in >String</span>[]); println(<span class=hljs-string >&quot;Animation directory: <span class=hljs-subst >$(anim.dir)</span>&quot;</span>) <span class=hljs-keyword >end</span>
    nx_v, ny_v = (nx-<span class=hljs-number >2</span>)*dims[<span class=hljs-number >1</span>], (ny-<span class=hljs-number >2</span>)*dims[<span class=hljs-number >2</span>]
    <span class=hljs-keyword >if</span> (nx_v*ny_v*sizeof(Data.<span class=hljs-built_in >Number</span>) &gt; <span class=hljs-number >0.8</span>*Sys.free_memory()) error(<span class=hljs-string >&quot;Not enough memory for visualization.&quot;</span>) <span class=hljs-keyword >end</span>
    C_v   = zeros(nx_v, ny_v) <span class=hljs-comment ># global array for visu</span>
    C_inn = zeros(nx-<span class=hljs-number >2</span>, ny-<span class=hljs-number >2</span>) <span class=hljs-comment ># no halo local array for visu</span>
    Xi_g, Yi_g = <span class=hljs-built_in >LinRange</span>(dx+dx/<span class=hljs-number >2</span>, Lx-dx-dx/<span class=hljs-number >2</span>, nx_v), <span class=hljs-built_in >LinRange</span>(dy+dy/<span class=hljs-number >2</span>, Ly-dy-dy/<span class=hljs-number >2</span>, ny_v) <span class=hljs-comment ># inner points only</span>
<span class=hljs-keyword >end</span></code></pre>
<p>Then, the plotting routine can be adapted to first gather the inner points of the local domains into the global array &#40;using <code>gather&#33;</code> function&#41; and then plot and/or save the global array &#40;here <code>C_v</code>&#41; from the master process <code>me&#61;&#61;0</code>:</p>
<pre><code class="julia hljs"><span class=hljs-comment ># Visualize</span>
<span class=hljs-keyword >if</span> do_visu &amp;&amp; (it % nout == <span class=hljs-number >0</span>)
    C_inn .= <span class=hljs-built_in >Array</span>(C)[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>,<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>]; gather!(C_inn, C_v)
    <span class=hljs-keyword >if</span> (me==<span class=hljs-number >0</span>)
        opts = (aspect_ratio=<span class=hljs-number >1</span>, xlims=(Xi_g[<span class=hljs-number >1</span>], Xi_g[<span class=hljs-keyword >end</span>]), ylims=(Yi_g[<span class=hljs-number >1</span>], Yi_g[<span class=hljs-keyword >end</span>]), clims=(<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), c=:turbo, xlabel=<span class=hljs-string >&quot;Lx&quot;</span>, ylabel=<span class=hljs-string >&quot;Ly&quot;</span>, title=<span class=hljs-string >&quot;time = <span class=hljs-subst >$(round(it*dt, sigdigits=<span class=hljs-number >3</span>)</span>)&quot;</span>)
        heatmap(Xi_g, Yi_g, <span class=hljs-built_in >Array</span>(C_v)&#x27;; opts...); frame(anim)
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre>
<p>To finally generate the <code>gif</code>, one needs to place the following after the time loop:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >if</span> (do_visu &amp;&amp; me==<span class=hljs-number >0</span>) gif(anim, <span class=hljs-string >&quot;diffusion_2D_mxpu.gif&quot;</span>, fps = <span class=hljs-number >5</span>)  <span class=hljs-keyword >end</span></code></pre>
<div class=note ><div class=title >💡 Note</div>
<div class=messg >We here did not rely on CUDA-aware MPI. To use this feature set &#40;and export&#41; <code>IGG_CUDAAWARE_MPI&#61;1</code>. Note that the examples using ImplicitGlobalGrid.jl would also work if <code>USE_GPU &#61; false</code>; however, the communication and computation overlap feature is then currently not yet available as its implementation relies at present on leveraging CUDA streams.</div></div>
<h3 id=wrapping_up ><a href="#wrapping_up" class=header-anchor >Wrapping up</a></h3>
<p>Let&#39;s recall what we learned today about distributed computing in Julia using GPUs:</p>
<ul>
<li><p>We used fake parallelisation to understand the correct boundary exchange procedure.</p>

<li><p>We implemented 1D and 2D diffusion solvers in Julia using MPI for distributed memory parallelisation on both CPUs and GPUs &#40;using blocking communication&#41;.</p>

<li><p>We saw how combining <code>ParallelStencil.jl</code> with <code>ImplicitGlobalGrid.jl</code> permits to implement distributed memory parallelisation on multiple CPU and GPUs.</p>

</ul>

<p><a href="#content">⤴ <em><strong>back to Content</strong></em></a></p>
<h1 id=documenting_your_code ><a href="#documenting_your_code" class=header-anchor >Documenting your code</a></h1>
<p>This lecture we will learn:</p>
<ul>
<li><p>documentation vs code-comments</p>

<li><p>why to write documentation</p>

<li><p>some Julia tools:</p>
<ul>
<li><p>docstrings</p>

<li><p><a href="https://github.com/JuliaDocs/Documenter.jl">https://github.com/JuliaDocs/Documenter.jl</a></p>

<li><p><a href="https://github.com/fredrikekre/Literate.jl">https://github.com/fredrikekre/Literate.jl</a></p>

</ul>

</ul>
<p><img src="https://pcweenies.com/wp-content/uploads/2012/01/2012-01-12_pcw.jpg" alt=comic  /></p>
<h3 id=why_should_i_document_my_code ><a href="#why_should_i_document_my_code" class=header-anchor >Why should I document my code?</a></h3>
<p>Why should I write code comments?</p>
<ul>
<li><p><a href="https://blog.codinghorror.com/code-tells-you-how-comments-tell-you-why/">&quot;Code Tells You How, Comments Tell You Why&quot;</a></p>
<ul>
<li><p>code should be made understandable by itself, as much as possible</p>

<li><p>comments then should be to tell the &quot;why&quot; you&#39;re doing something</p>

</ul>

<li><p><em>but</em> I do a lot of structuring comments as well</p>

<li><p>math-y variables tend to be short and need a comment as well</p>

</ul>
<p>Why should I write documentation?</p>
<ul>
<li><p>documentation should give a bigger overview of what your code does</p>
<ul>
<li><p>at the function-level &#40;doc-strings&#41;</p>

<li><p>at the package-level &#40;README, full-fledged documentation&#41;</p>

</ul>

<li><p>to let other people and your future self &#40;probably most important&#41; understand what your code is about</p>

</ul>
<h3 id=documentation_easily_rots ><a href="#documentation_easily_rots" class=header-anchor >Documentation easily rots...</a></h3>
<p>Worse than no documentation/code comments is documentation which is outdated.</p>
<p>I find the best way to keep documentation up to date is:</p>
<ul>
<li><p>have documentation visible to you, e.g. GitHub README</p>

<li><p>document what you need yourself</p>

<li><p>use examples and run them as part of CI &#40;doc-tests, example-scripts&#41;</p>

</ul>
<h3 id=documentation_tools_doc-strings ><a href="#documentation_tools_doc-strings" class=header-anchor >Documentation tools: doc-strings</a></h3>
<p>A Julia doc-string &#40;<a href="https://docs.julialang.org/en/v1/manual/documentation/">Julia manual</a>&#41;:</p>
<ul>
<li><p>is just a string before the object &#40;no new-line&#41;; interpreted as markdown-string</p>

<li><p>can be attached to most things &#40;functions, variables, modules, macros, types&#41;</p>

<li><p>can be queried with <code>?</code></p>

</ul>
<pre><code class="julia hljs"><span class=hljs-string >&quot;&quot;&quot;
    transform(r, θ) = (r*cos(θ), r*sin(θ))

Transform polar to cartesian coordinates.
&quot;&quot;&quot;</span>
transform(r, θ) = (r*cos(θ), r*sin(θ))

<span class=hljs-string >&quot;Typical size of beer crate&quot;</span>
<span class=hljs-keyword >const</span> BEERBOX = <span class=hljs-number >12</span></code></pre>
<pre><code class="julia hljs">?BEERBOX</code></pre>
<h3 id=documentation_tools_doc-strings_with_examples ><a href="#documentation_tools_doc-strings_with_examples" class=header-anchor >Documentation tools: doc-strings with examples</a></h3>
<p>One can add examples to doc-strings &#40;they can even be part of testing: <a href="https://juliadocs.github.io/Documenter.jl/stable/man/doctests/">doc-tests</a>&#41;.</p>
<ul>
<li><p>run it in the REPL and copy paste to the docstring</p>

</ul>
<pre><code class="julia hljs"><span class=hljs-string >&quot;&quot;&quot;
    transform(r, θ) = (r*cos(θ), r*sin(θ))

Transform polar to cartesian coordinates.

# Example
```jldoctest
julia&gt; transform(4.5, pi/5)
(3.6405764746872635, 2.6450336353161292)
```
&quot;&quot;&quot;</span>
transform(r, θ) = (r*cos(θ), r*sin(θ))</code></pre>
<pre><code class="julia hljs">?transform</code></pre>
<h3 id=documentation_tools_github_markdown_rendering ><a href="#documentation_tools_github_markdown_rendering" class=header-anchor >Documentation tools: Github markdown rendering</a></h3>
<p>The easiest way to write long-form documentation is to just use GitHub&#39;s markdown rendering.</p>
<p>A nice example is <a href="https://github.com/luraess/parallel-gpu-workshop-JuliaCon21#parallel-cpu-implementation">this short course</a> by Ludovic &#40;incidentally about solving PDEs on GPUs 🙂&#41;.</p>
<ul>
<li><p>images are rendered</p>

<li><p>in-page links are easy, e.g. <code>&#91;_back to workshop material_&#93;&#40;#workshop-material&#41;</code></p>

<li><p>top-left has a burger-menu for page navigation</p>

<li><p>can be edited within the web-page &#40;pencil-icon&#41;</p>

</ul>
<p>👉 this is a good and low-overhead way to produce pretty nice documentation</p>
<h3 id=documentation_tools_literatejl ><a href="#documentation_tools_literatejl" class=header-anchor >Documentation tools: Literate.jl</a></h3>
<p>There are several tools which render .jl files &#40;with special formatting&#41; into markdown files.  These files can then be added to Github and will be rendered there.</p>
<ul>
<li><p>we&#39;re using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a></p>

<li><p>format is described <a href="https://fredrikekre.github.io/Literate.jl/v2/fileformat/">here</a></p>

<li><p>files stay valid Julia scripts, i.e. they can be executed without Literate.jl</p>

</ul>
<p>Example</p>
<ul>
<li><p>output markdown in: <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00-L8Documentation.jl/blob/4bbeb3ddda046490847f050b02d3fc5d9308695b/scripts/car_travels.jl">course-101-0250-00-L8Documentation.jl: scripts/car_travels.jl</a></p>

<li><p>output markdown in: <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00-L8Documentation.jl/blob/4bbeb3ddda046490847f050b02d3fc5d9308695b/scripts/car_travels.md">course-101-0250-00-L8Documentation.jl: scripts/car_travels.md</a></p>

</ul>
<pre><code class="julia hljs">Literate.markdown(<span class=hljs-string >&quot;car_travels.jl&quot;</span>, directory_of_this_file, execute=<span class=hljs-literal >true</span>, documenter=<span class=hljs-literal >false</span>, credit=<span class=hljs-literal >false</span>)</code></pre>
<p>But this is not automatic&#33;  Manual steps: run Literate, add files, commit and push...</p>
<h3 id=documentation_tools_automating_literatejl ><a href="#documentation_tools_automating_literatejl" class=header-anchor >Documentation tools: Automating Literate.jl</a></h3>
<p>As is done on <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00-L8Documentation.jl">course-101-0250-00-L8Documentation.jl</a></p>
<pre><code class="yml hljs"><span class=hljs-attr >name:</span> <span class=hljs-string >Run</span> <span class=hljs-string >Literate.jl</span>
<span class=hljs-comment ># adapted from https://lannonbr.com/blog/2019-12-09-git-commit-in-actions</span>

<span class=hljs-attr >on:</span> <span class=hljs-string >push</span>

<span class=hljs-attr >jobs:</span>
  <span class=hljs-attr >lit:</span>
    <span class=hljs-attr >runs-on:</span> <span class=hljs-string >ubuntu-latest</span>
    <span class=hljs-attr >steps:</span>
      <span class=hljs-comment ># Checkout the branch</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >actions/checkout@v2</span>

      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >julia-actions/setup-julia@v1</span>
        <span class=hljs-attr >with:</span>
          <span class=hljs-attr >version:</span> <span class=hljs-string >&#x27;1.8&#x27;</span>
          <span class=hljs-attr >arch:</span> <span class=hljs-string >x64</span>

      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >actions/cache@v1</span>
        <span class=hljs-attr >env:</span>
          <span class=hljs-attr >cache-name:</span> <span class=hljs-string >cache-artifacts</span>
        <span class=hljs-attr >with:</span>
          <span class=hljs-attr >path:</span> <span class=hljs-string >~/.julia/artifacts</span>
          <span class=hljs-attr >key:</span> <span class=hljs-string >${{</span> <span class=hljs-string >runner.os</span> <span class=hljs-string >}}-test-${{</span> <span class=hljs-string >env.cache-name</span> <span class=hljs-string >}}-${{</span> <span class=hljs-string >hashFiles(&#x27;**/Project.toml&#x27;)</span> <span class=hljs-string >}}</span>
          <span class=hljs-attr >restore-keys:</span> <span class=hljs-string >|
            ${{ runner.os }}-test-${{ env.cache-name }}-
            ${{ runner.os }}-test-
            ${{ runner.os }}-
</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >julia-actions/julia-buildpkg@v1</span>

      <span class=hljs-bullet >-</span> <span class=hljs-attr >name:</span> <span class=hljs-string >run</span> <span class=hljs-string >Literate</span>
        <span class=hljs-attr >run:</span> <span class=hljs-string >julia</span> <span class=hljs-string >--color=yes</span> <span class=hljs-string >--project</span> <span class=hljs-string >-e</span> <span class=hljs-string >&#x27;cd(&quot;scripts&quot;); include(&quot;literate-script.jl&quot;)&#x27;</span>

      <span class=hljs-bullet >-</span> <span class=hljs-attr >name:</span> <span class=hljs-string >setup</span> <span class=hljs-string >git</span> <span class=hljs-string >config</span>
        <span class=hljs-attr >run:</span> <span class=hljs-string >|
          # setup the username and email. I tend to use &#x27;GitHub Actions Bot&#x27; with no email by default
          git config user.name &quot;GitHub Actions Bot&quot;
          git config user.email &quot;&lt;&gt;&quot;
</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >name:</span> <span class=hljs-string >commit</span>
        <span class=hljs-attr >run:</span> <span class=hljs-string >|
          # Stage the file, commit and push
          git add scripts/md/*
          git commit -m &quot;Commit markdown files fom Literate&quot;
          git push origin master</span></code></pre>
<h3 id=documentation_tools_documenterjl ><a href="#documentation_tools_documenterjl" class=header-anchor >Documentation tools: Documenter.jl</a></h3>
<p>If you want to have full-blown documentation, including, e.g., automatic API documentation generation, versioning, then use <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a>.</p>
<p>Examples:</p>
<ul>
<li><p><a href="https://docs.julialang.org/en/v1/">https://docs.julialang.org/en/v1/</a></p>

<li><p><a href="https://mauro3.github.io/Parameters.jl/stable/">https://mauro3.github.io/Parameters.jl/stable/</a></p>

</ul>
<p><em><strong>Notes:</strong></em></p>
<ul>
<li><p>it&#39;s geared towards Julia-packages, less for a bunch-of-scripts as in our lecture</p>

<li><p>Documenter.jl also integrates with Literate.jl.</p>

<li><p>for more free-form websites, use <a href="https://github.com/tlienart/Franklin.jl">https://github.com/tlienart/Franklin.jl</a> &#40;as the course website does&#41;</p>

<li><p>if you want to use it, it&#39;s easiest to generate your package with <a href="https://github.com/invenia/PkgTemplates.jl">PkgTemplates.jl</a> which will generate the Documenter-setup for you.</p>

</ul>

<p><a href="#content">⤴ <em><strong>back to Content</strong></em></a></p>
<h1 id=exercises_-_lecture_8 ><a href="#exercises_-_lecture_8" class=header-anchor >Exercises - lecture 8</a></h1>
<div class=warning ><div class=title >⚠️ Warning&#33;</div>
<div class=messg ><strong>Exercise 1</strong> has to be handed in and uploaded to your private &#40;shared&#41; GitHub repository, in a <em><strong>newly created <code>lecture_8</code></strong></em> folder. The git commit hash &#40;or SHA&#41; of the final push needs to be uploaded on Moodle &#40;<a href="/homework">more</a>&#41;.<br /><strong>Exercise 2</strong> contributes to your project and has to be handed in and uploaded to your private &#40;shared&#41; GitHub repository, in the <strong>PorousConvection</strong> project folder. The git commit hash &#40;or SHA&#41; of the final push needs to be uploaded on Moodle &#40;<a href="/homework">more</a>&#41;.</div></div>
<h2 id=exercise_1_-_towards_distributed_memory_computing_on_gpus ><a href="#exercise_1_-_towards_distributed_memory_computing_on_gpus" class=header-anchor >Exercise 1 - <strong>Towards distributed memory computing on GPUs</strong></a></h2>
<p>👉 See <a href="/logistics/#submission">Logistics</a> for submission details.</p>
<p>The goal of this exercise is to:</p>
<ul>
<li><p>Familiarise with distributed computing</p>

<li><p>Learn about MPI on the way</p>

</ul>
<p>In this exercise, you will:</p>
<ul>
<li><p>finalise the fake parallelisation scripts discussed in lecture 8 &#40;2 procs and <code>n</code> procs&#41;</p>

<li><p>finalise the 2D Julia MPI script</p>

<li><p>Create a Julia MPI GPU version of the 2D Julia MPI script discussed <a href="#task_5_multi-gpu_homework">here</a></p>

</ul>
<p>Create a new <code>lectrue_8</code> folder for this first exercise in your shared private GitHub repository for this week&#39;s exercises.</p>
<h3 id=task_1 ><a href="#task_1" class=header-anchor >Task 1</a></h3>
<p>Finalise the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_diffusion_1D_2procs.jl</code></a> and <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_diffusion_1D_nprocs.jl</code></a> scripts discussed during lecture 8. Make sure to correctly implement the halo update in order to exchange the internal boundaries among the fake parallel processes &#40;left and right and <code>ip</code> in the &quot;2procs&quot; and &quot;nprocs&quot; codes, respectively&#41;. See <a href="#fake_parallelisation">here</a> for details.</p>
<p>Report in two separate figures the final distribution of concentration <code>C</code> for both fake parallel codes. Include these figure in a first section of your lecture&#39;s 8 <code>README.md</code> adding a description sentence to each.</p>
<h3 id=task_2 ><a href="#task_2" class=header-anchor >Task 2</a></h3>
<p>Finalise the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_diffusion_2D_mpi.jl</code></a> script discussed during lecture 8. In particular, finalise the <code>update_halo</code> functions to allow for correct internal boundary exchange among the distributed parallel MPI processes. Add the final code to your GitHub lecture 8 folder.</p>
<p>For each of the &#40;4&#41; neighbour exchanges:</p>
<ol>
<li><p>start by defining a sendbuffer <code>sendbuf</code> to hold the vector you need to send</p>

<li><p>initialise a receive buffer <code>recvbuf</code> to later hold the vector received from the corresponding neighbouring process</p>

<li><p>use <code>MPI.Send</code> and <code>MPI.Recv&#33;</code> functions to perform the boundary exchange</p>

<li><p>Assign the values within the receive buffer to the corresponding row or column of the array <code>A</code></p>

</ol>
<div class=note ><div class=title >💡 Note</div>
<div class=messg >Apply similar overlap and halo update as in the fake parallelisation examples. Look-up <a href="https://juliaparallel.github.io/MPI.jl/latest/pointtopoint/#MPI.Send">MPI.Send</a> and <a href="https://juliaparallel.github.io/MPI.jl/latest/pointtopoint/#MPI.Recv&#33;">MPI.recv&#33;</a> for further details.</div></div>
<p>In a new section of your lecture&#39;s 8 <code>README.md</code>, add a .gif animation showing the diffusion of the quantity <code>C</code>, <strong>running on 4 MPI processes</strong>, for the physical and numerical parameters suggested in the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/l8_diffusion_2D_mpi.jl">initial file</a>. Add a short description of the results and provide the command used to launch the script in the <code>README.md</code> as well.</p>
<h3 id=task_3 ><a href="#task_3" class=header-anchor >Task 3</a></h3>
<p>Create a multi-GPU implementation of the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/"><code>l8_diffusion_2D_mpi.jl</code></a> script as suggested <a href="#task_5_multi-gpu_homework">here</a>. To this end, create a new script <code>l8_diffusion_2D_mpi_gpu.jl</code> that you will upload to your lecture 8 GitHub repository upon completion.</p>
<p>Translate the <code>l8_diffusion_2D_mpi.jl</code> code from exercise 1 &#40;task 3&#41; to GPU using GPU array programming. You can use a similar approach as in the CPU code to perform the boundary updates. You should use <code>copyto&#33;</code> function in order to copy the data from the GPU memory into the send buffers &#40;CPU memory&#41; or to copy the receive buffer data to the GPU array.</p>
<p>The steps to realise this task summarise as following:</p>
<ol>
<li><p>use GPU array initialisation &#40;<code>CUDA.zeros</code>, <code>CuArray&#40;&#41;</code>, ...&#41;</p>

<li><p>gather the GPU arrays back on the host memory for visualisation or saving &#40;using <code>Array&#40;&#41;</code>&#41;</p>

<li><p>modify the <code>update_halo</code> function; use <code>copyto&#33;</code> to copy device data to the host into the send buffer or to copy host data to the device from the receive buffer</p>

</ol>
<p>In a new &#40;3rd&#41; section of your lecture&#39;s 8 <code>README.md</code>, add .gif animation showing the diffusion of the quantity <code>C</code>, <strong>running on 4 GPUs &#40;MPI processes&#41;</strong>, for the physical and numerical parameters suggested in the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l8_scripts/l8_diffusion_2D_mpi.jl">initial file</a>. Add a short description of the results and provide the command used to launch the script in the <code>README.md</code> as well. Note what changes were needed to go from CPU to GPU in this distributed solver.</p>

<p><a href="#content">⤴ <em><strong>back to Content</strong></em></a></p>
<hr />
<h2 id=exercise_2_-_multi-xpu_computing ><a href="#exercise_2_-_multi-xpu_computing" class=header-anchor >Exercise 2 - <strong>Multi-xPU computing</strong></a></h2>
<p>👉 See <a href="/logistics/#submission">Logistics</a> for submission details.</p>
<p>The goal of this exercise is to:</p>
<ul>
<li><p>Familiarise with distributed computing</p>

<li><p>Combine <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl">ImplicitGlobalGrid.jl</a> and <a href="https://github.com/omlins/ParallelStencil.jl">ParallelStencil.jl</a></p>

<li><p>Learn about GPU MPI on the way</p>

</ul>
<div class=warning ><div class=title >⚠️ Warning&#33;</div>
<div class=messg >Code from this exercise 2 has to be uploaded to the <code>scripts</code> folder within your <code>PorousConvection</code> project.</div></div>
<p>In this exercise, you will:</p>
<ul>
<li><p>Create a multi-xPU version of your thermal porous convection 3D xPU code you finalised in lecture 7</p>

<li><p>Keep it xPU compatible using <code>ParallelStencil.jl</code></p>

<li><p>Deploy it on multiple xPUs using <code>ImplicitGlobalGrid.jl</code></p>

</ul>
<p>👉 You&#39;ll find a version of the <code>PorousConvection_3D_xpu.jl</code> code in the solutions folder on Polybox after exercises deadline if needed to get you started.</p>
<ol>
<li><p>Copy your working <code>PorousConvection_3D_xpu.jl</code> code developed for the exercises in Lecture 7 and rename it <code>PorousConvection_3D_multixpu.jl</code>.</p>

<li><p>Add at the beginning of the code</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> ImplicitGlobalGrid,MPI</code></pre>
<ol start=3 >
<li><p>Also add global maximum computation using MPI</p>

</ol>
<pre><code class="julia hljs">max_g(A) = (max_l = maximum(A); MPI.Allreduce(max_l, MPI.MAX, MPI.COMM_WORLD))</code></pre>
<ol start=4 >
<li><p>In the <code># numerics</code> section, initialise the global grid right after defining <code>nx,ny,nz</code> and use now global grid <code>nx_g&#40;&#41;</code>,<code>ny_g&#40;&#41;</code> and <code>nz_g&#40;&#41;</code> for defining <code>maxiter</code> and <code>ncheck</code>, as well as in any other places when needed.</p>

</ol>
<pre><code class="julia hljs">nx,ny       = <span class=hljs-number >2</span>*(nz+<span class=hljs-number >1</span>)-<span class=hljs-number >1</span>,nz
me, dims    = init_global_grid(nx, ny, nz)  <span class=hljs-comment ># init global grid and more</span>
b_width     = (<span class=hljs-number >8</span>,<span class=hljs-number >8</span>,<span class=hljs-number >4</span>)                       <span class=hljs-comment ># for comm / comp overlap</span></code></pre>
<ol start=5 >
<li><p>Modify the temperature initialisation using ImplicitGlobalGrid&#39;s global coordinate helpers &#40;<code>x_g</code>, etc...&#41;, including one internal boundary condition update &#40;update halo&#41;:</p>

</ol>
<pre><code class="julia hljs">T           = <span class=hljs-meta >@zeros</span>(nx  ,ny  ,nz  )
T          .= Data.<span class=hljs-built_in >Array</span>([ΔT*exp(-(x_g(ix,dx,T)+dx/<span class=hljs-number >2</span>-lx/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span> -(y_g(iy,dy,T)+dy/<span class=hljs-number >2</span>-ly/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span> -(z_g(iz,dz,T)+dz/<span class=hljs-number >2</span>-lz/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span>) <span class=hljs-keyword >for</span> ix=<span class=hljs-number >1</span>:size(T,<span class=hljs-number >1</span>),iy=<span class=hljs-number >1</span>:size(T,<span class=hljs-number >2</span>),iz=<span class=hljs-number >1</span>:size(T,<span class=hljs-number >3</span>)])
T[:,:,<span class=hljs-number >1</span>].=ΔT/<span class=hljs-number >2</span>; T[:,:,<span class=hljs-keyword >end</span>].=-ΔT/<span class=hljs-number >2</span>
update_halo!(T)
T_old       = copy(T)</code></pre>
<ol start=6 >
<li><p>Prepare for visualisation, making sure only <code>me&#61;&#61;0</code> creates the output directory. Also, prepare an array for storing inner points only &#40;no halo&#41; <code>T_inn</code> as well as global array to gather subdomains <code>T_v</code></p>

</ol>
<pre><code class="julia hljs"><span class=hljs-keyword >if</span> do_viz
    <span class=hljs-literal >ENV</span>[<span class=hljs-string >&quot;GKSwstype&quot;</span>]=<span class=hljs-string >&quot;nul&quot;</span>
    <span class=hljs-keyword >if</span> (me==<span class=hljs-number >0</span>) <span class=hljs-keyword >if</span> isdir(<span class=hljs-string >&quot;viz3Dmpi_out&quot;</span>)==<span class=hljs-literal >false</span> mkdir(<span class=hljs-string >&quot;viz3Dmpi_out&quot;</span>) <span class=hljs-keyword >end</span>; loadpath=<span class=hljs-string >&quot;viz3Dmpi_out/&quot;</span>; anim=Animation(loadpath,<span class=hljs-built_in >String</span>[]); println(<span class=hljs-string >&quot;Animation directory: <span class=hljs-subst >$(anim.dir)</span>&quot;</span>) <span class=hljs-keyword >end</span>
    nx_v,ny_v,nz_v = (nx-<span class=hljs-number >2</span>)*dims[<span class=hljs-number >1</span>],(ny-<span class=hljs-number >2</span>)*dims[<span class=hljs-number >2</span>],(nz-<span class=hljs-number >2</span>)*dims[<span class=hljs-number >3</span>]
    <span class=hljs-keyword >if</span> (nx_v*ny_v*nz_v*sizeof(Data.<span class=hljs-built_in >Number</span>) &gt; <span class=hljs-number >0.8</span>*Sys.free_memory()) error(<span class=hljs-string >&quot;Not enough memory for visualization.&quot;</span>) <span class=hljs-keyword >end</span>
    T_v   = zeros(nx_v, ny_v, nz_v) <span class=hljs-comment ># global array for visu</span>
    T_inn = zeros(nx-<span class=hljs-number >2</span>, ny-<span class=hljs-number >2</span>, nz-<span class=hljs-number >2</span>) <span class=hljs-comment ># no halo local array for visu</span>
    xi_g,zi_g = <span class=hljs-built_in >LinRange</span>(-lx/<span class=hljs-number >2</span>+dx+dx/<span class=hljs-number >2</span>, lx/<span class=hljs-number >2</span>-dx-dx/<span class=hljs-number >2</span>, nx_v), <span class=hljs-built_in >LinRange</span>(-lz+dz+dz/<span class=hljs-number >2</span>, -dz-dz/<span class=hljs-number >2</span>, nz_v) <span class=hljs-comment ># inner points only</span>
    iframe = <span class=hljs-number >0</span>
<span class=hljs-keyword >end</span></code></pre>
<ol start=7 >
<li><p>Use the <code>max_g</code> function in the timestep <code>dt</code> definition &#40;instead of <code>maximum</code>&#41; as one now needs to gather the global maximum among all MPI processes.</p>

<li><p>Moving to the time loop, add halo update function <code>update_halo&#33;</code> after the kernel that computes the fluid fluxes. You can additionally wrap it in the <code>@hide_communication</code> block to enable communication/computation overlap &#40;using <code>b_width</code> defined above&#41;</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-meta >@hide_communication</span> b_width <span class=hljs-keyword >begin</span>
    <span class=hljs-meta >@parallel</span> compute_Dflux!(qDx,qDy,qDz,Pf,T,k_ηf,_dx,_dy,_dz,αρg,_1_θ_dτ_D)
    update_halo!(qDx,qDy,qDz)
<span class=hljs-keyword >end</span></code></pre>
<ol start=9 >
<li><p>Apply a similar step to the temperature update, where you can also include boundary condition computation as following &#40;⚠️ no other construct is currently allowed&#41;</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-meta >@hide_communication</span> b_width <span class=hljs-keyword >begin</span>
    <span class=hljs-meta >@parallel</span> update_T!(T,qTx,qTy,qTz,dTdt,_dx,_dy,_dz,_1_dt_β_dτ_T)
    <span class=hljs-meta >@parallel</span> (<span class=hljs-number >1</span>:size(T,<span class=hljs-number >2</span>),<span class=hljs-number >1</span>:size(T,<span class=hljs-number >3</span>)) bc_x!(T)
    <span class=hljs-meta >@parallel</span> (<span class=hljs-number >1</span>:size(T,<span class=hljs-number >1</span>),<span class=hljs-number >1</span>:size(T,<span class=hljs-number >3</span>)) bc_y!(T)
    update_halo!(T)
<span class=hljs-keyword >end</span></code></pre>
<ol start=10 >
<li><p>Use now the <code>max_g</code> function instead of <code>maximum</code> to collect the global maximum among all local arrays spanning all MPI processes.</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-comment ># time step</span>
dt = <span class=hljs-keyword >if</span> it == <span class=hljs-number >1</span>
    <span class=hljs-number >0.1</span>*min(dx,dy,dz)/(αρg*ΔT*k_ηf)
<span class=hljs-keyword >else</span>
    min(<span class=hljs-number >5.0</span>*min(dx,dy,dz)/(αρg*ΔT*k_ηf),ϕ*min(dx/max_g(abs.(qDx)), dy/max_g(abs.(qDy)), dz/max_g(abs.(qDz)))/<span class=hljs-number >3.1</span>)
<span class=hljs-keyword >end</span></code></pre>
<ol start=11 >
<li><p>Make sure all printing statements are only executed by <code>me&#61;&#61;0</code> in order to avoid each MPI process to print to screen, and use <code>nx_g&#40;&#41;</code> instead of local <code>nx</code> in the printed statements when assessing the iteration per number of grid points.</p>

<li><p>Update the visualisation and output saving part</p>

</ol>
<pre><code class="julia hljs"><span class=hljs-comment ># visualisation</span>
<span class=hljs-keyword >if</span> do_viz &amp;&amp; (it % nvis == <span class=hljs-number >0</span>)
    T_inn .= <span class=hljs-built_in >Array</span>(T)[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>,<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>,<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>]; gather!(T_inn, T_v)
    <span class=hljs-keyword >if</span> me==<span class=hljs-number >0</span>
        p1=heatmap(xi_g,zi_g,T_v[:,ceil(<span class=hljs-built_in >Int</span>,ny_g()/<span class=hljs-number >2</span>),:]&#x27;;xlims=(xi_g[<span class=hljs-number >1</span>],xi_g[<span class=hljs-keyword >end</span>]),ylims=(zi_g[<span class=hljs-number >1</span>],zi_g[<span class=hljs-keyword >end</span>]),aspect_ratio=<span class=hljs-number >1</span>,c=:turbo)
        <span class=hljs-comment ># display(p1)</span>
        png(p1,<span class=hljs-meta >@sprintf</span>(<span class=hljs-string >&quot;viz3Dmpi_out/%04d.png&quot;</span>,iframe+=<span class=hljs-number >1</span>))
        save_array(<span class=hljs-meta >@sprintf</span>(<span class=hljs-string >&quot;viz3Dmpi_out/out_T_%04d&quot;</span>,iframe),convert.(<span class=hljs-built_in >Float32</span>,T_v))
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre>
<ol start=13 >
<li><p>Finalise the global grid before returning from the main function</p>

</ol>
<pre><code class="julia hljs">finalize_global_grid()
<span class=hljs-keyword >return</span></code></pre>
<p>If you made it up to here, you should now be able to launch your <code>PorousConvection_3D_multixpu.jl</code> code on multiple GPUs. Let&#39;s give it a try 🔥</p>
<p>Make sure to have set following parameters</p>
<pre><code class="julia hljs">lx,ly,lz    = <span class=hljs-number >40.0</span>,<span class=hljs-number >20.0</span>,<span class=hljs-number >20.0</span>
Ra          = <span class=hljs-number >1000</span>
nz          = <span class=hljs-number >63</span>
nx,ny       = <span class=hljs-number >2</span>*(nz+<span class=hljs-number >1</span>)-<span class=hljs-number >1</span>,nz
b_width     = (<span class=hljs-number >8</span>,<span class=hljs-number >8</span>,<span class=hljs-number >4</span>) <span class=hljs-comment ># for comm / comp overlap</span>
nt          = <span class=hljs-number >500</span>
nvis        = <span class=hljs-number >50</span></code></pre>
<p>Then, launch the script on Piz Daint on 8 GPU nodes upon adapting the the <code>runme_mpi_daint.sh</code> or <code>sbatch sbatch_mpi_daint.sh</code> scripts &#40;see <a href="/software_install/#cuda-aware_mpi_on_piz_daint">here</a>&#41; using CUDA-aware MPI 🚀</p>
<p>The final 2D slice &#40;at <code>ny_g&#40;&#41;/2</code>&#41; produced should look as following and take about 25min to run:</p>
<p><img src="../assets/literate_figures/l8_ex2_porous_convect_mpi_sl.png" alt="3D porous convection MPI" /></p>
<h3 id=task ><a href="#task" class=header-anchor >Task</a></h3>
<p>Now that you made sure the code runs as expected, launch <code>PorousConvection_3D_multixpu.jl</code> for 4000 steps on 8 GPUs at higher resolution &#40;global grid of &#41; setting:</p>
<pre><code class="julia hljs">nz          = <span class=hljs-number >127</span>
nx,ny       = <span class=hljs-number >2</span>*(nz+<span class=hljs-number >1</span>)-<span class=hljs-number >1</span>,nz
nt          = <span class=hljs-number >4000</span>
nvis        = <span class=hljs-number >100</span></code></pre>
<p>and keeping other parameters unchanged.</p>
<p>Use <code>sbtach</code> command to launch a non-interactive job which may take about 5 hours to execute.</p>
<p>Produce a figure or animation showing the final stage of temperature distribution in 3D and add it to a new section titled <code>## Porous convection 3D MPI</code> in the <code>PorousConvection</code> project subfolder&#39;s <code>README.md</code>. You can use the Makie visualisation helper script from Lecture 7 for this purpose.</p>

<p><a href="#content">⤴ <em><strong>back to Content</strong></em></a></p>
<hr />
<h2 id=exercise_3_-_automatic_documentation_in_julia ><a href="#exercise_3_-_automatic_documentation_in_julia" class=header-anchor >Exercise 3 - <strong>Automatic documentation in Julia</strong></a></h2>
<p>👉 See <a href="/logistics/#submission">Logistics</a> for submission details.</p>
<p>The goal of this exercise is to:</p>
<ul>
<li><p>write some documentation</p>
<ul>
<li><p>using <a href="https://docs.julialang.org/en/v1/manual/documentation/">doc-strings</a></p>

<li><p>using <a href="https://github.com/fredrikekre/Literate.jl"><code>Literate.jl</code></a></p>

</ul>

</ul>
<p>One task you&#39;ve already done, namely to update the <code>README.md</code> of this set of exercises&#33;</p>
<p>Tasks:</p>
<ul>
<li><p>add doc-string to the functions you created for this exercise</p>

<li><p>turn the script of Exercise 3 into a <code>Literate.jl</code> script. Compile it to markdown, git-add the output &#40;both <code>.md</code>-file and any figures&#41; and push it.</p>

</ul>

<p><a href="#content">⤴ <em><strong>back to Content</strong></em></a></p>
<div class=page-foot >
  <div class=copyright >
    <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a><br>
    Last modified: November 07, 2022. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div>