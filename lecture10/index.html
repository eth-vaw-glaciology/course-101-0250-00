<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <script src="/libs/lunr/lunr.min.js"></script> <script src="/libs/lunr/lunr_index.js"></script> <script src="/libs/lunr/lunrclient.min.js"></script> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/poole_hyde.css"> <link rel=stylesheet  href="/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=icon  href="/assets/favicon.png"> <title>Lecture 10</title> <style> .content {max-width: 50rem} </style> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/assets/vaw_logo.png" style="width: 180px; height: auto; display: inline"> <div style="margin-bottom: 0.5em"><a href="/"> Fall 2025</a> <span style="opacity: 0.7;">| <a href=https://www.vorlesungen.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2025W&ansicht=KATALOGDATEN&lerneinheitId=193496&lang=en> ETHZ 101-0250-00</a></span></div> <br> <h1><a href="/">Solving partial differential equations in parallel on GPUs I</a></h1> <div style="line-height:18px; font-size: 15px; opacity: 0.85">by &nbsp; <a href="https://vaw.ethz.ch/en/people/person-detail.MjcwOTYw.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ludovic R√§ss</a>, &nbsp; <a href="https://vaw.ethz.ch/en/personen/person-detail.html?persid=124402">Mauro Werder</a>, &nbsp; <a href="https://www.cscs.ch/about/staff/">Samuel Omlin</a> & <br> <a href="https://vaw.ethz.ch/en/people/person-detail.MzAwMjIy.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ivan Utkin</a> </div> </div> <br> <style> </style> <nav class=sidebar-nav  style="opacity: 0.9; margin-bottom: 1cm;"> <a class="sidebar-nav-item " href="/"><b>Welcome</b></a> <a class="sidebar-nav-item " href="/logistics/">Logistics</a> <a class="sidebar-nav-item " href="/homework/">Homeworks</a> <a class="sidebar-nav-item " href="/software_install/">Software install</a> <a class="sidebar-nav-item " href="/extras/">Extras</a> <br> <div class=course-section >Part 1 ‚Äì Introduction</div> <a class="sidebar-nav-item active" href="/lecture1/">Lecture 1 ‚Äì Introduction to Julia</a> <a class="sidebar-nav-item " href="/lecture2/">Lecture 2 ‚Äì PDEs & physical processes</a> <a class="sidebar-nav-item " href="/lecture3/"> Lecture 3 ‚Äì Solving elliptic PDEs</a> <a class="sidebar-nav-item " href="/lecture4/">Lecture 4 ‚Äì Coupled multi-physics</a> <div class=course-section >Part 2 ‚Äì Solving PDEs on GPUs</div> <a class="sidebar-nav-item " href="/lecture5/">Lecture 5 ‚Äì Porous convection</a> <a class="sidebar-nav-item " href="/lecture6/">Lecture 6 ‚Äì Parallel computing</a> <a class="sidebar-nav-item " href="/lecture7/">Lecture 7 ‚Äì GPU computing</a> <a class="sidebar-nav-item " href="/lecture8/">Lecture 8 ‚Äì xPU computing</a> <div class=course-section >Part 3 ‚Äì Multi-GPU computing (projects)</div> <a class="sidebar-nav-item " href="/lecture9/">Lecture 9 ‚Äì Julia MPI & multi-xPU</a> <a class="sidebar-nav-item active" href="/lecture10/">Lecture 10 ‚Äì ImplicitGlobalGrid.jl</a> <a class="sidebar-nav-item " href="/lecture11/">Lecture 11 ‚Äì Multi-xPU & final project</a> <a class="sidebar-nav-item " href="/lecture12/">Lecture 12 ‚Äì Advanced optimisations</a> </nav> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/search/index.html"> </form> <br> <br> </div> </div> <div class="content container"> <div class=franklin-content > <h1 id=lecture_10 ><a href="#lecture_10" class=header-anchor >Lecture 10</a></h1> <blockquote> <p><strong>Agenda</strong><br />üìö Distributed multi-xPU computing with ImplicitGlobalGrid.jl<br />üíª Documenting your code<br />üöß Exercises:</p> <ul> <li><p>2D diffusion with multi-xPU</p> </ul> </blockquote> <hr /> <p><a id=content  class=anchor ></a> <strong>Content</strong></p> <div class=franklin-toc ><ol><li><a href="#lecture_10">Lecture 10</a><li><a href="#using_implicitglobalgridjl">Using ImplicitGlobalGrid.jl</a><li><a href="#documenting_your_code">Documenting your code</a><li><a href="#exercises_-_lecture_10">Exercises - lecture 10</a><ol><li><a href="#exercise_1_multi-xpu_computing">Exercise 1 ‚Äî <strong>Multi-xPU computing</strong></a></ol></ol></div> <p><a href="#exercises_-_lecture_10"><em>üëâ get started with exercises</em></a></p> <hr /> <h1 id=using_implicitglobalgridjl ><a href="#using_implicitglobalgridjl" class=header-anchor >Using ImplicitGlobalGrid.jl</a></h1> <h3 id=the_goal_of_this_lecture_10 ><a href="#the_goal_of_this_lecture_10" class=header-anchor >The goal of this lecture 10:</a></h3> <p>Distributed computing</p> <ul> <li><p>learn about hiding MPI communication behind computations using asynchronous MPI calls</p> <li><p>combine ImplicitGlobalGrid.jl and ParallelStencil.jl together</p> </ul> <p>Let&#39;s have look at <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl">ImplicitGlobalGrid.jl</a>&#39;s repository.</p> <p>ImplicitGlobalGrid.jl can render distributed parallelisation with GPU and CPU for HPC a very simple task. Moreover, ImplicitGlobalGrid.jl elegantly combines with <a href="https://github.com/omlins/ParallelStencil.jl">ParallelStencil.jl</a>.</p> <p>Finally, the cool part: using both packages together enables to <a href="https://github.com/omlins/ParallelStencil.jl#seamless-interoperability-with-communication-packages-and-hiding-communication">hide communication behind computation</a>. This feature enables a parallel efficiency close to 1.</p> <h3 id=getting_started_with_implicitglobalgrid ><a href="#getting_started_with_implicitglobalgrid" class=header-anchor >Getting started with ImplicitGlobalGrid</a></h3> <p>For this development, we&#39;ll start from the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l9_scripts/"><code>l9_diffusion_2D_perf_xpu.jl</code></a> code.</p> <p>Only a few changes are required to enable multi-xPU execution, namely:</p> <ol> <li><p>Initialise the implicit global grid</p> <li><p>Use global coordinates to compute the initial condition</p> <li><p>Update halo &#40;and overlap communication with computation&#41;</p> <li><p>Finalise the global grid</p> <li><p>Tune visualisation</p> </ol> <p>But before we start programming the multi-xPU implementation, let&#39;s get setup with GPU MPI on daint.alps. Follow steps are needed:</p> <ul> <li><p>Launch a <code>salloc</code> on 2 nodes</p> <li><p>Install the required MPI-related packages</p> <li><p>Test your setup running <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l9_scripts/"><code>l9_hello_mpi.jl</code></a> and <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l9_scripts/"><code>l9_hello_mpi_gpu.jl</code></a> scripts on 1-2 nodes</p> </ul> <div class=note ><div class=title >üí° Note</div> <div class=messg >See <a href="/software_install/#gpu_computing_on_alps">GPU computing on Alps</a> for detailed information on how to run MPI GPU &#40;multi-GPU&#41; applications on daint.alps.</div></div> <p>To &#40;<strong>1.</strong>&#41; initialise the global grid, one first needs to use the package</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> ImplicitGlobalGrid</code></pre>
<p>Then, one can add the global grid initialisation in the <code># Derived numerics</code> section</p>
<pre><code class="julia hljs">me, dims = init_global_grid(nx, ny, <span class=hljs-number >1</span>; select_device = <span class=hljs-literal >false</span>)  <span class=hljs-comment ># Initialization of MPI and more...</span>
dx, dy  = Lx/nx_g(), Ly/ny_g()</code></pre>
<div class=note ><div class=title >üí° Note</div>
<div class=messg >On Alps, SLURM makes device with ID&#61;0 visible to each MPI rank, which requires to disable device selection in the call to <code>init_global_grid&#40;...; select_device &#61; false&#41;</code>.</div></div>
<p>Then, for &#40;<strong>2.</strong>&#41;, one can use <code>x_g&#40;&#41;</code> and <code>y_g&#40;&#41;</code> to compute the global coordinates in the initialisation &#40;to correctly spread the Gaussian distribution over all local processes&#41;</p>
<pre><code class="julia hljs">C       = <span class=hljs-meta >@zeros</span>(nx,ny)
C      .= Data.<span class=hljs-built_in >Array</span>([exp(-(x_g(ix,dx,C)+dx/<span class=hljs-number >2</span> -Lx/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span> -(y_g(iy,dy,C)+dy/<span class=hljs-number >2</span> -Ly/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span>) <span class=hljs-keyword >for</span> ix=<span class=hljs-number >1</span>:size(C,<span class=hljs-number >1</span>), iy=<span class=hljs-number >1</span>:size(C,<span class=hljs-number >2</span>)])</code></pre>
<p>The halo update &#40;<strong>3.</strong>&#41; can be simply performed adding following line after the <code>compute&#33;</code> kernel</p>
<pre><code class="julia hljs">update_halo!(C)</code></pre>
<p>Now, when running on GPUs, it is possible to hide MPI communication behind computations&#33;</p>
<p>This option implements as:</p>
<pre><code class="julia hljs"><span class=hljs-meta >@hide_communication</span> (<span class=hljs-number >8</span>, <span class=hljs-number >2</span>) <span class=hljs-keyword >begin</span>
    <span class=hljs-meta >@parallel</span> compute!(C2, C, D_dx, D_dy, dt, _dx, _dy, size_C1_2, size_C2_2)
    C, C2 = C2, C <span class=hljs-comment ># pointer swap</span>
    update_halo!(C)
<span class=hljs-keyword >end</span></code></pre>
<p>The <code>@hide_communication &#40;8, 2&#41;</code> will first compute the first and last 8 and 2 grid points in x and y dimension, respectively. Then, while exchanging boundaries the rest of the local domains computations will be perform &#40;overlapping the MPI communication&#41;.</p>
<p>To &#40;<strong>4.</strong>&#41; finalise the global grid,</p>
<pre><code class="julia hljs">finalize_global_grid()</code></pre>
<p>needs to be added before the <code>return</code> of the &quot;main&quot;.</p>
<p>The last changes to take care of is to &#40;<strong>5.</strong>&#41; handle visualisation in an appropriate fashion. Here, several options exists.</p>
<ul>
<li><p>One approach would for each local process to dump the local domain results to a file &#40;with process ID <code>me</code> in the filename&#41; in order to reconstruct to global grid with a post-processing visualisation script &#40;as done in the previous examples&#41;. Libraries like, e.g., <a href="https://adios2.readthedocs.io/en/latest">ADIOS2</a> may help out there.</p>

<li><p>Another approach would be to gather the global grid results on a master process before doing further steps as disk saving or plotting.</p>

</ul>
<p>To implement the latter and generate a <code>gif</code>, one needs to define a global array for visualisation:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >if</span> do_visu
    <span class=hljs-keyword >if</span> (me==<span class=hljs-number >0</span>) <span class=hljs-literal >ENV</span>[<span class=hljs-string >&quot;GKSwstype&quot;</span>]=<span class=hljs-string >&quot;nul&quot;</span>; <span class=hljs-keyword >if</span> isdir(<span class=hljs-string >&quot;viz2D_mxpu_out&quot;</span>)==<span class=hljs-literal >false</span> mkdir(<span class=hljs-string >&quot;viz2D_mxpu_out&quot;</span>) <span class=hljs-keyword >end</span>; loadpath = <span class=hljs-string >&quot;./viz2D_mxpu_out/&quot;</span>; anim = Animation(loadpath,<span class=hljs-built_in >String</span>[]); println(<span class=hljs-string >&quot;Animation directory: <span class=hljs-subst >$(anim.dir)</span>&quot;</span>) <span class=hljs-keyword >end</span>
    nx_v, ny_v = (nx-<span class=hljs-number >2</span>)*dims[<span class=hljs-number >1</span>], (ny-<span class=hljs-number >2</span>)*dims[<span class=hljs-number >2</span>]
    <span class=hljs-keyword >if</span> (nx_v*ny_v*sizeof(Data.<span class=hljs-built_in >Number</span>) &gt; <span class=hljs-number >0.8</span>*Sys.free_memory()) error(<span class=hljs-string >&quot;Not enough memory for visualization.&quot;</span>) <span class=hljs-keyword >end</span>
    C_v   = zeros(nx_v, ny_v) <span class=hljs-comment ># global array for visu</span>
    C_inn = zeros(nx-<span class=hljs-number >2</span>, ny-<span class=hljs-number >2</span>) <span class=hljs-comment ># no halo local array for visu</span>
    xi_g, yi_g = <span class=hljs-built_in >LinRange</span>(dx+dx/<span class=hljs-number >2</span>, Lx-dx-dx/<span class=hljs-number >2</span>, nx_v), <span class=hljs-built_in >LinRange</span>(dy+dy/<span class=hljs-number >2</span>, Ly-dy-dy/<span class=hljs-number >2</span>, ny_v) <span class=hljs-comment ># inner points only</span>
<span class=hljs-keyword >end</span></code></pre>
<p>Then, the plotting routine can be adapted to first gather the inner points of the local domains into the global array &#40;using <code>gather&#33;</code> function&#41; and then plot and/or save the global array &#40;here <code>C_v</code>&#41; from the master process <code>me&#61;&#61;0</code>:</p>
<pre><code class="julia hljs"><span class=hljs-comment ># Visualize</span>
<span class=hljs-keyword >if</span> do_visu &amp;&amp; (it % nout == <span class=hljs-number >0</span>)
    C_inn .= <span class=hljs-built_in >Array</span>(C)[<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>,<span class=hljs-number >2</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>]; gather!(C_inn, C_v)
    <span class=hljs-keyword >if</span> (me==<span class=hljs-number >0</span>)
        opts = (aspect_ratio=<span class=hljs-number >1</span>, xlims=(xi_g[<span class=hljs-number >1</span>], xi_g[<span class=hljs-keyword >end</span>]), ylims=(yi_g[<span class=hljs-number >1</span>], yi_g[<span class=hljs-keyword >end</span>]), clims=(<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), c=:turbo, xlabel=<span class=hljs-string >&quot;Lx&quot;</span>, ylabel=<span class=hljs-string >&quot;Ly&quot;</span>, title=<span class=hljs-string >&quot;time = <span class=hljs-subst >$(round(it*dt, sigdigits=<span class=hljs-number >3</span>)</span>)&quot;</span>)
        heatmap(xi_g, yi_g, <span class=hljs-built_in >Array</span>(C_v)&#x27;; opts...); frame(anim)
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre>
<p>To finally generate the <code>gif</code>, one needs to place the following after the time loop:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >if</span> (do_visu &amp;&amp; me==<span class=hljs-number >0</span>) gif(anim, <span class=hljs-string >&quot;diffusion_2D_mxpu.gif&quot;</span>, fps = <span class=hljs-number >5</span>)  <span class=hljs-keyword >end</span></code></pre>
<div class=note ><div class=title >üí° Note</div>
<div class=messg >We here did rely on CUDA-aware MPI. To use this feature set &#40;and export&#41; <code>IGG_CUDAAWARE_MPI&#61;1</code>. Note that the examples using ImplicitGlobalGrid.jl would also work if <code>USE_GPU &#61; false</code>; however, the communication and computation overlap feature is then currently not yet available as its implementation relies at present on leveraging GPU streams.</div></div>
<h3 id=hiding_communication ><a href="#hiding_communication" class=header-anchor >Hiding communication</a></h3>
<p>Hiding communication behind computation is a common optimisation technique in distributed stencil computing.</p>
<p>You can think about it as each MPI rank being a watermelon.</p>
<ol>
<li><p>We first want to compute the updates for the crust region &#40;green&#41;, and then directly start the MPI non-blocking communication &#40;Isend/Irecv&#41;.</p>

<li><p>In the meantime, we asychronously compute the updates of the inner region &#40;red&#41; of the watermelon.</p>

</ol>
<p>The aim is to hide step &#40;1.&#41; while computing step &#40;2.&#41;.</p>
<p>We can examine the effect of hiding communication looking at the profiler trace produced running a 3D diffusion code under NVIDIA Nsight System profiler &#40;see the <a href="/software_install/#profiling_on_alps">Profiling on Alps</a> section about how to launch the profiler&#41;.</p>
<ul>
<li><p>Profiling trace with hide communication enabled:</p>

</ul>
<p><img src="../assets/literate_figures/l10_alps_hidecomm.png" alt=hidecomm  /></p>
<ul>
<li><p>Profiling trace with naive implementation and hide communication disabled:</p>

</ul>
<p><img src="../assets/literate_figures/l10_alps_nohidecomm.png" alt="no hidecomm" /></p>

<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<h1 id=documenting_your_code ><a href="#documenting_your_code" class=header-anchor >Documenting your code</a></h1>
<p>This lecture we will learn:</p>
<ul>
<li><p>documentation vs code-comments</p>

<li><p>why to write documentation</p>

<li><p>GitHub tools:</p>
<ul>
<li><p>rendering of markdown files</p>

<li><p>gh-pages</p>

</ul>

<li><p>some Julia tools:</p>
<ul>
<li><p>docstrings</p>

<li><p><a href="https://github.com/fredrikekre/Literate.jl">https://github.com/fredrikekre/Literate.jl</a></p>

<li><p><a href="https://github.com/JuliaDocs/Documenter.jl">https://github.com/JuliaDocs/Documenter.jl</a></p>

</ul>

</ul>
<p><img src="https://pcweenies.com/wp-content/uploads/2012/01/2012-01-12_pcw.jpg" alt=comic  /></p>
<h3 id=why_should_i_document_my_code ><a href="#why_should_i_document_my_code" class=header-anchor >Why should I document my code?</a></h3>
<p>Why should I write code comments?</p>
<ul>
<li><p><a href="https://blog.codinghorror.com/code-tells-you-how-comments-tell-you-why/">&quot;Code Tells You How, Comments Tell You Why&quot;</a></p>
<ul>
<li><p>code should be made understandable by itself, as much as possible</p>

<li><p>comments then should be to tell the &quot;why&quot; you&#39;re doing something</p>

</ul>

<li><p><em>but</em> I do a lot of structuring comments as well</p>

<li><p>math-y variables tend to be short and need a comment as well</p>

</ul>
<p>Why should I write documentation?</p>
<ul>
<li><p>documentation should give a bigger overview of what your code does</p>
<ul>
<li><p>at the function-level &#40;doc-strings&#41;</p>

<li><p>at the package-level &#40;README, full-fledged documentation&#41;</p>

</ul>

<li><p>to let other people and your future self &#40;probably most importantly&#41; understand what your code is about</p>

</ul>
<h3 id=documentation_easily_rots ><a href="#documentation_easily_rots" class=header-anchor >Documentation easily rots...</a></h3>
<p>Worse than no documentation/code comments is documentation which is outdated.</p>
<p>I find the best way to keep documentation up to date is:</p>
<ul>
<li><p>have documentation visible to you, e.g. GitHub README</p>

<li><p>document what you need yourself</p>

<li><p>use examples and run them as part of CI &#40;doc-tests, example-scripts&#41;</p>

</ul>
<h3 id=documentation_tools_doc-strings ><a href="#documentation_tools_doc-strings" class=header-anchor >Documentation tools: doc-strings</a></h3>
<p>A Julia doc-string &#40;<a href="https://docs.julialang.org/en/v1/manual/documentation/">Julia manual</a>&#41;:</p>
<ul>
<li><p>is just a string before the object &#40;no blank-line inbetween&#41;; interpreted as markdown-string</p>

<li><p>can be attached to most things &#40;functions, variables, modules, macros, types&#41;</p>

<li><p>can be queried with <code>?</code></p>

</ul>
<pre><code class="julia hljs"><span class=hljs-string >&quot;Typical size of a beer crate&quot;</span>
<span class=hljs-keyword >const</span> BEERBOX = <span class=hljs-number >12</span></code></pre>
<pre><code class="julia hljs">?BEERBOX</code></pre>
<h3 id=documentation_tools_doc-strings_with_examples ><a href="#documentation_tools_doc-strings_with_examples" class=header-anchor >Documentation tools: doc-strings with examples</a></h3>
<p>One can add examples to doc-strings &#40;they can even be part of testing: <a href="https://juliadocs.github.io/Documenter.jl/stable/man/doctests/">doc-tests</a>&#41;.</p>
<p>&#40;Run it in the REPL and copy paste to the docstring.&#41;</p>
<pre><code class="julia hljs"><span class=hljs-string >&quot;&quot;&quot;
    transform(r, Œ∏)

Transform polar `(r,Œ∏)` to cartesian coordinates `(x,y)`.

# Example
```jldoctest
julia&gt; transform(4.5, pi/5)
(3.6405764746872635, 2.6450336353161292)
```
&quot;&quot;&quot;</span>
transform(r, Œ∏) = (r*cos(Œ∏), r*sin(Œ∏))</code></pre>
<pre><code class="julia hljs">?transform</code></pre>
<h3 id=documentation_tools_github_markdown_rendering ><a href="#documentation_tools_github_markdown_rendering" class=header-anchor >Documentation tools: GitHub markdown rendering</a></h3>
<p>The easiest way to write long-form documentation is to just use GitHub&#39;s markdown rendering.</p>
<p>A nice example is <a href="https://github.com/luraess/parallel-gpu-workshop-JuliaCon21">this short course</a> by Ludovic &#40;incidentally about solving PDEs on GPUs üôÇ&#41;.</p>
<ul>
<li><p>images are rendered</p>

<li><p>in-page links are easy, e.g. <code>&#91;_back to workshop material_&#93;&#40;#workshop-material&#41;</code></p>

<li><p>top-left has a burger-menu for page navigation</p>

<li><p>can be edited within the web-page &#40;pencil-icon&#41;</p>

</ul>
<p>üëâ this is a good and low-overhead way to produce pretty nice documentation</p>
<h3 id=documentation_tools_literatejl ><a href="#documentation_tools_literatejl" class=header-anchor >Documentation tools: Literate.jl</a></h3>
<p>There are several tools which render .jl files &#40;with special formatting&#41; into markdown files.  These files can then be added to GitHub and will be rendered there.</p>
<ul>
<li><p>we&#39;re using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a></p>

<li><p>format is described <a href="https://fredrikekre.github.io/Literate.jl/v2/fileformat/">here</a></p>

<li><p>files stay valid Julia scripts, i.e. they can be executed without Literate.jl</p>

</ul>
<p>Example</p>
<ul>
<li><p>input julia-code in: <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00-L8Documentation.jl/blob/4bbeb3ddda046490847f050b02d3fc5d9308695b/scripts/car_travels.jl">course-101-0250-00-L8Documentation.jl: scripts/car_travels.jl</a></p>

<li><p>output markdown in: <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00-L8Documentation.jl/blob/4bbeb3ddda046490847f050b02d3fc5d9308695b/scripts/car_travels.md">course-101-0250-00-L8Documentation.jl: scripts/car_travels.md</a> created with:</p>

</ul>
<pre><code class="julia hljs">Literate.markdown(<span class=hljs-string >&quot;car_travels.jl&quot;</span>, directory_of_this_file, execute=<span class=hljs-literal >true</span>, documenter=<span class=hljs-literal >false</span>, credit=<span class=hljs-literal >false</span>)</code></pre>
<p>But this is not automatic&#33;  Manual steps: run Literate, add files, commit and push...</p>
<p>or use GitHub Actions...</p>
<h3 id=documentation_tools_automating_literatejl ><a href="#documentation_tools_automating_literatejl" class=header-anchor >Documentation tools: Automating Literate.jl</a></h3>
<p>Demonstrated in the repo <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00-L8Documentation.jl">course-101-0250-00-L8Documentation.jl</a></p>
<pre><code class="yml hljs"><span class=hljs-attr >name:</span> <span class=hljs-string >Run</span> <span class=hljs-string >Literate.jl</span>
<span class=hljs-comment ># adapted from https://lannonbr.com/blog/2019-12-09-git-commit-in-actions</span>

<span class=hljs-attr >on:</span> <span class=hljs-string >push</span>

<span class=hljs-attr >jobs:</span>
  <span class=hljs-attr >lit:</span>
    <span class=hljs-attr >runs-on:</span> <span class=hljs-string >ubuntu-latest</span>
    <span class=hljs-attr >steps:</span>
      <span class=hljs-comment ># Checkout the branch</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >actions/checkout@v4</span>

      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >julia-actions/setup-julia@latest</span>
        <span class=hljs-attr >with:</span>
          <span class=hljs-attr >version:</span> <span class=hljs-string >&#x27;1.12&#x27;</span>
          <span class=hljs-attr >arch:</span> <span class=hljs-string >x64</span>

      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >julia-actions/cache@v1</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >uses:</span> <span class=hljs-string >julia-actions/julia-buildpkg@latest</span>

      <span class=hljs-bullet >-</span> <span class=hljs-attr >name:</span> <span class=hljs-string >run</span> <span class=hljs-string >Literate</span>
        <span class=hljs-attr >run:</span> <span class=hljs-string >QT_QPA_PLATFORM=offscreen</span> <span class=hljs-string >julia</span> <span class=hljs-string >--color=yes</span> <span class=hljs-string >--project</span> <span class=hljs-string >-e</span> <span class=hljs-string >&#x27;cd(&quot;scripts&quot;); include(&quot;literate-script.jl&quot;)&#x27;</span>

      <span class=hljs-bullet >-</span> <span class=hljs-attr >name:</span> <span class=hljs-string >setup</span> <span class=hljs-string >git</span> <span class=hljs-string >config</span>
        <span class=hljs-attr >run:</span> <span class=hljs-string >|
          # setup the username and email. I tend to use &#x27;GitHub Actions Bot&#x27; with no email by default
          git config user.name &quot;GitHub Actions Bot&quot;
          git config user.email &quot;&lt;&gt;&quot;
</span>
      <span class=hljs-bullet >-</span> <span class=hljs-attr >name:</span> <span class=hljs-string >commit</span>
        <span class=hljs-attr >run:</span> <span class=hljs-string >|
          # Stage the file, commit and push
          git add scripts/md/*
          git commit -m &quot;Commit markdown files fom Literate&quot;
          git push origin master</span></code></pre>
<h3 id=documentation_tools_documenterjl ><a href="#documentation_tools_documenterjl" class=header-anchor >Documentation tools: Documenter.jl</a></h3>
<p>If you want to have full-blown documentation, including, e.g., automatic API documentation generation, versioning, then use <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a>.</p>
<p>Examples:</p>
<ul>
<li><p><a href="https://docs.julialang.org/en/v1/">https://docs.julialang.org/en/v1/</a></p>

<li><p><a href="https://mauro3.github.io/Parameters.jl/stable/">https://mauro3.github.io/Parameters.jl/stable/</a></p>

</ul>
<p><em><strong>Notes:</strong></em></p>
<ul>
<li><p>it&#39;s geared towards Julia-packages, less for a bunch-of-scripts as in our lecture</p>

<li><p>Documenter.jl also integrates with Literate.jl.</p>

<li><p>for more free-form websites, use <a href="https://github.com/tlienart/Franklin.jl">https://github.com/tlienart/Franklin.jl</a> &#40;as the course website does&#41;</p>

<li><p>if you want to use it, it&#39;s easiest to generate your package with <a href="https://github.com/invenia/PkgTemplates.jl">PkgTemplates.jl</a> which will generate the Documenter-setup for you.</p>

<li><p><strong>we don&#39;t use it in this course</strong></p>

</ul>

<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<h1 id=exercises_-_lecture_10 ><a href="#exercises_-_lecture_10" class=header-anchor >Exercises - lecture 10</a></h1>
<h2 id=exercise_1_multi-xpu_computing ><a href="#exercise_1_multi-xpu_computing" class=header-anchor >Exercise 1 ‚Äî <strong>Multi-xPU computing</strong></a></h2>
<p>üëâ See <a href="/logistics/#submission">Logistics</a> for submission details.</p>
<p>The goal of this exercise is to:</p>
<ul>
<li><p>Familiarise with distributed computing</p>

<li><p>Combine <a href="https://github.com/eth-cscs/ImplicitGlobalGrid.jl">ImplicitGlobalGrid.jl</a> and <a href="https://github.com/omlins/ParallelStencil.jl">ParallelStencil.jl</a></p>

<li><p>Learn about GPU MPI on the way</p>

</ul>
<p>In this exercise, you will:</p>
<ul>
<li><p>Create a multi-xPU version of your the 2D xPU diffusion solver</p>

<li><p>Keep it xPU compatible using <code>ParallelStencil.jl</code></p>

<li><p>Deploy it on multiple xPUs using <code>ImplicitGlobalGrid.jl</code></p>

</ul>
<p>Start by fetching the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/l9_scripts/"><code>l9_diffusion_2D_perf_xpu.jl</code></a> code from the <code>scripts/l9_scripts</code> folder and copy it to your <code>lecture_10</code> folder.</p>
<p>Make a copy and rename it <code>diffusion_2D_perf_multixpu.jl</code>.</p>
<h3 id=task_1 ><a href="#task_1" class=header-anchor >Task 1</a></h3>
<p>Follow the steps listed in the section from lecture 10 about <a href="#using_implicitglobalgridjl">using <code>ImplicitGlobalGrid.jl</code></a> to add multi-xPU support to the 2D diffusion code.</p>
<p>The 5 steps you&#39;ll need to implement are summarised hereafter:</p>
<ol>
<li><p>Initialise the implicit global grid</p>

<li><p>Use global coordinates to compute the initial condition</p>

<li><p>Update halo &#40;and overlap communication with computation&#41;</p>

<li><p>Finalise the global grid</p>

<li><p>Tune visualisation</p>

</ol>
<p>Once the above steps are implemented, head to daint.alps and configure either an <code>salloc</code> or prepare a <code>sbatch</code> script to access 1 node.</p>
<h3 id=task_2 ><a href="#task_2" class=header-anchor >Task 2</a></h3>
<p>Run the single xPU <code>l9_diffusion_2D_perf_xpu.jl</code> code on a single CPU and single GPU &#40;changing the <code>USE_GPU</code> flag accordingly&#41; for following parameters</p>
<pre><code class="julia hljs"><span class=hljs-comment ># Physics</span>
Lx, Ly  = <span class=hljs-number >10.0</span>, <span class=hljs-number >10.0</span>
D       = <span class=hljs-number >1.0</span>
ttot    = <span class=hljs-number >1.0</span>
<span class=hljs-comment ># Numerics</span>
nx, ny  = <span class=hljs-number >126</span>, <span class=hljs-number >126</span>
nout    = <span class=hljs-number >20</span></code></pre>
<p>and save output <code>C</code> data. Confirm that the difference between CPU and GPU implementation is negligible, reporting it in a new section of the <code>README.md</code> for this exercise 2 within the <code>lecture_10</code> folder in your shared private GitHub repo.</p>
<h3 id=task_3 ><a href="#task_3" class=header-anchor >Task 3</a></h3>
<p>Then run the newly created <code>diffusion_2D_perf_multixpu.jl</code> script with following parameters on <strong>4 MPI processes</strong> having set <code>USE_GPU &#61; true</code>:</p>
<pre><code class="julia hljs"><span class=hljs-comment ># Physics</span>
Lx, Ly  = <span class=hljs-number >10.0</span>, <span class=hljs-number >10.0</span>
D       = <span class=hljs-number >1.0</span>
ttot    = <span class=hljs-number >1e0</span>
<span class=hljs-comment ># Numerics</span>
nx, ny  = <span class=hljs-number >64</span>, <span class=hljs-number >64</span> <span class=hljs-comment ># number of grid points</span>
nout    = <span class=hljs-number >20</span>
<span class=hljs-comment ># Derived numerics</span>
me, dims = init_global_grid(nx, ny, <span class=hljs-number >1</span>; select_device = <span class=hljs-literal >false</span>)  <span class=hljs-comment ># Initialization of MPI and more...</span></code></pre>
<p>Save the global <code>C_v</code> output array. Ensure its size matches the inner points of the single xPU produced output &#40;<code>C&#91;2:end-1,2:end-1&#93;</code>&#41; and then compare the results to the existing 2 outputs produced in Task 2</p>
<h3 id=task_4 ><a href="#task_4" class=header-anchor >Task 4</a></h3>
<p>Now that we are confident the xPU and multi-xPU codes produce correct physical output, we will asses performance.</p>
<p>Use the code <code>diffusion_2D_perf_multixpu.jl</code> and make sure to deactivate visualisation, saving or any other operation that would save to disk or slow the code down.</p>
<p><strong>Strong scaling:</strong> Using a single GPU, gather the effective memory throughput <code>T_eff</code> varying <code>nx, ny</code> as following</p>
<pre><code class="julia hljs">nx = ny = <span class=hljs-number >16</span> * <span class=hljs-number >2</span> .^ (<span class=hljs-number >1</span>:<span class=hljs-number >10</span>)</code></pre>
<div class=warning ><div class=title >‚ö†Ô∏è Warning&#33;</div>
<div class=messg >Make sur the code only spends about 1-2 seconds in the time loop, adapting <code>ttot</code> or <code>nt</code> accordingly.</div></div>
<p>In a new figure you&#39;ll add to the <code>README.md</code>, report <code>T_eff</code> as function of <code>nx</code>, and include a short comment on what you see.</p>
<h3 id=task_5 ><a href="#task_5" class=header-anchor >Task 5</a></h3>
<p><strong>Weak scaling:</strong> Select the smallest <code>nx,ny</code> values from previous step &#40;2.&#41; for which you&#39;ve gotten the best <code>T_eff</code>. Run now the same code using this optimal local resolution varying the number of MPI process as following <code>np &#61; 1,4,16,25,64</code>.</p>
<div class=warning ><div class=title >‚ö†Ô∏è Warning&#33;</div>
<div class=messg >Make sure the code only executes a couple of seconds each time otherwise we will run out of node hours for the rest of the course.</div></div>
<p>In a new figure, report the execution time for the various runs <strong>normalising them with the execution time of the single process run</strong>. Comment in one sentence on what you see.</p>
<h3 id=task_6 ><a href="#task_6" class=header-anchor >Task 6</a></h3>
<p>Finally, let&#39;s assess the impact of hiding communication behind computation achieved using the <code>@hide_communication</code> macro in the multi-xPU code.</p>
<p>Using the 64 MPI processes configuration, run the multi-xPU code changing the values of the tuple after <code>@hide_communication</code> such that</p>
<pre><code class="julia hljs"><span class=hljs-meta >@hide_communication</span> (<span class=hljs-number >2</span>,<span class=hljs-number >2</span>)
<span class=hljs-meta >@hide_communication</span> (<span class=hljs-number >16</span>,<span class=hljs-number >4</span>)
<span class=hljs-meta >@hide_communication</span> (<span class=hljs-number >16</span>,<span class=hljs-number >16</span>)</code></pre>
<p>Then, you should also run once the code commenting both <code>@hide_communication</code> and corresponding <code>end</code> statements. On a figure report the execution time as function of <code>&#91;no-hidecomm, &#40;2,2&#41;, &#40;8,2&#41;, &#40;16,4&#41;, &#40;16,16&#41;&#93;</code> &#40;note that the <code>&#40;8,2&#41;</code> case you should have from Task 4 and/or 5&#41; making sure to <strong>normalise it by the single process execution time</strong> &#40;from Task 5&#41;. Add a short comment related to your results.</p>

<p><a href="#content">‚§¥ <em><strong>back to Content</strong></em></a></p>
<div class=page-foot >
  <div class=copyright >
    <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a><br>
    Last modified: December 08, 2025. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div>