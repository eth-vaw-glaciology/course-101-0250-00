<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <script src="/libs/lunr/lunr.min.js"></script> <script src="/libs/lunr/lunr_index.js"></script> <script src="/libs/lunr/lunrclient.min.js"></script> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/poole_hyde.css"> <link rel=stylesheet  href="/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=icon  href="/assets/favicon.png"> <title>Lecture 10</title> <style> .content {max-width: 50rem} </style> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/assets/vaw_logo.png" style="width: 180px; height: auto; display: inline"> <div style="font-weight: margin-bottom: 0.5em"><a href="/"> Fall 2022</a> <span style="opacity: 0.7;">| <a href="http://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2022W&ansicht=KATALOGDATEN&lerneinheitId=162403&lang=en"> ETHZ 101-0250-00</a></span></div> <br> <h1><a href="/">Solving partial differential equations in parallel on GPUs</a></h1> <div style="line-height:18px; font-size: 15px; opacity: 0.85">by &nbsp; <a href="https://vaw.ethz.ch/en/people/person-detail.MjcwOTYw.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ludovic Räss</a>, &nbsp; <a href="https://vaw.ethz.ch/en/personen/person-detail.html?persid=124402">Mauro Werder</a>, &nbsp; <a href="https://www.cscs.ch/about/staff/">Samuel Omlin</a> & <br> <a href="https://vaw.ethz.ch/en/people/person-detail.MzAwMjIy.TGlzdC8xOTYxLDE1MTczNjI1ODA=.html">Ivan Utkin</a> </div> </div> <br> <style> </style> <nav class=sidebar-nav  style="opacity: 0.9; margin-bottom: 1.2cm;"> <a class="sidebar-nav-item " href="/"><b>Welcome</b></a> <a class="sidebar-nav-item " href="/logistics/">Logistics</a> <a class="sidebar-nav-item " href="/homework/">Homework</a> <a class="sidebar-nav-item " href="/software_install/">Software install</a> <a class="sidebar-nav-item " href="/extras/">Extras</a> <br> <div class=course-section >Part 1 - Introduction</div> <a class="sidebar-nav-item active" href="/lecture1/">Lecture 1 - Why Julia GPU</a> <a class="sidebar-nav-item " href="/lecture2/">Lecture 2 - PDEs & physical processes</a> <a class="sidebar-nav-item " href="/lecture3/">Lecture 3 - Solving elliptic PDEs</a> <div class=course-section >Part 2 - Solving PDEs on GPUs</div> <a class="sidebar-nav-item " href="/lecture4/">Lecture 4 - Porous convection</a> <a class="sidebar-nav-item " href="/lecture5/">Lecture 5 - Parallel computing</a> <a class="sidebar-nav-item " href="/lecture6/">Lecture 6 - GPU computing</a> <div class=course-section >Part 3 - Multi-GPU computing (projects)</div> <a class="sidebar-nav-item " href="/lecture7/">Lecture 7 - xPU computing</a> <a class="sidebar-nav-item " href="/lecture8/">Lecture 8 - Julia MPI & multi-xPU</a> <a class="sidebar-nav-item " href="/lecture9/">Lecture 9 - Multi-xPU & Projects</a> <a class="sidebar-nav-item active" href="/lecture10/">Lecture 10 - Advanced optimisations</a> <div class=course-section >Final Projects</div> <a class="sidebar-nav-item " href="/final_proj/">Infos about final projects</a> </nav> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/search/index.html"> </form> <br> <br> </div> </div> <div class="content container"> <div class=franklin-content > <h1 id=lecture_10 ><a href="#lecture_10" class=header-anchor >Lecture 10</a></h1> <blockquote> <p><strong>Agenda</strong><br />📚 Projects Q&amp;A, Controlling shared memory and registers &#40;on-chip&#41;<br />💻 MPI &amp; Advanced optimisations Q&amp;A<br />🚧 Exercises: Q&amp;A</p> </blockquote> <hr /> <p><a id=content  class=anchor ></a> <strong>Content</strong></p> <div class=franklin-toc ><ol><li><a href="#lecture_10">Lecture 10</a><li><a href="#gpu_computing_and_performance_assessment">GPU computing and performance assessment</a><ol><li><a href="#recap_on_scientific_applications_performance">Recap on scientific applications&#39; performance</a><li><a href="#on-chip_memory_usage">On-chip memory usage</a></ol><li><a href="#exercises_-_lecture_10">Exercises - lecture 10</a><ol><li><a href="#exercise_1_-_push-ups_with_memory_copy">Exercise 1 - <strong>Push-ups with memory copy</strong></a><li><a href="#exercise_2_-_advanced_data_transfer_optimisations_part_1">Exercise 2 - <strong>Advanced data transfer optimisations &#40;part 1&#41;</strong></a><li><a href="#exercise_3_-_advanced_data_transfer_optimisations_part_2">Exercise 3 - <strong>Advanced data transfer optimisations &#40;part 2&#41;</strong></a></ol></ol></div> <p><a href="#exercises_-_lecture_10"><em>👉 get started with exercises</em></a></p> <hr /> <h1 id=gpu_computing_and_performance_assessment ><a href="#gpu_computing_and_performance_assessment" class=header-anchor >GPU computing and performance assessment</a></h1> <h3 id=the_goal_of_this_lecture_10_is_to ><a href="#the_goal_of_this_lecture_10_is_to" class=header-anchor >The goal of this lecture 10 is to:</a></h3> <ul> <li><p>learn how to use shared memory &#40;on-chip&#41; to avoid main memory accesses and communicate between threads; and</p> <li><p>learn how to control registers for storing intermediate results on-chip.</p> </ul> <h2 id=recap_on_scientific_applications_performance ><a href="#recap_on_scientific_applications_performance" class=header-anchor >Recap on scientific applications&#39; performance</a></h2> <p>We will start with a brief recap on the peak performance of current hardware and on performance evaluation of iterative stencil-based PDE solvers.</p> <p>The performance of most scientific applications nowadays is bound by memory access speed &#40;<em>memory-bound</em>&#41; rather than by the speed computations can be done &#40;<em>compute-bound</em>&#41;.</p> <p>The reason is that current GPUs &#40;and CPUs&#41; can do many more computations in a given amount of time than they can access numbers from main memory.</p> <p>This situation is the result of a much faster increase of computation speed with respect to memory access speed over the last decades, until we hit the &quot;memory wall&quot; at the beginning of the century:</p> <p><img src="../assets/literate_figures/l10_flop_to_memaccess_ratio.png" alt=flop_to_memaccess_ratio  /> <em>Source: John McCalpin, Texas Advanced Computing Center &#40;modified&#41;</em> <div class=note ><div class=title >💡 Note</div> <div class=messg >The position of the memory wall is to be considered very approximative.</div></div></p> <p>This imbalance can be quantified by dividing the computation peak performance &#91;GFLOP/s&#93; by the memory access peak performance &#91;GB/s&#93; and multiplied by the size of a number in Bytes &#40;for simplicity, theoretical peak performance values as specified by the vendors can be used&#41;. For example for the Tesla P100 GPU, it is:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mfrac><mrow><mn>5300</mn><mtext> </mtext><mrow><mo stretchy=false >[</mo><mi mathvariant=normal >G</mi><mi mathvariant=normal >F</mi><mi mathvariant=normal >l</mi><mi mathvariant=normal >o</mi><mi mathvariant=normal >p</mi><mi mathvariant=normal >/</mi><mi mathvariant=normal >s</mi><mo stretchy=false >]</mo></mrow></mrow><mrow><mn>732</mn><mtext> </mtext><mrow><mo stretchy=false >[</mo><mi mathvariant=normal >G</mi><mi mathvariant=normal >B</mi><mi mathvariant=normal >/</mi><mi mathvariant=normal >s</mi><mo stretchy=false >]</mo></mrow></mrow></mfrac><mtext> </mtext><mo>×</mo><mtext> </mtext><mn>8</mn><mo>=</mo><mn>58</mn></mrow><annotation encoding="application/x-tex"> \frac{5300 ~\mathrm{[GFlop/s]}}{732 ~\mathrm{[GB/s]}}~×~8 = 58 </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:2.363em;vertical-align:-0.936em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.427em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >7</span><span class=mord >3</span><span class=mord >2</span><span class="mspace nobreak"> </span><span class=mord ><span class=mopen >[</span><span class="mord mathrm">G</span><span class="mord mathrm">B</span><span class="mord mathrm">/</span><span class="mord mathrm">s</span><span class=mclose >]</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >5</span><span class=mord >3</span><span class=mord >0</span><span class=mord >0</span><span class="mspace nobreak"> </span><span class=mord ><span class=mopen >[</span><span class="mord mathrm">G</span><span class="mord mathrm">F</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">p</span><span class="mord mathrm">/</span><span class="mord mathrm">s</span><span class=mclose >]</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace nobreak"> </span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >×</span><span class="mspace nobreak"> </span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mord >8</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.64444em;vertical-align:0em;"></span><span class=mord >5</span><span class=mord >8</span></span></span></span></span> <p>&#40;here computed with double precision values taken from <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-p100/pdf/nvidia-tesla-p100-PCIe-datasheet.pdf">the vendor&#39;s product specification sheet</a>&#41;.</p> <p>So we can do 58 floating point operations per number read from main memory or written to it.</p> <p>As a consequence, we can consider <strong>floating point operations be &quot;for free&quot;</strong> when we work in the memory-bounded regime as in this lecture.</p> <p>Naturally, when realizing that our PDE solvers&#39; are memory-bound, we start to analyze the memory throughput.</p> <p>In lecture 6, we have though seen that the <em>total memory throughput</em> is often not a good metric to evaluate the optimality of an implementation.</p> <p>As a result, we defined the <em>effective memory throughput</em>, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi mathvariant=normal >e</mi><mi mathvariant=normal >f</mi><mi mathvariant=normal >f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_\mathrm{eff}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, metric for iterative stencil-based PDE solvers.</p> <p>The effective memory access <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mrow><mi mathvariant=normal >e</mi><mi mathvariant=normal >f</mi><mi mathvariant=normal >f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">A_\mathrm{eff}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal">A</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> &#91;GB&#93; Sum of:</p> <ul> <li><p>twice the memory footprint of the unknown fields, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi mathvariant=normal >u</mi></msub></mrow><annotation encoding="application/x-tex">D_\mathrm{u}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">u</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, &#40;fields that depend on their own history and that need to be updated every iteration&#41;</p> <li><p>known fields, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi mathvariant=normal >k</mi></msub></mrow><annotation encoding="application/x-tex">D_\mathrm{k}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">k</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, that do not change every iteration.</p> </ul> <p>The effective memory access divided by the execution time per iteration, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mrow><mi mathvariant=normal >i</mi><mi mathvariant=normal >t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">t_\mathrm{it}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.76508em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal">t</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">i</span><span class="mord mathrm mtight">t</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> &#91;sec&#93;, defines the effective memory throughput, <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi mathvariant=normal >e</mi><mi mathvariant=normal >f</mi><mi mathvariant=normal >f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_\mathrm{eff}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> &#91;GB/s&#93;:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><msub><mi>A</mi><mrow><mi mathvariant=normal >e</mi><mi mathvariant=normal >f</mi><mi mathvariant=normal >f</mi></mrow></msub><mo>=</mo><mn>2</mn><mtext> </mtext><msub><mi>D</mi><mi mathvariant=normal >u</mi></msub><mo>+</mo><msub><mi>D</mi><mi mathvariant=normal >k</mi></msub></mrow><annotation encoding="application/x-tex"> A_\mathrm{eff} = 2~D_\mathrm{u} + D_\mathrm{k} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal">A</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord >2</span><span class="mspace nobreak"> </span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">u</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">k</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><msub><mi>T</mi><mrow><mi mathvariant=normal >e</mi><mi mathvariant=normal >f</mi><mi mathvariant=normal >f</mi></mrow></msub><mo>=</mo><mfrac><msub><mi>A</mi><mrow><mi mathvariant=normal >e</mi><mi mathvariant=normal >f</mi><mi mathvariant=normal >f</mi></mrow></msub><msub><mi>t</mi><mrow><mi mathvariant=normal >i</mi><mi mathvariant=normal >t</mi></mrow></msub></mfrac></mrow><annotation encoding="application/x-tex"> T_\mathrm{eff} = \frac{A_\mathrm{eff}}{t_\mathrm{it}} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:2.19633em;vertical-align:-0.8360000000000001em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.36033em;"><span style="top:-2.3139999999999996em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord ><span class="mord mathnormal">t</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">i</span><span class="mord mathrm mtight">t</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord ><span class="mord mathnormal">A</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> <p>The upper bound of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi mathvariant=normal >e</mi><mi mathvariant=normal >f</mi><mi mathvariant=normal >f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_\mathrm{eff}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi mathvariant=normal >p</mi><mi mathvariant=normal >e</mi><mi mathvariant=normal >a</mi><mi mathvariant=normal >k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_\mathrm{peak}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.969438em;vertical-align:-0.286108em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">p</span><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight">a</span><span class="mord mathrm mtight">k</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> as measured, e.g., by <a href="https://www.researchgate.net/publication/51992086_Memory_bandwidth_and_machine_balance_in_high_performance_computers">McCalpin, 1995</a> for CPUs or a GPU analogue. Defining the <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi mathvariant=normal >e</mi><mi mathvariant=normal >f</mi><mi mathvariant=normal >f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_\mathrm{eff}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span><span class="mord mathrm mtight" style="margin-right:0.07778em;">f</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> metric, we assume that:</p> <ol> <li><p>we evaluate an iterative stencil-based solver,</p> <li><p>the problem size is much larger than the cache sizes and</p> <li><p>the usage of time blocking is not feasible or advantageous &#40;reasonable for real-world applications&#41;.</p> </ol> <div class=note ><div class=title >💡 Note</div> <div class=messg >Fields within the effective memory access that do not depend on their own history; such fields can be re-computed on the fly or stored on-chip.</div></div> <p>When solving the <a href="#exercises_-_lecture_9">exercises</a> later, we invite you to a &quot;time travel&quot;&#33;</p> <p>We will ask you to solve them first on the consumer electronics GPU Titan Xm in double precision, even though its double precision floating point peak performance is very low; then, we will ask you to rerun your solutions on the Tesla V100 GPUs.</p> <p>Why do we talk about &quot;time travel&quot;?</p> <p>It is because the Titan Xm&#39;s ratio between compute access speed and memory access speed is much lower for double precision than for single precision.</p> <p>The ratio for double precision corresponds to what was common in the early 2000s. So we &quot;time-travel&quot; 15 to 20 years back&#33;</p> <p><img src="../assets/literate_figures/l10_flop_to_memaccess_ratio2.png" alt=flop_to_memaccess_ratio  /> <em>Source: John McCalpin, Texas Advanced Computing Center &#40;modified&#41;</em> <div class=note ><div class=title >💡 Note</div> <div class=messg >The position of the memory wall is to be considered very approximative.</div></div></p> <p>Let us compute the Titan Xm&#39;s ratio for double precision &#40;the specifications are, e.g., found <a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632">here</a>&#41;:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mfrac><mrow><mn>209</mn><mtext> </mtext><mrow><mo stretchy=false >[</mo><mi mathvariant=normal >G</mi><mi mathvariant=normal >F</mi><mi mathvariant=normal >l</mi><mi mathvariant=normal >o</mi><mi mathvariant=normal >p</mi><mi mathvariant=normal >/</mi><mi mathvariant=normal >s</mi><mo stretchy=false >]</mo></mrow></mrow><mrow><mn>337</mn><mtext> </mtext><mrow><mo stretchy=false >[</mo><mi mathvariant=normal >G</mi><mi mathvariant=normal >B</mi><mi mathvariant=normal >/</mi><mi mathvariant=normal >s</mi><mo stretchy=false >]</mo></mrow></mrow></mfrac><mtext> </mtext><mo>×</mo><mtext> </mtext><mn>8</mn><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex"> \frac{209 ~\mathrm{[GFlop/s]}}{337 ~\mathrm{[GB/s]}}~×~8 = 5 </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:2.363em;vertical-align:-0.936em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.427em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >3</span><span class=mord >3</span><span class=mord >7</span><span class="mspace nobreak"> </span><span class=mord ><span class=mopen >[</span><span class="mord mathrm">G</span><span class="mord mathrm">B</span><span class="mord mathrm">/</span><span class="mord mathrm">s</span><span class=mclose >]</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >2</span><span class=mord >0</span><span class=mord >9</span><span class="mspace nobreak"> </span><span class=mord ><span class=mopen >[</span><span class="mord mathrm">G</span><span class="mord mathrm">F</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">p</span><span class="mord mathrm">/</span><span class="mord mathrm">s</span><span class=mclose >]</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace nobreak"> </span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >×</span><span class="mspace nobreak"> </span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mord >8</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.64444em;vertical-align:0em;"></span><span class=mord >5</span></span></span></span></span> <p>So we can do only 5 floating point operations per number read from main memory or written to it.</p> <div class=note ><div class=title >💡 Note</div> <div class=messg >As a consquence, think in every exercise if you are compute-bound or memory-bound when using the Titan Xm in double precision&#33;</div></div> <p>As a side note, in single precision, we can do 80 floating point operations per number read from main memory or written to it:</p> <span class=katex-display ><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML" display=block ><semantics><mrow><mfrac><mrow><mn>6690</mn><mtext> </mtext><mrow><mo stretchy=false >[</mo><mi mathvariant=normal >G</mi><mi mathvariant=normal >F</mi><mi mathvariant=normal >l</mi><mi mathvariant=normal >o</mi><mi mathvariant=normal >p</mi><mi mathvariant=normal >/</mi><mi mathvariant=normal >s</mi><mo stretchy=false >]</mo></mrow></mrow><mrow><mn>337</mn><mtext> </mtext><mrow><mo stretchy=false >[</mo><mi mathvariant=normal >G</mi><mi mathvariant=normal >B</mi><mi mathvariant=normal >/</mi><mi mathvariant=normal >s</mi><mo stretchy=false >]</mo></mrow></mrow></mfrac><mtext> </mtext><mo>×</mo><mtext> </mtext><mn>4</mn><mo>=</mo><mn>80</mn></mrow><annotation encoding="application/x-tex"> \frac{6690 ~\mathrm{[GFlop/s]}}{337 ~\mathrm{[GB/s]}}~×~4 = 80 </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:2.363em;vertical-align:-0.936em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.427em;"><span style="top:-2.314em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >3</span><span class=mord >3</span><span class=mord >7</span><span class="mspace nobreak"> </span><span class=mord ><span class=mopen >[</span><span class="mord mathrm">G</span><span class="mord mathrm">B</span><span class="mord mathrm">/</span><span class="mord mathrm">s</span><span class=mclose >]</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >6</span><span class=mord >6</span><span class=mord >9</span><span class=mord >0</span><span class="mspace nobreak"> </span><span class=mord ><span class=mopen >[</span><span class="mord mathrm">G</span><span class="mord mathrm">F</span><span class="mord mathrm">l</span><span class="mord mathrm">o</span><span class="mord mathrm">p</span><span class="mord mathrm">/</span><span class="mord mathrm">s</span><span class=mclose >]</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace nobreak"> </span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >×</span><span class="mspace nobreak"> </span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mord >4</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.64444em;vertical-align:0em;"></span><span class=mord >8</span><span class=mord >0</span></span></span></span></span> <p><a href="#content">⤴ <em><strong>back to Content</strong></em></a></p> <h2 id=on-chip_memory_usage ><a href="#on-chip_memory_usage" class=header-anchor >On-chip memory usage</a></h2> <p>We now want get a high-level overview on how we can control on-chip memory in order to reduce redundant main memory access.</p> <p>In the essence, we want to store values that we need to access multiple times with a same thread or threadblock in on-chip memory, in order to avoid accessing them multiple times in main memory.</p> <p>Let us first look at the different kinds of memory.</p> <p>There is memory private to each thread &#40;&quot;local memory&quot;&#41;, shared between thread blocks &#40;&quot;shared memory&quot;&#41; and shared between all threads of the grid &#40;&quot;global memory&quot; or &quot;main memory&quot;&#41;.</p> <p><img src="../assets/literate_figures/l10_cuda_mem.png" alt=cuda_mem  /></p> <p>To use shared memory for our PDE solvers, we can use the strategy depicted in the following image:</p> <p><img src="../assets/literate_figures/l10_cuda_domain.png" alt=cuda_domain  /></p> <p>In this approach, we allocate a cell in shared memory per each thread of the block, <strong>plus a halo on all sides</strong>.</p> <p>The threads at the boundaries of the block will read from there when doing finite differences.</p> <p>As a result, we will need to read the data corresponding to a thread block &#40;see image&#41; only once to shared memory and then we can compute all the required finite differences reading only from there.</p> <p>Making basic use of &quot;local memory&quot; is very simple: it is enough to define a variable inside a kernel and it will be allocated private to the each thread.</p> <p>Scalars &#40;and possibly small arrays&#41; will be stored in registers if the kernel does not use too many &#40;else it is stored in global memory as noted earlier&#41;.</p> <p>This &quot;control&quot; of register usage becomes often particularly useful when each thread does not only compute the results for one cell but for multiple cells, e.g., adjacent in the last dimension.</p> <p>In that case, the registers can store, e.g., intermediate results.</p> <p>The following image shows the scenario where each thread computes the results for a column of cells in z dimension &#40;this can be achieved by simply doing a loop over the z dimension&#41;:</p> <p><img src="../assets/literate_figures/l10_cuda_column.png" alt=cuda_column  /></p> <p><a href="#content">⤴ <em><strong>back to Content</strong></em></a></p> <h1 id=exercises_-_lecture_10 ><a href="#exercises_-_lecture_10" class=header-anchor >Exercises - lecture 10</a></h1> <div class=warning ><div class=title >⚠️ Warning&#33;</div> <div class=messg >The exercises from lecture 10 are optional and should serve as basis for final projects aiming at implementing advanced code optimisations as well as to anyone interested.</div></div> <h2 id=exercise_1_-_push-ups_with_memory_copy ><a href="#exercise_1_-_push-ups_with_memory_copy" class=header-anchor >Exercise 1 - <strong>Push-ups with memory copy</strong></a></h2> <p>👉 See <a href="/logistics/#submission">Logistics</a> for submission details &#40;this exercise doesn&#39;t need to be handed-in&#41;.</p> <p>The goal of this exercise is to:</p> <ul> <li><p>ensure you can run a Julia notebook on your octopus node</p> <li><p>refresh how to establish the peak memory throughput of your GPU</p> </ul> <h3 id=getting_started ><a href="#getting_started" class=header-anchor >Getting started</a></h3> <p>👉 Make sure:</p> <ol> <li><p>You are logged-in to your node on octopus &#40;<a href="/software_install/#login_to_your_node">how-to</a>&#41;</p> <li><p>Head to <code>/scratch/&lt;username&gt;/lecture09/</code></p> <li><p>Get your Julia environment ready so that you can <a href="/software_install/#running_a_jupyter_notebook">start a Jupyter notebook server</a></p> <li><p>Open the introduction notebook <em>Benchmarking memory copy and establishing peak memory access performance</em> &#40;<code>/scratch/&lt;username&gt;/lecture09/l6_1-gpu-memcopy.ipynb</code>&#41;</p> <li><p>Run the notebook to establish the performance baseline <em>&#40;you should have a copy of it in your <code>lecture09</code> folder on octopus&#41;</em>.</p> </ol> <p><a href="#content">⤴ <em><strong>back to Content</strong></em></a></p> <hr /> <h2 id=exercise_2_-_advanced_data_transfer_optimisations_part_1 ><a href="#exercise_2_-_advanced_data_transfer_optimisations_part_1" class=header-anchor >Exercise 2 - <strong>Advanced data transfer optimisations &#40;part 1&#41;</strong></a></h2> <p>👉 See <a href="/logistics/#submission">Logistics</a> for submission details.</p> <p>The goal of this exercise is to:</p> <ul> <li><p>learn how to use shared memory &#40;on-chip&#41; to avoid main memory accesses.</p> </ul> <p>Prerequisites:</p> <ul> <li><p>the introduction notebook <em>Benchmarking memory copy and establishing peak memory access performance</em> &#40;<a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/slide-notebooks/notebooks/l6_1-gpu-memcopy.ipynb"><code>l6_1-gpu-memcopy.ipynb</code></a>&#41;</p> <li><p>the <em>Data transfer optimisation notebook</em> &#40;<a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/exercise-notebooks/notebooks/lecture6_ex1.ipynb"><code>lecture6_ex1.ipynb</code></a>&#41;</p> </ul> <p><a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/LICENSE.md"><em>This content is distributed under MIT licence. Authors: S. Omlin &#40;CSCS&#41;, L. Räss &#40;ETHZ&#41;.</em></a></p> <h3 id=getting_started__2 ><a href="#getting_started__2" class=header-anchor >Getting started</a></h3> <p>👉 Download the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/exercise-notebooks/notebooks/lecture10_ex2.ipynb"><code>lecture10_ex2.ipynb</code></a> notebook and edit it.</p> <p>We will again use the packages <code>CUDA</code>, <code>BenchmarkTools</code> and <code>Plots</code> to create a little performance laboratory:</p> <pre><code class="julia hljs">] activate .</code></pre>
<pre><code class="julia hljs">] instantiate</code></pre>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> CUDA
<span class=hljs-keyword >using</span> BenchmarkTools
<span class=hljs-keyword >using</span> Plots</code></pre>
<p>Let us consider the same 2-D heat diffusion solver as in the second part of the first <em>Data transfer optimisation notebook</em> &#40;<a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/exercise-notebooks/notebooks/lecture6_ex1.ipynb"><code>lecture6_ex1.ipynb</code></a>&#41;:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> diffusion2D()
    <span class=hljs-comment ># Physics</span>
    lam      = <span class=hljs-number >1.0</span>                                          <span class=hljs-comment ># Thermal conductivity</span>
    c0       = <span class=hljs-number >2.0</span>                                          <span class=hljs-comment ># Heat capacity</span>
    lx, ly   = <span class=hljs-number >10.0</span>, <span class=hljs-number >10.0</span>                                   <span class=hljs-comment ># Length of computational domain in dimension x and y</span>

    <span class=hljs-comment ># Numerics</span>
    nx, ny   = <span class=hljs-number >32</span>*<span class=hljs-number >2</span>, <span class=hljs-number >32</span>*<span class=hljs-number >2</span>                                   <span class=hljs-comment ># Number of gridpoints in dimensions x and y</span>
    nt       = <span class=hljs-number >100</span>                                          <span class=hljs-comment ># Number of time steps</span>
    dx       = lx/(nx-<span class=hljs-number >1</span>)                                    <span class=hljs-comment ># Space step in x-dimension</span>
    dy       = ly/(ny-<span class=hljs-number >1</span>)                                    <span class=hljs-comment ># Space step in y-dimension</span>
    _dx, _dy = <span class=hljs-number >1.0</span>/dx, <span class=hljs-number >1.0</span>/dy

    <span class=hljs-comment ># Array initializations</span>
    T    = CUDA.zeros(<span class=hljs-built_in >Float64</span>, nx, ny)                      <span class=hljs-comment ># Temperature</span>
    T2   = CUDA.zeros(<span class=hljs-built_in >Float64</span>, nx, ny)                      <span class=hljs-comment ># 2nd array for Temperature</span>
    Ci   = CUDA.zeros(<span class=hljs-built_in >Float64</span>, nx, ny)                      <span class=hljs-comment ># 1/Heat capacity</span>

    <span class=hljs-comment ># Initial conditions</span>
    Ci .= <span class=hljs-number >1</span>/c0                                              <span class=hljs-comment ># 1/Heat capacity (could vary in space)</span>
    T  .= CuArray([<span class=hljs-number >10.0</span>*exp(-(((ix-<span class=hljs-number >1</span>)*dx-lx/<span class=hljs-number >2</span>)/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span>-(((iy-<span class=hljs-number >1</span>)*dy-ly/<span class=hljs-number >2</span>)/<span class=hljs-number >2</span>)^<span class=hljs-number >2</span>) <span class=hljs-keyword >for</span> ix=<span class=hljs-number >1</span>:size(T,<span class=hljs-number >1</span>), iy=<span class=hljs-number >1</span>:size(T,<span class=hljs-number >2</span>)]) <span class=hljs-comment ># Initialization of Gaussian temperature anomaly</span>
    T2 .= T;                                                 <span class=hljs-comment ># Assign also T2 to get correct boundary conditions.</span>

    <span class=hljs-comment ># Time loop</span>
    dt  = min(dx^<span class=hljs-number >2</span>,dy^<span class=hljs-number >2</span>)/lam/maximum(Ci)/<span class=hljs-number >4.1</span>                <span class=hljs-comment ># Time step for 2D Heat diffusion</span>
    opts = (aspect_ratio=<span class=hljs-number >1</span>, xlims=(<span class=hljs-number >1</span>, nx), ylims=(<span class=hljs-number >1</span>, ny), clims=(<span class=hljs-number >0.0</span>, <span class=hljs-number >10.0</span>), c=:davos, xlabel=<span class=hljs-string >&quot;Lx&quot;</span>, ylabel=<span class=hljs-string >&quot;Ly&quot;</span>) <span class=hljs-comment ># plotting options</span>
    <span class=hljs-meta >@gif</span> <span class=hljs-keyword >for</span> it = <span class=hljs-number >1</span>:nt
        diffusion2D_step!(T2, T, Ci, lam, dt, _dx, _dy)     <span class=hljs-comment ># Diffusion time step.</span>
        heatmap(<span class=hljs-built_in >Array</span>(T)&#x27;; opts...)                         <span class=hljs-comment ># Visualization</span>
        T, T2 = T2, T                                       <span class=hljs-comment ># Swap the aliases T and T2 (does not perform any array copy)</span>
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> diffusion2D_step!(T2, T, Ci, lam, dt, _dx, _dy)
    threads = (<span class=hljs-number >32</span>, <span class=hljs-number >8</span>)
    blocks  = (size(T2,<span class=hljs-number >1</span>)÷threads[<span class=hljs-number >1</span>], size(T2,<span class=hljs-number >2</span>)÷threads[<span class=hljs-number >2</span>])
    <span class=hljs-meta >@cuda</span> blocks=blocks threads=threads update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)
<span class=hljs-keyword >end</span></code></pre>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)
    ix = (blockIdx().x-<span class=hljs-number >1</span>) * blockDim().x + threadIdx().x
    iy = (blockIdx().y-<span class=hljs-number >1</span>) * blockDim().y + threadIdx().y
    <span class=hljs-keyword >if</span> (ix&gt;<span class=hljs-number >1</span> &amp;&amp; ix&lt;size(T2,<span class=hljs-number >1</span>) &amp;&amp; iy&gt;<span class=hljs-number >1</span> &amp;&amp; iy&lt;size(T2,<span class=hljs-number >2</span>))
        <span class=hljs-meta >@inbounds</span> T2[ix,iy] = T[ix,iy] + dt*Ci[ix,iy]*(
                              - ((-lam*(T[ix+<span class=hljs-number >1</span>,iy] - T[ix,iy])*_dx) - (-lam*(T[ix,iy] - T[ix-<span class=hljs-number >1</span>,iy])*_dx))*_dx
                              - ((-lam*(T[ix,iy+<span class=hljs-number >1</span>] - T[ix,iy])*_dy) - (-lam*(T[ix,iy] - T[ix,iy-<span class=hljs-number >1</span>])*_dy))*_dy
                              )
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span></code></pre>
<p>Moreover, for benchmarking activities, we will require again the following arrays and scalars &#40;use again the <code>nx&#61;ny</code> found best in the introduction notebook; you can modify the value if it is not right for you&#41;:</p>
<pre><code class="julia hljs">nx = ny = <span class=hljs-number >512</span>*<span class=hljs-number >32</span>
T    = CUDA.rand(<span class=hljs-built_in >Float64</span>, nx, ny);
T2   = CUDA.rand(<span class=hljs-built_in >Float64</span>, nx, ny);
Ci   = CUDA.rand(<span class=hljs-built_in >Float64</span>, nx, ny);
lam = _dx = _dy = dt = rand();</code></pre>
<p>In the introduction notebook, we determined how the performance of memory copy behaved with in function of the number of threads per blocks. We will do the same now for the temperature update kernel.</p>
<div class=note ><div class=title >💡 Note</div>
<div class=messg >Make sure to have no other notebook <strong>kernel</strong> running; array sizes are close to device DRAM max and you may get an out-of-mem error otherwise.</div></div>
<h3 id=task_1_performance_evaluation ><a href="#task_1_performance_evaluation" class=header-anchor >Task 1 &#40;Performance evaluation&#41;</a></h3>
<p>Determine the effective memory throughput, <code>T_eff</code>, of the kernel <code>update_temperature&#33;</code> in function of the number of threads, fixing the number of threads in x dimension to 32. <div class=note ><div class=title >💡 Note</div>
<div class=messg >You can base yourself on the corresponding activity in the introduction notebook &#40;remember to compute now <code>T_eff</code> rather than <code>T_tot</code>&#41;.</div></div></p>
<pre><code class="julia hljs"><span class=hljs-comment ># solution</span>
max_threads  = attribute(device(),CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)
thread_count = []
throughputs  = []
<span class=hljs-keyword >for</span> pow = <span class=hljs-number >0</span>:<span class=hljs-built_in >Int</span>(log2(max_threads/<span class=hljs-number >32</span>))
    threads = (<span class=hljs-number >32</span>, <span class=hljs-number >2</span>^pow)
    blocks  = <span class=hljs-comment >#...</span>
    t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> <span class=hljs-meta >@cuda</span> <span class=hljs-comment >#...</span>
    T_eff = <span class=hljs-comment >#...</span>
    push!(thread_count, prod(threads))
    push!(throughputs, T_eff)
    println(<span class=hljs-string >&quot;(threads=<span class=hljs-variable >$threads</span>) T_eff = <span class=hljs-subst >$(T_eff)</span>&quot;</span>)
<span class=hljs-keyword >end</span></code></pre>
<p>Save the best thread/block configuration measured for reusing it later &#40;adapt the code if your variable names above do not match&#41;:</p>
<pre><code class="julia hljs">T_tot_max, index = findmax(throughputs)
threads = (<span class=hljs-number >32</span>, thread_count[index]÷<span class=hljs-number >32</span>)
blocks  = (nx÷threads[<span class=hljs-number >1</span>], ny÷threads[<span class=hljs-number >2</span>])</code></pre>
<p>You could probably observe that this kernel is more sensitive to the thread/block configuration than the memory copy kernel. The reason is that the thread/block configuration directly influences the way the fast memory situated on-chip &#40;here high-level cache and registers&#41; is used in order to avoid redundant main memory accesses. We will now explicitly control part of the the on-chip memory usage, using so called &quot;shared memory&quot;, which is repurposed high-level cache. This will give some insights on how certain parameters relate to on-chip memory usage. However, we will not implement a diffusion kernel with shared memory at once, but in little steps.</p>
<p>Let us start with relating the <code>update_temperature&#33;</code> kernel back to the triad memory copy kernel investigated in the introduction notebook. We can observe that if we remove the derivatives from the <code>update_temperature&#33;</code> kernel then we end up with a simple triad memory copy kernel, except for an additional if-statement to avoid updating the boundary values &#40;for simplicity, we do not remove the unused function arguments which we will use again in the next experiments&#41;:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)
    ix = (blockIdx().x-<span class=hljs-number >1</span>) * blockDim().x + threadIdx().x
    iy = (blockIdx().y-<span class=hljs-number >1</span>) * blockDim().y + threadIdx().y
    <span class=hljs-keyword >if</span> (ix&gt;<span class=hljs-number >1</span> &amp;&amp; ix&lt;size(T2,<span class=hljs-number >1</span>) &amp;&amp; iy&gt;<span class=hljs-number >1</span> &amp;&amp; iy&lt;size(T2,<span class=hljs-number >2</span>))
        <span class=hljs-meta >@inbounds</span> T2[ix,iy] = T[ix,iy] + dt*Ci[ix,iy]
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span></code></pre>
<p>This kernel should consequently achieve a <code>T_tot</code> of essentially the value of <code>T_peak</code> &#40;if an optimal thread/block configuration is used&#41;. Moreover, for this case <code>T_eff &#61; T_tot</code>. Let us verify quickly that <code>T_eff</code> is essentially equal <code>T_peak</code> here &#40;measured 561 GB/s with the Tesla P100 GPU&#41;:</p>
<pre><code class="julia hljs">t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> <span class=hljs-meta >@cuda</span> blocks=$blocks threads=$threads update_temperature!($T2, $T, $Ci, $lam, $dt, $_dx, $_dy); synchronize() <span class=hljs-keyword >end</span>
T_eff = (<span class=hljs-number >2</span>*<span class=hljs-number >1</span>+<span class=hljs-number >1</span>)*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<p>We will do now our first shared memory experiment with this simple triad kernel.</p>
<h3 id=task_2_shared_memory_basics ><a href="#task_2_shared_memory_basics" class=header-anchor >Task 2 &#40;Shared memory basics&#41;</a></h3>
<p>Modify the above <code>update_temperature&#33;</code> kernel &#40;which now does just triad memory copy&#41; as follows: read the values of the temperature array <code>T</code> into shared memory; then, subsequently, read the temperature values from there when updating <code>T2</code>. To help you, the structure of the kernel is already given; you only need to complete the unfinished lines.</p>
<p><div class=note ><div class=title >💡 Note</div>
<div class=messg >Use <a href="https://juliagpu.gitlab.io/CUDA.jl/api/kernel/#CUDA.@cuDynamicSharedMem"><code>@cuDynamicSharedMem</code></a> to allocate the required shared memory</div></div> <div class=note ><div class=title >💡 Note</div>
<div class=messg >Shared memory is block-local, i.e., shared between the threads of a same block.</div></div> <div class=note ><div class=title >💡 Note</div>
<div class=messg >Shared memory as well as registers are a very limited resource and the amount a kernel needs increases normally with the number of threads launched per block. As a result, the maximum number of threads launchable per block can be restricted by the needed on-chip resources to a value less than the general limit of the device &#40;attribute <code>CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK</code>&#41;. The CUDA occupancy API lets query the maximum number of threads possible for a given kernel &#40;see <a href="https://cuda.juliagpu.org/stable/api/compiler/#CUDA.maxthreads"><code>maxthreads</code></a>&#41;.</div></div></p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)
    ix = (blockIdx().x-<span class=hljs-number >1</span>) * blockDim().x + threadIdx().x
    iy = (blockIdx().y-<span class=hljs-number >1</span>) * blockDim().y + threadIdx().y
    tx = <span class=hljs-comment ># local thread id, x dimension</span>
    ty = <span class=hljs-comment ># local thread id, y dimension</span>
    T_l = <span class=hljs-comment ># allocation of a block-local temperature array (in shared memory)</span>
    <span class=hljs-meta >@inbounds</span> T_l[tx,ty] = <span class=hljs-comment ># read the values of the temperature array `T` into shared memory</span>
    <span class=hljs-keyword >if</span> (ix&gt;<span class=hljs-number >1</span> &amp;&amp; ix&lt;size(T2,<span class=hljs-number >1</span>) &amp;&amp; iy&gt;<span class=hljs-number >1</span> &amp;&amp; iy&lt;size(T2,<span class=hljs-number >2</span>))
        <span class=hljs-meta >@inbounds</span> T2[ix,iy] = <span class=hljs-comment >#=read temperature values from shared memory=#</span>  + dt*Ci[ix,iy]
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span></code></pre>
<h3 id=task_3_shared_memory_basics ><a href="#task_3_shared_memory_basics" class=header-anchor >Task 3 &#40;Shared memory basics&#41;</a></h3>
<p>Launch the kernel requesting the required amount of shared memory; compute the <code>T_eff</code> achieved. <div class=note ><div class=title >💡 Note</div>
<div class=messg >The <code>@cuda</code> macro supports the keyword <code>shmem</code> to request the required amount of shared memory; note that it must be indicated in bytes &#40;use sizeof&#40;&#41; to get the number of bytes used by the datatype used&#41;.</div></div></p>
<pre><code class="julia hljs"><span class=hljs-comment ># solution</span></code></pre>
<p>You should not observe any significant change in <code>T_eff</code> compared to the previous kernel &#40;measured as before 561 GB/s with the Tesla P100 GPU&#41;.</p>
<p>When we will add back the derivatives later, then each thread will read values on the left, right, bottom and top of it. We will want the threads to read the temperature values from the block-local array <code>T_l</code>, not from <code>T</code> anymore. However, right now each thread maps directly to a cell of <code>T_l</code>; thus, the threads at the boundary of the block would read out-of-bounds when reading the &quot;neighbour cells&quot;. We therefore need to add a &quot;halo&quot; to <code>T_l</code> that will contain the required values.</p>
<h3 id=task_4_shared_memory ><a href="#task_4_shared_memory" class=header-anchor >Task 4 &#40;Shared memory&#41;</a></h3>
<p>Modify the <code>update_temperature&#33;</code> kernel from Task 2 as follows: add a &quot;halo&quot; of size <code>1</code> to <code>T_l</code> on each side, i.e. on the left, right, bottom and top. To this purpose, you need to modify the allocation of <code>T_l</code> and adapt the local thread ids <code>tx</code> and <code>ty</code> accordingly. Then, launch the new kernel adjusting the required amount of shared memory and compute <code>T_eff</code>. To help you, the structure of the kernel is already given; you only need to complete the unfinished lines.</p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)
    ix = (blockIdx().x-<span class=hljs-number >1</span>) * blockDim().x + threadIdx().x
    iy = (blockIdx().y-<span class=hljs-number >1</span>) * blockDim().y + threadIdx().y
    tx =  <span class=hljs-comment ># adjust the local thread id in y dimension</span>
    ty =  <span class=hljs-comment ># adjust the local thread id in y dimension</span>
    T_l = <span class=hljs-comment ># adjust the shared memory allocation</span>
    <span class=hljs-meta >@inbounds</span> T_l[tx,ty] = T[ix,iy]
    <span class=hljs-keyword >if</span> (ix&gt;<span class=hljs-number >1</span> &amp;&amp; ix&lt;size(T2,<span class=hljs-number >1</span>) &amp;&amp; iy&gt;<span class=hljs-number >1</span> &amp;&amp; iy&lt;size(T2,<span class=hljs-number >2</span>))
        <span class=hljs-meta >@inbounds</span> T2[ix,iy] = T_l[tx,ty] + dt*Ci[ix,iy]
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span>

t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> <span class=hljs-meta >@cuda</span> blocks=$blocks threads=$threads shmem=<span class=hljs-comment >#=adjust the shared memory=#</span> update_temperature!($T2, $T, $Ci, $lam, $dt, $_dx, $_dy); synchronize() <span class=hljs-keyword >end</span>
T_eff = (<span class=hljs-number >2</span>*<span class=hljs-number >1</span>+<span class=hljs-number >1</span>)*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<p><code>T_eff</code> did certainly not significantly change, as you probably expected as we did not access more data than before &#40;measured as before 561 GB/s with the Tesla P100 GPU&#41;.</p>
<h3 id=task_5_shared_memory ><a href="#task_5_shared_memory" class=header-anchor >Task 5 &#40;Shared memory&#41;</a></h3>
<p>Modify the <code>update_temperature&#33;</code> kernel from Task 4 as follows: read the required values into the newly added halo of <code>T_l</code>. Then, compute again <code>T_eff</code>. To help you, the structure of the kernel is already given; you only need to complete the unfinished lines.</p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)
    ix = (blockIdx().x-<span class=hljs-number >1</span>) * blockDim().x + threadIdx().x
    iy = (blockIdx().y-<span class=hljs-number >1</span>) * blockDim().y + threadIdx().y
    tx = threadIdx().x+<span class=hljs-number >1</span>
    ty = threadIdx().y+<span class=hljs-number >1</span>
    T_l = <span class=hljs-meta >@cuDynamicSharedMem</span>(eltype(T), (blockDim().x+<span class=hljs-number >2</span>, blockDim().y+<span class=hljs-number >2</span>))
    <span class=hljs-meta >@inbounds</span> T_l[tx,ty] = T[ix,iy]
    <span class=hljs-keyword >if</span> (ix&gt;<span class=hljs-number >1</span> &amp;&amp; ix&lt;size(T2,<span class=hljs-number >1</span>) &amp;&amp; iy&gt;<span class=hljs-number >1</span> &amp;&amp; iy&lt;size(T2,<span class=hljs-number >2</span>))
        <span class=hljs-meta >@inbounds</span> <span class=hljs-keyword >if</span> (threadIdx().x == <span class=hljs-number >1</span>)            <span class=hljs-comment >#=read the required values to the left halo of `T_l`=#</span> <span class=hljs-keyword >end</span>
        <span class=hljs-meta >@inbounds</span> <span class=hljs-keyword >if</span> (threadIdx().x == blockDim().x) <span class=hljs-comment >#=read the required values to the right halo of `T_l`=#</span> <span class=hljs-keyword >end</span>
        <span class=hljs-meta >@inbounds</span> <span class=hljs-keyword >if</span>                                 <span class=hljs-comment >#=read the required values to the bottom halo of `T_l`=#</span> <span class=hljs-keyword >end</span>
        <span class=hljs-meta >@inbounds</span> <span class=hljs-keyword >if</span>                                 <span class=hljs-comment >#=read the required values to the top halo of `T_l`=#</span> <span class=hljs-keyword >end</span>
        <span class=hljs-meta >@inbounds</span> T2[ix,iy] = T_l[tx,ty] + dt*Ci[ix,iy]
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span>

t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> <span class=hljs-meta >@cuda</span> blocks=$blocks threads=$threads shmem=prod($threads.+<span class=hljs-number >2</span>)*sizeof(<span class=hljs-built_in >Float64</span>) update_temperature!($T2, $T, $Ci, $lam, $dt, $_dx, $_dy); synchronize() <span class=hljs-keyword >end</span>
T_eff = (<span class=hljs-number >2</span>*<span class=hljs-number >1</span>+<span class=hljs-number >1</span>)*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<p><code>T_eff</code> certainly decreased a bit due to the additional read-in of the halo of <code>T_l</code> &#40;measured 538 GB/s with the Tesla P100 GPU&#41;, except if the compiler would have understood that the halo is never used and therefore never done these additional reads. In order to create the 2-D diffusion kernel using shared memory, the last step is to add back the derivatives.</p>
<h3 id=task_6_shared_memory ><a href="#task_6_shared_memory" class=header-anchor >Task 6 &#40;Shared memory&#41;</a></h3>
<p>Modify the <code>update_temperature&#33;</code> kernel from Task 5 as follows: add back the derivatives that we removed at the beginning of the notebook and modify them to read the temperature from <code>T_l</code> rather then from <code>T</code>. Then, verify that the diffusion works as expected and compute again <code>T_eff</code>.</p>
<p>To help you, the structure of the kernel is already given; you only need to complete the unfinished lines.</p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> update_temperature!(T2, T, Ci, lam, dt, _dx, _dy)
    ix = (blockIdx().x-<span class=hljs-number >1</span>) * blockDim().x + threadIdx().x
    iy = (blockIdx().y-<span class=hljs-number >1</span>) * blockDim().y + threadIdx().y
    tx = threadIdx().x+<span class=hljs-number >1</span>
    ty = threadIdx().y+<span class=hljs-number >1</span>
    T_l = <span class=hljs-meta >@cuDynamicSharedMem</span>(eltype(T), (blockDim().x+<span class=hljs-number >2</span>, blockDim().y+<span class=hljs-number >2</span>))
    <span class=hljs-meta >@inbounds</span> T_l[tx,ty] = T[ix,iy]
    <span class=hljs-keyword >if</span> (ix&gt;<span class=hljs-number >1</span> &amp;&amp; ix&lt;size(T2,<span class=hljs-number >1</span>) &amp;&amp; iy&gt;<span class=hljs-number >1</span> &amp;&amp; iy&lt;size(T2,<span class=hljs-number >2</span>))
        <span class=hljs-meta >@inbounds</span> <span class=hljs-keyword >if</span> (threadIdx().x == <span class=hljs-number >1</span>)            T_l[tx-<span class=hljs-number >1</span>,ty] = T[ix-<span class=hljs-number >1</span>,iy] <span class=hljs-keyword >end</span>
        <span class=hljs-meta >@inbounds</span> <span class=hljs-keyword >if</span> (threadIdx().x == blockDim().x) T_l[tx+<span class=hljs-number >1</span>,ty] = T[ix+<span class=hljs-number >1</span>,iy] <span class=hljs-keyword >end</span>
        <span class=hljs-meta >@inbounds</span> <span class=hljs-keyword >if</span> (threadIdx().y == <span class=hljs-number >1</span>)            T_l[tx,ty-<span class=hljs-number >1</span>] = T[ix,iy-<span class=hljs-number >1</span>] <span class=hljs-keyword >end</span>
        <span class=hljs-meta >@inbounds</span> <span class=hljs-keyword >if</span> (threadIdx().y == blockDim().y) T_l[tx,ty+<span class=hljs-number >1</span>] = T[ix,iy+<span class=hljs-number >1</span>] <span class=hljs-keyword >end</span>
        sync_threads()
        <span class=hljs-meta >@inbounds</span> T2[ix,iy] = T_l[tx,ty] + dt*Ci[ix,iy]*(
                    <span class=hljs-comment ># add the computation of the derivatives</span>
                    <span class=hljs-comment ># ...</span>
                    )
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span>
<span class=hljs-keyword >end</span>

diffusion2D()

t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> <span class=hljs-meta >@cuda</span> blocks=$blocks threads=$threads shmem=prod($threads.+<span class=hljs-number >2</span>)*sizeof(<span class=hljs-built_in >Float64</span>) update_temperature!($T2, $T, $Ci, $lam, $dt, $_dx, $_dy); synchronize() <span class=hljs-keyword >end</span>
T_eff = (<span class=hljs-number >2</span>*<span class=hljs-number >1</span>+<span class=hljs-number >1</span>)*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<div class=note ><div class=title >💡 Note</div>
<div class=messg >The we have added a call to <code>sync_threads&#40;&#41;</code> at the end of all reads into shared memory &#40;i.e. <code>T_l</code>&#41; in order to ensure that no thread tries to read a from a &quot;neighboring cell&quot; before it contains the required value.</div></div>
<p><code>T_eff</code> should not have decreased significantly when adding back the derivatives &#40;measured, as in Task 5, 538 GB/s with the Tesla P100 GPU&#41; even though they constitute the major part of the computations&#33; This confirms one more time empirically that the performance of solvers as the above is essentially defined by how much we can avoid redundant main memory accesses.</p>
<h3 id=task_7_performance_evaluation ><a href="#task_7_performance_evaluation" class=header-anchor >Task 7 &#40;Performance evaluation&#41;</a></h3>
<p>Compute by how much percent you can improve the performance of the solver at most.</p>
<pre><code class="julia hljs"><span class=hljs-comment ># solution</span></code></pre>
<p>Congratulations&#33; You have implemented a 2-D diffusion solver using shared memory&#33;</p>

<p><a href="#content">⤴ <em><strong>back to Content</strong></em></a></p>
<hr />
<h2 id=exercise_3_-_advanced_data_transfer_optimisations_part_2 ><a href="#exercise_3_-_advanced_data_transfer_optimisations_part_2" class=header-anchor >Exercise 3 - <strong>Advanced data transfer optimisations &#40;part 2&#41;</strong></a></h2>
<p>👉 See <a href="/logistics/#submission">Logistics</a> for submission details.</p>
<p>The goal of this exercise is to:</p>
<ul>
<li><p>learn how to control registers for storing intermediate results on-chip;</p>

<li><p>learn how to use shared memory &#40;on-chip&#41; to communicate between threads.</p>

</ul>
<p>Prerequisites:</p>
<ul>
<li><p>the introduction notebook <em>Benchmarking memory copy and establishing peak memory access performance</em> &#40;<a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/slide-notebooks/notebooks/l6_1-gpu-memcopy.ipynb"><code>l6_1-gpu-memcopy.ipynb</code></a>&#41;</p>

<li><p>the <em>Data transfer optimisation notebook</em> &#40;<a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/exercise-notebooks/notebooks/lecture6_ex1.ipynb"><code>lecture6_ex1.ipynb</code></a>&#41;</p>

<li><p>the second <em>Data transfer optimisation notebook</em> &#40;Exercise 2 <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/exercise-notebooks/notebooks/lecture10_ex2.ipynb"><code>lecture10_ex2.ipynb</code></a>&#41;</p>

</ul>
<p><a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/LICENSE.md"><em>This content is distributed under MIT licence. Authors: S. Omlin &#40;CSCS&#41;, L. Räss &#40;ETHZ&#41;.</em></a></p>
<h3 id=getting_started__3 ><a href="#getting_started__3" class=header-anchor >Getting started</a></h3>
<p>👉 Download the <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/exercise-notebooks/notebooks/lecture10_ex3.ipynb"><code>lecture9_ex3.ipynb</code></a> notebook and edit it.</p>
<p>We will again use the packages <code>CUDA</code>, <code>BenchmarkTools</code> and <code>Plots</code> to create a little performance laboratory:</p>
<pre><code class="julia hljs">] activate .</code></pre>
<pre><code class="julia hljs">] instantiate</code></pre>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> CUDA
<span class=hljs-keyword >using</span> BenchmarkTools
<span class=hljs-keyword >using</span> Plots</code></pre>
<p>In the last notebook &#40;<a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/exercise-notebooks/notebooks/lecture9_ex2.ipynb"><code>lecture9_ex2.ipynb</code></a>&#41;, you learned how to explicitly control part of the the on-chip memory usage, using so called &quot;shared memory&quot;. We will learn now how to control a second kind of fast memory on-chip: registers. To this purpose we will implement the <code>cumsum&#33;</code> function on GPU - for the sake of simplicity, we will only write it for 3-D arrays.</p>
<p>Here is the documentation of the function <code>cumsum&#33;</code></p>
<pre><code class="julia-repl hljs">help?&gt; CUDA.cumsum!
  cumsum!(B, A; dims::Integer)

  Cumulative sum of A along the dimension dims, storing the result in B. See
  also cumsum.</code></pre>
<p>And here is a small example of how cumsum&#33; works for 3-D arrays:</p>
<pre><code class="julia hljs">A = CUDA.ones(<span class=hljs-number >4</span>,<span class=hljs-number >4</span>,<span class=hljs-number >4</span>)
B = CUDA.zeros(<span class=hljs-number >4</span>,<span class=hljs-number >4</span>,<span class=hljs-number >4</span>)
cumsum!(B, A; dims=<span class=hljs-number >1</span>)
cumsum!(B, A; dims=<span class=hljs-number >2</span>)
cumsum!(B, A; dims=<span class=hljs-number >3</span>)</code></pre>
<p>For benchmarking activities, we will allocate again large arrays, matching closely the number of grid points of the array size found best in the introduction notebook &#40;you can modify the value if it is not right for you&#41;:</p>
<pre><code class="julia hljs">nx = ny = nz = <span class=hljs-number >512</span>
A = CUDA.rand(<span class=hljs-built_in >Float64</span>, nx, ny, nz);
B = CUDA.zeros(<span class=hljs-built_in >Float64</span>, nx, ny, nz);</code></pre>
<p>Moreover, we preallocate also an array to store reference results obtained from <code>CUDA.cumsum&#33;</code> for verification.</p>
<pre><code class="julia hljs">B_ref = CUDA.zeros(<span class=hljs-built_in >Float64</span>, nx, ny, nz);</code></pre>
<p>Now, we are set up to get started.</p>
<p>If we only consider main memory access, then the <code>cumsum&#33;</code> will ideally need to read one array &#40;<code>A</code>&#41; and write one array &#40;<code>B</code>&#41;. No other main memory access should be required. Let us therefore create an ad hoc adaption of the effective memory throughput metric for iterative PDE solvers &#40;presented in the second notebook&#41; to this problem. We will call the metric <code>T_eff_cs</code> and compute it as follows: <code>T_eff_cs &#61; 2*1/1e9*nx*ny*nz*sizeof&#40;Float64&#41;/t_it</code></p>
<p>The upper bound of <code>T_eff_cs</code> is given by the maximum throughput achievable when copying exactly one aray to another. For this specific case, you might have measured in our benchmarks in the introduction notebook a slightly less good performance &#40;measured 550 GB/s with the Tesla P100 GPU&#41; then what we established as <code>T_peak</code>. Thus, we will consider here this measured value to be the upper bound of <code>T_eff_cs</code>.</p>
<p>We will now adapt the corresponding 2-D memory copy kernel in the most straightforward way to work for 3-D arrays. Then, we will modify this memory copy code in little steps until we have a well performing <code>cumsum&#33;</code> GPU function.</p>
<p>Here is the adapted memory copy kernel from the introduction notebook and the <code>T_tot</code> we achieve with it &#40;for the first two dimensions, we use again the thread/block configuration that we found best in the introduction notebook and we use one thread in the third dimension; you can modify the values if it is not right for you&#41;:</p>
<div class=note ><div class=title >💡 Note</div>
<div class=messg >The usage of the variables <code>A</code> and <code>B</code> is reversed in comparison with the previous notebooks in order to match the documentation of cumsum&#33;.</div></div>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> memcopy!(B, A)
    ix = (blockIdx().x-<span class=hljs-number >1</span>) * blockDim().x + threadIdx().x
    iy = (blockIdx().y-<span class=hljs-number >1</span>) * blockDim().y + threadIdx().y
    iz = (blockIdx().z-<span class=hljs-number >1</span>) * blockDim().z + threadIdx().z
    <span class=hljs-meta >@inbounds</span> B[ix,iy,iz] = A[ix,iy,iz]
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span></code></pre>
<pre><code class="julia hljs">threads = (<span class=hljs-number >32</span>, <span class=hljs-number >8</span>, <span class=hljs-number >1</span>)
blocks  = nx.÷threads
t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> <span class=hljs-meta >@cuda</span> blocks=$blocks threads=$threads memcopy!($B, $A); synchronize() <span class=hljs-keyword >end</span>
T_tot = <span class=hljs-number >2</span>*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*nz*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<p>Let us first implement the cummulative sum over the third dimension &#40;index <code>iz</code>&#41;. The first step is to read in A &#40;and write B&#41; in a way that will later allow to easily do the required cumsums, which is an inherently serial operation. However, we want to try to avoid a serious degradation of the memory throughput when doing so.</p>
<h3 id=task_1_grid-stride_loops ><a href="#task_1_grid-stride_loops" class=header-anchor >Task 1 &#40;grid-stride loops&#41;</a></h3>
<p>Modify the above <code>memcopy&#33;</code> kernel to read in A and write B in a serial manner with respect to the third dimension &#40;index <code>iz</code>&#41;, i.e., with parallelization only over the first two dimensions &#40;index <code>ix</code> and <code>iy</code>&#41;. Launch the kernel with the same thread configuration as above and adapt the the block configuration for the third dimension correctly. Verify the correctness of your kernel. Then, compute <code>T_tot</code> and explain the measured performance. <div class=note ><div class=title >💡 Note</div>
<div class=messg >You need to launch the kernel with only one thread and one block in the third dimension and, inside the kernel, do a loop over the third dimension. Use <code>iz</code> as loop index as it will replace the previous <code>iz</code> computed from the thread location in the grid.</div></div> <div class=note ><div class=title >💡 Note</div>
<div class=messg >The operator <code>≈</code> allows to check if two arrays contain the same values &#40;within a tolerance&#41;. Use this to verify your memory copy kernel.</div></div></p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> memcopy!(B, A)
    ix = (blockIdx().x-<span class=hljs-number >1</span>) * blockDim().x + threadIdx().x
    iy = (blockIdx().y-<span class=hljs-number >1</span>) * blockDim().y + threadIdx().y
    <span class=hljs-keyword >for</span> iz <span class=hljs-comment >#...</span>
        <span class=hljs-comment >#...</span>
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span>

threads = (<span class=hljs-number >32</span>, <span class=hljs-number >8</span>, <span class=hljs-number >1</span>)
blocks  = (nx÷threads[<span class=hljs-number >1</span>], ny÷threads[<span class=hljs-number >2</span>], <span class=hljs-number >1</span>)</code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Verification</span>
B .= <span class=hljs-number >0.0</span>;
<span class=hljs-meta >@cuda</span> <span class=hljs-comment >#...</span>
B ≈ A</code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Performance</span>
t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> <span class=hljs-comment >#...# end</span>
T_tot = <span class=hljs-number >2</span>*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*nz*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<p>Great&#33; You just implemented a so called <a href="https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/">grid-stride loop</a>. It exhibits a good memory access pattern and it allows to easily reuse, e.g., intermediate results from previous iterations in the loop. You probably observe a <code>T_tot</code> that is a bit below the one measured in the previous experiment &#40;measured 520 GB/s with the Tesla P100 GPU&#41;. There is certainly room to improve this memory copy kernel a bit, but we will consider it sufficient in order to continue with this exercise.</p>
<h3 id=task_2_controlling_registers ><a href="#task_2_controlling_registers" class=header-anchor >Task 2 &#40;controlling registers&#41;</a></h3>
<p>Write a kernel <code>cumsum_dim3&#33;</code> which computes the cumulative sum over the 3rd dimension of a 3-D array. To this purpose modify the memory copy kernel from Task 1. Verify the correctness of your kernel against <code>CUDA.cumsum&#33;</code>. Then, compute <code>T_eff_cs</code> as defined above, explain the measured performance and compare it to the one obtained with the generic <code>CUDA.cumsum&#33;</code>. <div class=note ><div class=title >💡 Note</div>
<div class=messg >Define a variable <code>cumsum_iz</code> before the loop and initialize it to 0.0 in order to cumulate the sum.</div></div> <div class=note ><div class=title >💡 Note</div>
<div class=messg >The operator <code>≈</code> allows to check if two arrays contain the same values &#40;within a tolerance&#41;. Use this to verify your results against <code>CUDA.cumsum&#33;</code> &#40;remember that for the verification, we already preallocated an array <code>B_ref</code> at the beginning, which you can use now&#41;.</div></div></p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> cumsum_dim3!(B, A)
    ix = (blockIdx().x-<span class=hljs-number >1</span>) * blockDim().x + threadIdx().x
    iy = (blockIdx().y-<span class=hljs-number >1</span>) * blockDim().y + threadIdx().y
    cumsum_iz = <span class=hljs-number >0.0</span>
    <span class=hljs-keyword >for</span> iz <span class=hljs-comment >#...</span>
        <span class=hljs-comment >#...</span>
    <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span>

<span class=hljs-comment ># Verification</span>
<span class=hljs-meta >@cuda</span> <span class=hljs-comment >#...</span>
CUDA.cumsum!(B_ref, A; dims=<span class=hljs-number >3</span>);
B ≈ B_ref</code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Performance</span>
t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> <span class=hljs-meta >@cuda</span> <span class=hljs-comment >#...# end</span>
T_eff_cs = <span class=hljs-number >2</span>*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*nz*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<pre><code class="julia hljs">t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> CUDA.cumsum!($B, $A; dims=<span class=hljs-number >3</span>); synchronize() <span class=hljs-keyword >end</span>
T_eff_cs = <span class=hljs-number >2</span>*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*nz*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<p>You should observe no significant difference between <code>T_eff_cs</code> measured here and <code>T_tot</code> computed in Task 1 &#40;measured 520 GB/s with the Tesla P100 GPU&#41;. In fact, scalar variables defined in a kernel, like <code>cumsum_iz</code>, will be stored in registers as long as there are enough available &#40;if not, then so called <a href="https://developer.download.nvidia.com/CUDA/training/register_spilling.pdf">register spilling</a> to main memory takes place&#41;. As access to register is extremely fast, the summation added in this Task did certainly not reduce the measured performance. It is, once again, the memory copy speed that completely determines the performance of the kernel, because we have successfully controlled the use of registers&#33;</p>
<p>We will now implement the cummulative sum over the 2nd dimension &#40;index <code>iy</code>&#41;. As for the previous implementation, the first step is to read in A &#40;and write B&#41; in a way that will later allow to easily do the required cumsums without exhibiting significant memory throughput degradation.</p>
<h3 id=task_3_kernel_loops ><a href="#task_3_kernel_loops" class=header-anchor >Task 3 &#40;kernel loops&#41;</a></h3>
<p>Modify the <code>memcopy&#33;</code> kernel given in the beginning to read in A and write B in a serial manner with respect to the second dimension &#40;index <code>iy</code>&#41;, i.e., with parallelization only over the first and the last dimensions &#40;index <code>ix</code> and <code>iz</code>&#41;. Launch the kernel with the same amount of threads as in Task 1, however, place them all in the first dimension &#40;i.e. use one thread in the second and third dimensions&#41;; adapt the the block configuration correctly. Verify the correctness of your kernel. Then, compute <code>T_tot</code> and explain the measured performance.</p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> memcopy!(B, A)
    <span class=hljs-comment >#...</span>
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span>

threads = (<span class=hljs-number >256</span>, <span class=hljs-number >1</span>, <span class=hljs-number >1</span>)
blocks  = <span class=hljs-comment >#...</span></code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Verification</span>
B .= <span class=hljs-number >0.0</span>;
<span class=hljs-meta >@cuda</span> <span class=hljs-comment >#...</span>
B ≈ A</code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Performance</span>
t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> <span class=hljs-meta >@cuda</span> <span class=hljs-comment >#...# end</span>
T_tot = <span class=hljs-number >2</span>*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*nz*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<p>You should observe no big difference between <code>T_tot</code> measured here and <code>T_tot</code> computed in Task 1 &#40;measured 519 GB/s with the Tesla P100 GPU&#41; as this kernel is accessing memory also in large contiguous chunks.</p>
<h3 id=task_4_controlling_registers ><a href="#task_4_controlling_registers" class=header-anchor >Task 4 &#40;controlling registers&#41;</a></h3>
<p>Write a kernel <code>cumsum_dim2&#33;</code> which computes the cumulative sum over the 2nd dimension of a 3-D array. To this purpose modify the memory copy kernel from Task 3. Verify the correctness of your kernel against <code>CUDA.cumsum&#33;</code>. Then, compute <code>T_eff_cs</code> as defined above, explain the measured performance and compare it to the one obtained with the generic <code>CUDA.cumsum&#33;</code>.</p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> cumsum_dim2!(B, A)
    ix = (blockIdx().x-<span class=hljs-number >1</span>) * blockDim().x + threadIdx().x
    iz = (blockIdx().z-<span class=hljs-number >1</span>) * blockDim().z + threadIdx().z
    cumsum_iy = <span class=hljs-number >0.0</span>
    <span class=hljs-comment >#...</span>
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span>

<span class=hljs-comment ># Verification</span>
<span class=hljs-meta >@cuda</span> <span class=hljs-comment >#...</span>
CUDA.cumsum!(B_ref, A; dims=<span class=hljs-number >2</span>);
B ≈ B_ref</code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Performance</span>
t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> <span class=hljs-comment >#...# end</span>
T_eff_cs = <span class=hljs-number >2</span>*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*nz*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<pre><code class="julia hljs">t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-comment >#...</span>
T_eff_cs = <span class=hljs-number >2</span>*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*nz*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<p>Again, you should observe no significant difference between <code>T_eff_cs</code> measured here and <code>T_tot</code> computed in Task 3 &#40;measured 519 GB/s with the Tesla P100 GPU&#41;, as you probably expected.</p>
<p>We will now implement the cumulative sum over the 1st dimension &#40;index <code>ix</code>&#41;. As for the previous implementations, the first step is to read in A &#40;and write B&#41; in a way that will later allow to easily do the required cumsums without exhibiting significant memory throughput degradation.</p>
<h3 id=task_5_kernel_loops ><a href="#task_5_kernel_loops" class=header-anchor >Task 5 &#40;kernel loops&#41;</a></h3>
<p>Modify the <code>memcopy&#33;</code> kernel given in the beginning to read in A and write B in a serial manner with respect to the first dimension &#40;index <code>ix</code>&#41;, i.e., with parallelization only over the second and third dimensions &#40;index <code>iy</code> and <code>iz</code>&#41;. Launch the kernel with the same amount of threads as in Task 1, however, place them all in the second dimension &#40;i.e. use one thread in the first and third dimensions&#41;; adapt the the block configuration correctly. Verify the correctness of your kernel. Then, think about what performance you expect, compute <code>T_tot</code> and explain the measured performance.</p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> memcopy!(B, A)
    <span class=hljs-comment >#...</span>
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span>

threads = (<span class=hljs-number >1</span>, <span class=hljs-number >256</span>, <span class=hljs-number >1</span>)
blocks  = <span class=hljs-comment >#...</span></code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Verification</span>
<span class=hljs-comment >#...</span></code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Performance</span>
t_it = <span class=hljs-comment >#...</span>
T_tot = <span class=hljs-comment >#...</span></code></pre>
<p>You likely observe <code>T_tot</code> to be an order of magnitude or more below <code>T_tot</code> measured in Task 1 and 3 &#40;measured 36 GB/s with the Tesla P100 GPU&#41; because, in contrast to the previous kernels, this kernel accesses memory discontinuously with a large stride &#40;of <code>nx</code> numbers&#41; between each requested number.</p>
<p>There obviously is no point in creating a <code>cumsum&#33;</code> kernel out of this <code>memcopy&#33;</code> kernel: the performance could only be the same or worse &#40;even though worse is not to expect&#41;. Hence, we will try to improve this memory copy kernel by accessing multiple contiguous numbers from memory, which we can achieve by launching more then one threads in the first dimension, adapting the loop accordingly.</p>
<h3 id=task_6_kernel_loops ><a href="#task_6_kernel_loops" class=header-anchor >Task 6 &#40;kernel loops&#41;</a></h3>
<p>Modify the <code>memcopy&#33;</code> kernel from Task 5 to enable reading in 32 numbers at a time in the first dimension &#40;index <code>ix</code>&#41; rather than one number at a time as before. Launch the kernel with just 32 threads, all placed in the first dimension; adapt the the block configuration if you need to. Verify the correctness of your kernel. Then, think about what performance you expect now, compute <code>T_tot</code> and explain the measured performance. <div class=note ><div class=title >💡 Note</div>
<div class=messg >You could hardcode the kernel to read 32 numbers at a time, but we prefer to write it more generally allowing to launch the kernel with a different number of threads in the first dimension &#40;however, we do not want to enable more then one block though in this dimension&#41;.</div></div></p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> memcopy!(B, A)
    <span class=hljs-comment >#...</span>
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span>

threads = <span class=hljs-comment >#...</span>
blocks  = <span class=hljs-comment >#...</span></code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Verification</span>
<span class=hljs-comment >#...</span></code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Performance</span>
t_it = <span class=hljs-comment >#...</span>
T_tot = <span class=hljs-comment >#...</span></code></pre>
<p><code>T_tot</code> should now be similar to the one obtained in Task 1 and 3 or even a bit better &#40;measured 534 GB/s with the Tesla P100 GPU&#41; thanks to the additional concurrency compared to the other <code>memcopy&#33;</code> versions. We are therefore ready to implement the cummulative sum over the 1st dimension.</p>
<h3 id=task_7_communication_via_shared_memory ><a href="#task_7_communication_via_shared_memory" class=header-anchor >Task 7 &#40;communication via shared memory&#41;</a></h3>
<p>Write a kernel <code>cumsum_dim1&#33;</code> which computes the cumulative sum over the 1st dimension of a 3-D array. To this purpose modify the memory copy kernel from Task 6. Verify the correctness of your kernel against <code>CUDA.cumsum&#33;</code>. Then, compute <code>T_eff_cs</code> as defined above, explain the measured performance and compare it to the one obtained with the generic <code>CUDA.cumsum&#33;</code>. <div class=note ><div class=title >💡 Note</div>
<div class=messg >If you read the data into shared memory, then you can compute the cumulative sum, e.g., with the first thread.</div></div></p>
<pre><code class="julia hljs"><span class=hljs-comment ># hint</span>
<span class=hljs-keyword >function</span> cumsum_dim1!(B, A)
    iy = (blockIdx().y-<span class=hljs-number >1</span>) * blockDim().y + threadIdx().y
    iz = (blockIdx().z-<span class=hljs-number >1</span>) * blockDim().z + threadIdx().z
    tx = threadIdx().x
    shmem = <span class=hljs-meta >@cuDynamicSharedMem</span>(eltype(A), blockDim().x)
    cumsum_ix = <span class=hljs-number >0.0</span>
    <span class=hljs-comment >#...</span>
    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span>

<span class=hljs-comment ># Verification</span>
<span class=hljs-comment >#...</span></code></pre>
<pre><code class="julia hljs"><span class=hljs-comment ># Performance</span>
t_it = <span class=hljs-comment >#...</span>
T_eff_cs = <span class=hljs-comment >#...</span></code></pre>
<pre><code class="julia hljs">t_it = <span class=hljs-meta >@belapsed</span> <span class=hljs-keyword >begin</span> CUDA.cumsum!($B, $A; dims=<span class=hljs-number >1</span>); synchronize() <span class=hljs-keyword >end</span>
T_eff_cs = <span class=hljs-number >2</span>*<span class=hljs-number >1</span>/<span class=hljs-number >1e9</span>*nx*ny*nz*sizeof(<span class=hljs-built_in >Float64</span>)/t_it</code></pre>
<p><code>T_eff_cs</code> is probably significantly less good than the one obtained for the cumulative sums over the other dimensions, but still quite good if we keep in mind the <code>T_tot</code> achieved with the first memcopy manner in Task 5. A good strategy for tackling an optimal implementation would certainly be to use warp-level functions &#40;and if needed a more complex summation algorithm&#41;.</p>

<p><a href="#content">⤴ <em><strong>back to Content</strong></em></a></p>
<div class=page-foot >
  <div class=copyright >
    <a href="https://github.com/eth-vaw-glaciology/course-101-0250-00/"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a><br>
    Last modified: November 18, 2022. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div>